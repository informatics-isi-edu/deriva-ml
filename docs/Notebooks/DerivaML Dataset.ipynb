{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ff6e4c93a70770",
   "metadata": {},
   "source": [
    "# DerivaML Dataset\n",
    "\n",
    "DerivaML is a class library built on the Deriva Scientific Asset management system that is designed to help simplify a number of the basic operations associated with building and testing ML libraries based on common toolkits such as TensorFlow.  This notebook reviews the basic features of the DerivaML library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db30e3e5ccd3ad77",
   "metadata": {},
   "source": [
    "## Set up DerivaML  for test case"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T22:27:47.868020Z",
     "start_time": "2025-02-08T22:27:47.844171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "c5db57e2c177f04",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T22:27:47.906673Z",
     "start_time": "2025-02-08T22:27:47.876978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from deriva.core.utils.globus_auth_utils import GlobusNativeLogin\n",
    "from deriva_ml.demo_catalog import create_demo_catalog, DemoML\n",
    "from deriva_ml import MLVocab, DatasetBag, ExecutionConfiguration, Workflow, DerivaSystemColumns\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown, HTML, JSON"
   ],
   "id": "2f4014277eabec98",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "eb169235902c77be",
   "metadata": {},
   "source": [
    "Set the details for the catalog we want and authenticate to the server if needed."
   ]
  },
  {
   "cell_type": "code",
   "id": "677df6f200423a9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T22:27:47.972996Z",
     "start_time": "2025-02-08T22:27:47.917160Z"
    }
   },
   "source": [
    "hostname = 'dev.eye-ai.org'\n",
    "domain_schema = 'demo-schema'\n",
    "\n",
    "gnl = GlobusNativeLogin(host=hostname)\n",
    "if gnl.is_logged_in([hostname]):\n",
    "    print(\"You are already logged in.\")\n",
    "else:\n",
    "    gnl.login([hostname], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n",
    "    print(\"Login Successful\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are already logged in.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "cc01e85a63c60536",
   "metadata": {},
   "source": [
    "Create a test catalog and get an instance of the DemoML class."
   ]
  },
  {
   "cell_type": "code",
   "id": "a843170d141c8a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T22:28:12.793220Z",
     "start_time": "2025-02-08T22:27:47.984073Z"
    }
   },
   "source": [
    "test_catalog = create_demo_catalog(hostname, domain_schema)\n",
    "ml_instance = DemoML(hostname, test_catalog.catalog_id)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "48c7854af033245",
   "metadata": {},
   "source": [
    "## Configure DerivaML Datasets\n",
    "\n",
    "In Deriva-ML a dataset is used to aggregate instances of entities.  However, before we can create any datasets, we must configure \n",
    "Deriva-ML for the specifics of the datasets.  The first stp is we need to tell Deriva-ML what types of use defined objects can be associated with a dataset.  \n",
    "\n",
    "Note that out of the box, Deriva-ML is configured to allow datasets to contained dataset (i.e. nested datasets), so we don't need to do anything for that specific configuration."
   ]
  },
  {
   "cell_type": "code",
   "id": "21e3d412920ac963",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T22:28:14.500970Z",
     "start_time": "2025-02-08T22:28:12.801434Z"
    }
   },
   "source": [
    "print(f\"Current dataset_table element types: {[a.name for a in ml_instance.list_dataset_element_types()]}\")\n",
    "ml_instance.add_dataset_element_type(\"Subject\")\n",
    "ml_instance.add_dataset_element_type(\"Image\")\n",
    "print(f\"New dataset_table element types {[a.name for a in ml_instance.list_dataset_element_types()]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dataset_table element types: ['Dataset']\n",
      "New dataset_table element types ['Dataset', 'Subject', 'Image']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "874f49d81439e0c9",
   "metadata": {},
   "source": [
    "Now that we have configured our datasets, we need to identify the dataset types so we can distinguish between them."
   ]
  },
  {
   "cell_type": "code",
   "id": "6908fba1443aee13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T22:28:15.812766Z",
     "start_time": "2025-02-08T22:28:14.509244Z"
    }
   },
   "source": [
    "# Create a new dataset_table\n",
    "ml_instance.add_term(MLVocab.dataset_type, \"DemoSet\", description=\"A test dataset_table\")\n",
    "ml_instance.add_term(MLVocab.dataset_type, 'Partitioned', description=\"A partitioned dataset_table for ML training.\")\n",
    "ml_instance.add_term(MLVocab.dataset_type, \"Subject\", description=\"A test dataset_table\")\n",
    "ml_instance.add_term(MLVocab.dataset_type, \"Image\", description=\"A test dataset_table\")\n",
    "ml_instance.add_term(MLVocab.dataset_type, \"Training\", description=\"Training dataset_table\")\n",
    "ml_instance.add_term(MLVocab.dataset_type, \"Testing\", description=\"Training dataset_table\")\n",
    "ml_instance.add_term(MLVocab.dataset_type, \"Validation\", description=\"Validation dataset_table\")\n",
    "\n",
    "ml_instance.list_vocabulary_terms(MLVocab.dataset_type)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[VocabularyTerm(name='DemoSet', synonyms=[], id='ml-test:3AW', uri='/id/3AW', description='A test dataset_table', rid='3AW'),\n",
       " VocabularyTerm(name='Partitioned', synonyms=[], id='ml-test:3AY', uri='/id/3AY', description='A partitioned dataset_table for ML training.', rid='3AY'),\n",
       " VocabularyTerm(name='Subject', synonyms=[], id='ml-test:3B0', uri='/id/3B0', description='A test dataset_table', rid='3B0'),\n",
       " VocabularyTerm(name='Image', synonyms=[], id='ml-test:3B2', uri='/id/3B2', description='A test dataset_table', rid='3B2'),\n",
       " VocabularyTerm(name='Training', synonyms=[], id='ml-test:3B4', uri='/id/3B4', description='Training dataset_table', rid='3B4'),\n",
       " VocabularyTerm(name='Testing', synonyms=[], id='ml-test:3B6', uri='/id/3B6', description='Training dataset_table', rid='3B6'),\n",
       " VocabularyTerm(name='Validation', synonyms=[], id='ml-test:3B8', uri='/id/3B8', description='Validation dataset_table', rid='3B8')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "1f902cd9561842c2",
   "metadata": {},
   "source": [
    "Now create datasets and populate with elements from the test catalogs."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T22:28:17.960617Z",
     "start_time": "2025-02-08T22:28:15.826639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ml_instance.add_term(MLVocab.workflow_type, \"Create Dataset Notebook\", description=\"A Workflow that creates a new dataset_table\")\n",
    "\n",
    "# Now lets create model configuration for our program.\n",
    "api_workflow = Workflow(\n",
    "    name=\"API Workflow\",\n",
    "    url=\"https://github.com/informatics-isi-edu/deriva-ml/blob/main/docs/Notebooks/DerivaML%20Dataset.ipynb\",\n",
    "    workflow_type=\"Create Dataset Notebook\"\n",
    ")\n",
    "\n",
    "dataset_execution = ml_instance.create_execution(\n",
    "    ExecutionConfiguration(\n",
    "        workflow=api_workflow,\n",
    "        description=\"Our Sample Workflow instance\")\n",
    ")"
   ],
   "id": "9a1fe793df5b3438",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T22:28:19.478548Z",
     "start_time": "2025-02-08T22:28:17.964533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "subject_dataset = dataset_execution.create_dataset(['DemoSet', 'Subject'], description=\"A subject dataset_table\")\n",
    "image_dataset = dataset_execution.create_dataset(['DemoSet', 'Image'], description=\"A image training dataset_table\")\n",
    "datasets = pd.DataFrame(ml_instance.find_datasets()).drop(columns=DerivaSystemColumns)\n",
    "display(\n",
    "    Markdown('## Datasets'),\n",
    "    datasets)"
   ],
   "id": "10567931cbd3c322",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "## Datasets"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "   RID                     Description Version MLVocab.dataset_type\n",
       "0  3BG         A subject dataset_table     3BR   [DemoSet, Subject]\n",
       "1  3BT  A image training dataset_table     3C2     [DemoSet, Image]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RID</th>\n",
       "      <th>Description</th>\n",
       "      <th>Version</th>\n",
       "      <th>MLVocab.dataset_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3BG</td>\n",
       "      <td>A subject dataset_table</td>\n",
       "      <td>3BR</td>\n",
       "      <td>[DemoSet, Subject]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3BT</td>\n",
       "      <td>A image training dataset_table</td>\n",
       "      <td>3C2</td>\n",
       "      <td>[DemoSet, Image]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "6474d1edb488097f",
   "metadata": {},
   "source": [
    "And now that we have defined some datasets, we can add elements of the appropriate type to them.  We can see what is in our new datasets by listing the dataset members."
   ]
  },
  {
   "cell_type": "code",
   "id": "16bb5b8b8b2a5288",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T22:28:39.110844Z",
     "start_time": "2025-02-08T22:28:19.500742Z"
    }
   },
   "source": [
    "# Get list of subjects and images from the catalog using the DataPath API.\n",
    "dp = ml_instance.domain_path  # Each call returns a new path instance, so only call once...\n",
    "subject_rids = [i['RID'] for i in dp.tables['Subject'].entities().fetch()]\n",
    "image_rids = [i['RID'] for i in dp.tables['Image'].entities().fetch()]\n",
    "\n",
    "ml_instance.add_dataset_members(dataset_rid=subject_dataset, members=subject_rids)\n",
    "ml_instance.add_dataset_members(dataset_rid=image_dataset, members=image_rids)\n",
    "\n",
    "# List the contents of our datasets, and let's not include columns like modify time.\n",
    "display(\n",
    "    Markdown('## Subject Dataset'),\n",
    "    pd.DataFrame(ml_instance.list_dataset_members(subject_dataset)['Subject']).drop(columns=DerivaSystemColumns),\n",
    "    Markdown('## Image Dataset'),\n",
    "    pd.DataFrame(ml_instance.list_dataset_members(image_dataset)['Image']).drop(columns=DerivaSystemColumns))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "## Subject Dataset"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "    RID     Name\n",
       "0   33M   Thing1\n",
       "1   33P   Thing2\n",
       "2   33R   Thing3\n",
       "3   33T   Thing4\n",
       "4   33W   Thing5\n",
       "5   33Y   Thing6\n",
       "6   340   Thing7\n",
       "7   342   Thing8\n",
       "8   344   Thing9\n",
       "9   346  Thing10\n",
       "10  348  Thing11\n",
       "11  34A  Thing12\n",
       "12  34C  Thing13\n",
       "13  34E  Thing14\n",
       "14  34G  Thing15\n",
       "15  34J  Thing16\n",
       "16  34M  Thing17\n",
       "17  34P  Thing18\n",
       "18  34R  Thing19\n",
       "19  34T  Thing20"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RID</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33M</td>\n",
       "      <td>Thing1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33P</td>\n",
       "      <td>Thing2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33R</td>\n",
       "      <td>Thing3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33T</td>\n",
       "      <td>Thing4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33W</td>\n",
       "      <td>Thing5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33Y</td>\n",
       "      <td>Thing6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>340</td>\n",
       "      <td>Thing7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>342</td>\n",
       "      <td>Thing8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>344</td>\n",
       "      <td>Thing9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>346</td>\n",
       "      <td>Thing10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>348</td>\n",
       "      <td>Thing11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>34A</td>\n",
       "      <td>Thing12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>34C</td>\n",
       "      <td>Thing13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>34E</td>\n",
       "      <td>Thing14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>34G</td>\n",
       "      <td>Thing15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>34J</td>\n",
       "      <td>Thing16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>34M</td>\n",
       "      <td>Thing17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>34P</td>\n",
       "      <td>Thing18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>34R</td>\n",
       "      <td>Thing19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>34T</td>\n",
       "      <td>Thing20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "## Image Dataset"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "    RID                                                URL      Filename  \\\n",
       "0   34W  /hatrac/Image/26c58aba708feb63cdb2e0e7dd7e3c39...  test_33M.txt   \n",
       "1   34Y  /hatrac/Image/02d08f98e5c68b6e57ed7b9199e91f03...  test_33P.txt   \n",
       "2   350  /hatrac/Image/7418d42e0a002ead932d349f6d96ae2e...  test_33R.txt   \n",
       "3   352  /hatrac/Image/53eb519da6de7509bfdeb6b920244396...  test_33T.txt   \n",
       "4   354  /hatrac/Image/5d85d8527dcab04f3d8a3aeb6359e5e1...  test_33W.txt   \n",
       "5   356  /hatrac/Image/d0d02cac8ec2740c0c43cd9b8e478bc6...  test_33Y.txt   \n",
       "6   358  /hatrac/Image/9d7ef397078b3f2325f5614067bea21d...  test_340.txt   \n",
       "7   35A  /hatrac/Image/be24b72e0ce49f24bcb0730822c817cd...  test_342.txt   \n",
       "8   35C  /hatrac/Image/5422e4e0e69950821430ea62c3da60b9...  test_344.txt   \n",
       "9   35E  /hatrac/Image/37cbe61e794c67225a36b3c422e2418d...  test_346.txt   \n",
       "10  35G  /hatrac/Image/fe6b5fc0ed244044096c742ed32460cc...  test_348.txt   \n",
       "11  35J  /hatrac/Image/16e99f24d867f47479bfecbaa10428c3...  test_34A.txt   \n",
       "12  35M  /hatrac/Image/d73e7e52e1ba6b15ea219868fde92e91...  test_34C.txt   \n",
       "13  35P  /hatrac/Image/a4dc154076a57a1c02218017833c6f52...  test_34E.txt   \n",
       "14  35R  /hatrac/Image/cbc37a575a7ae12e1198a535016082ae...  test_34G.txt   \n",
       "15  35T  /hatrac/Image/1e24455cbd94125de71abb7778f621ce...  test_34J.txt   \n",
       "16  35W  /hatrac/Image/fa5bf26fdf775f58bd68b1e8fe12d25f...  test_34M.txt   \n",
       "17  35Y  /hatrac/Image/d72586767054a0658811ca7f5e800113...  test_34P.txt   \n",
       "18  360  /hatrac/Image/2f3aaaf9617f6931b0285cfe5cc6a045...  test_34R.txt   \n",
       "19  362  /hatrac/Image/f8432ef3b80a0680f6a8c9c5da27a160...  test_34T.txt   \n",
       "\n",
       "     Description  Length                               MD5  Name Subject  \n",
       "0   A test image      31  26c58aba708feb63cdb2e0e7dd7e3c39  None     33M  \n",
       "1   A test image      31  02d08f98e5c68b6e57ed7b9199e91f03  None     33P  \n",
       "2   A test image      32  7418d42e0a002ead932d349f6d96ae2e  None     33R  \n",
       "3   A test image      31  53eb519da6de7509bfdeb6b920244396  None     33T  \n",
       "4   A test image      30  5d85d8527dcab04f3d8a3aeb6359e5e1  None     33W  \n",
       "5   A test image      32  d0d02cac8ec2740c0c43cd9b8e478bc6  None     33Y  \n",
       "6   A test image      31  9d7ef397078b3f2325f5614067bea21d  None     340  \n",
       "7   A test image      32  be24b72e0ce49f24bcb0730822c817cd  None     342  \n",
       "8   A test image      32  5422e4e0e69950821430ea62c3da60b9  None     344  \n",
       "9   A test image      31  37cbe61e794c67225a36b3c422e2418d  None     346  \n",
       "10  A test image      32  fe6b5fc0ed244044096c742ed32460cc  None     348  \n",
       "11  A test image      31  16e99f24d867f47479bfecbaa10428c3  None     34A  \n",
       "12  A test image      31  d73e7e52e1ba6b15ea219868fde92e91  None     34C  \n",
       "13  A test image      31  a4dc154076a57a1c02218017833c6f52  None     34E  \n",
       "14  A test image      31  cbc37a575a7ae12e1198a535016082ae  None     34G  \n",
       "15  A test image      32  1e24455cbd94125de71abb7778f621ce  None     34J  \n",
       "16  A test image      31  fa5bf26fdf775f58bd68b1e8fe12d25f  None     34M  \n",
       "17  A test image      31  d72586767054a0658811ca7f5e800113  None     34P  \n",
       "18  A test image      31  2f3aaaf9617f6931b0285cfe5cc6a045  None     34R  \n",
       "19  A test image      31  f8432ef3b80a0680f6a8c9c5da27a160  None     34T  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Description</th>\n",
       "      <th>Length</th>\n",
       "      <th>MD5</th>\n",
       "      <th>Name</th>\n",
       "      <th>Subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34W</td>\n",
       "      <td>/hatrac/Image/26c58aba708feb63cdb2e0e7dd7e3c39...</td>\n",
       "      <td>test_33M.txt</td>\n",
       "      <td>A test image</td>\n",
       "      <td>31</td>\n",
       "      <td>26c58aba708feb63cdb2e0e7dd7e3c39</td>\n",
       "      <td>None</td>\n",
       "      <td>33M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34Y</td>\n",
       "      <td>/hatrac/Image/02d08f98e5c68b6e57ed7b9199e91f03...</td>\n",
       "      <td>test_33P.txt</td>\n",
       "      <td>A test image</td>\n",
       "      <td>31</td>\n",
       "      <td>02d08f98e5c68b6e57ed7b9199e91f03</td>\n",
       "      <td>None</td>\n",
       "      <td>33P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>350</td>\n",
       "      <td>/hatrac/Image/7418d42e0a002ead932d349f6d96ae2e...</td>\n",
       "      <td>test_33R.txt</td>\n",
       "      <td>A test image</td>\n",
       "      <td>32</td>\n",
       "      <td>7418d42e0a002ead932d349f6d96ae2e</td>\n",
       "      <td>None</td>\n",
       "      <td>33R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>352</td>\n",
       "      <td>/hatrac/Image/53eb519da6de7509bfdeb6b920244396...</td>\n",
       "      <td>test_33T.txt</td>\n",
       "      <td>A test image</td>\n",
       "      <td>31</td>\n",
       "      <td>53eb519da6de7509bfdeb6b920244396</td>\n",
       "      <td>None</td>\n",
       "      <td>33T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>354</td>\n",
       "      <td>/hatrac/Image/5d85d8527dcab04f3d8a3aeb6359e5e1...</td>\n",
       "      <td>test_33W.txt</td>\n",
       "      <td>A test image</td>\n",
       "      <td>30</td>\n",
       "      <td>5d85d8527dcab04f3d8a3aeb6359e5e1</td>\n",
       "      <td>None</td>\n",
       "      <td>33W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>356</td>\n",
       "      <td>/hatrac/Image/d0d02cac8ec2740c0c43cd9b8e478bc6...</td>\n",
       "      <td>test_33Y.txt</td>\n",
       "      <td>A test image</td>\n",
       "      <td>32</td>\n",
       "      <td>d0d02cac8ec2740c0c43cd9b8e478bc6</td>\n",
       "      <td>None</td>\n",
       "      <td>33Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>358</td>\n",
       "      <td>/hatrac/Image/9d7ef397078b3f2325f5614067bea21d...</td>\n",
       "      <td>test_340.txt</td>\n",
       "      <td>A test image</td>\n",
       "      <td>31</td>\n",
       "      <td>9d7ef397078b3f2325f5614067bea21d</td>\n",
       "      <td>None</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>35A</td>\n",
       "      <td>/hatrac/Image/be24b72e0ce49f24bcb0730822c817cd...</td>\n",
       "      <td>test_342.txt</td>\n",
       "      <td>A test image</td>\n",
       "      <td>32</td>\n",
       "      <td>be24b72e0ce49f24bcb0730822c817cd</td>\n",
       "      <td>None</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>35C</td>\n",
       "      <td>/hatrac/Image/5422e4e0e69950821430ea62c3da60b9...</td>\n",
       "      <td>test_344.txt</td>\n",
       "      <td>A test image</td>\n",
       "      <td>32</td>\n",
       "      <td>5422e4e0e69950821430ea62c3da60b9</td>\n",
       "      <td>None</td>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>35E</td>\n",
       "      <td>/hatrac/Image/37cbe61e794c67225a36b3c422e2418d...</td>\n",
       "      <td>test_346.txt</td>\n",
       "      <td>A test image</td>\n",
       "      <td>31</td>\n",
       "      <td>37cbe61e794c67225a36b3c422e2418d</td>\n",
       "      <td>None</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>35G</td>\n",
       "      <td>/hatrac/Image/fe6b5fc0ed244044096c742ed32460cc...</td>\n",
       "      <td>test_348.txt</td>\n",
       "      <td>A test image</td>\n",
       "      <td>32</td>\n",
       "      <td>fe6b5fc0ed244044096c742ed32460cc</td>\n",
       "      <td>None</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>35J</td>\n",
       "      <td>/hatrac/Image/16e99f24d867f47479bfecbaa10428c3...</td>\n",
       "      <td>test_34A.txt</td>\n",
       "      <td>A test image</td>\n",
       "      <td>31</td>\n",
       "      <td>16e99f24d867f47479bfecbaa10428c3</td>\n",
       "      <td>None</td>\n",
       "      <td>34A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>35M</td>\n",
       "      <td>/hatrac/Image/d73e7e52e1ba6b15ea219868fde92e91...</td>\n",
       "      <td>test_34C.txt</td>\n",
       "      <td>A test image</td>\n",
       "      <td>31</td>\n",
       "      <td>d73e7e52e1ba6b15ea219868fde92e91</td>\n",
       "      <td>None</td>\n",
       "      <td>34C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>35P</td>\n",
       "      <td>/hatrac/Image/a4dc154076a57a1c02218017833c6f52...</td>\n",
       "      <td>test_34E.txt</td>\n",
       "      <td>A test image</td>\n",
       "      <td>31</td>\n",
       "      <td>a4dc154076a57a1c02218017833c6f52</td>\n",
       "      <td>None</td>\n",
       "      <td>34E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>35R</td>\n",
       "      <td>/hatrac/Image/cbc37a575a7ae12e1198a535016082ae...</td>\n",
       "      <td>test_34G.txt</td>\n",
       "      <td>A test image</td>\n",
       "      <td>31</td>\n",
       "      <td>cbc37a575a7ae12e1198a535016082ae</td>\n",
       "      <td>None</td>\n",
       "      <td>34G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>35T</td>\n",
       "      <td>/hatrac/Image/1e24455cbd94125de71abb7778f621ce...</td>\n",
       "      <td>test_34J.txt</td>\n",
       "      <td>A test image</td>\n",
       "      <td>32</td>\n",
       "      <td>1e24455cbd94125de71abb7778f621ce</td>\n",
       "      <td>None</td>\n",
       "      <td>34J</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>35W</td>\n",
       "      <td>/hatrac/Image/fa5bf26fdf775f58bd68b1e8fe12d25f...</td>\n",
       "      <td>test_34M.txt</td>\n",
       "      <td>A test image</td>\n",
       "      <td>31</td>\n",
       "      <td>fa5bf26fdf775f58bd68b1e8fe12d25f</td>\n",
       "      <td>None</td>\n",
       "      <td>34M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>35Y</td>\n",
       "      <td>/hatrac/Image/d72586767054a0658811ca7f5e800113...</td>\n",
       "      <td>test_34P.txt</td>\n",
       "      <td>A test image</td>\n",
       "      <td>31</td>\n",
       "      <td>d72586767054a0658811ca7f5e800113</td>\n",
       "      <td>None</td>\n",
       "      <td>34P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>360</td>\n",
       "      <td>/hatrac/Image/2f3aaaf9617f6931b0285cfe5cc6a045...</td>\n",
       "      <td>test_34R.txt</td>\n",
       "      <td>A test image</td>\n",
       "      <td>31</td>\n",
       "      <td>2f3aaaf9617f6931b0285cfe5cc6a045</td>\n",
       "      <td>None</td>\n",
       "      <td>34R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>362</td>\n",
       "      <td>/hatrac/Image/f8432ef3b80a0680f6a8c9c5da27a160...</td>\n",
       "      <td>test_34T.txt</td>\n",
       "      <td>A test image</td>\n",
       "      <td>31</td>\n",
       "      <td>f8432ef3b80a0680f6a8c9c5da27a160</td>\n",
       "      <td>None</td>\n",
       "      <td>34T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "2ac02c487e8a3be3",
   "metadata": {},
   "source": [
    "## Create partitioned dataset\n",
    "\n",
    "Now let's create some subsets of the original dataset based on subject level metadata. We are going to create the subsets based on the metadata values of the subjects. We will download the subject dataset and look at its metadata to figure out how to partition the original data. Since we are not going to look at the images, we use the materialize=False option to save some time."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T22:32:41.200964Z",
     "start_time": "2025-02-08T22:32:40.358287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bag_path, bag_rid, bag_minid = ml_instance.download_dataset_bag(subject_dataset, materialize=False)\n",
    "dataset_bag = DatasetBag(bag_rid)\n",
    "print(f\"Bag materialized to {bag_path}\")"
   ],
   "id": "264d3eaf494178ef",
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for DatasetBag.__init__\n1\n  Input should be a valid string [type=string_type, input_value=PosixPath('/var/folders/0...bfbe9b9e9c/Dataset_3BG'), input_type=PosixPath]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ndbase\n  Missing required argument [type=missing_argument, input_value=ArgsKwargs((<deriva_ml.da...be9b9e9c/Dataset_3BG'))), input_type=ArgsKwargs]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing_argument",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValidationError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m bag_path, bag_rid, bag_minid \u001B[38;5;241m=\u001B[39m ml_instance\u001B[38;5;241m.\u001B[39mdownload_dataset_bag(subject_dataset, materialize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m----> 2\u001B[0m dataset_bag \u001B[38;5;241m=\u001B[39m \u001B[43mDatasetBag\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbag_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBag materialized to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbag_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/deriva-ml/lib/python3.12/site-packages/pydantic/_internal/_validate_call.py:38\u001B[0m, in \u001B[0;36mupdate_wrapper_attributes.<locals>.wrapper_function\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(wrapped)\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper_function\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 38\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapper\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/deriva-ml/lib/python3.12/site-packages/pydantic/_internal/_validate_call.py:111\u001B[0m, in \u001B[0;36mValidateCallWrapper.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m--> 111\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__pydantic_validator__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidate_python\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpydantic_core\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mArgsKwargs\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    112\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__return_pydantic_validator__:\n\u001B[1;32m    113\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__return_pydantic_validator__(res)\n",
      "\u001B[0;31mValidationError\u001B[0m: 2 validation errors for DatasetBag.__init__\n1\n  Input should be a valid string [type=string_type, input_value=PosixPath('/var/folders/0...bfbe9b9e9c/Dataset_3BG'), input_type=PosixPath]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ndbase\n  Missing required argument [type=missing_argument, input_value=ArgsKwargs((<deriva_ml.da...be9b9e9c/Dataset_3BG'))), input_type=ArgsKwargs]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing_argument"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "aefa71000a8888b5",
   "metadata": {},
   "source": "The domain model has two objects: Subject and Images where an Image is associated with a subject, but a subject can have multiple images associated with it.  Let's look at the subjects and partition into test and training datasets."
  },
  {
   "cell_type": "code",
   "id": "405cebbc9d686f9b",
   "metadata": {},
   "source": [
    "# Get information about the subjects.....\n",
    "subject_df = dataset_bag.get_table_as_dataframe('Subject')[['RID', 'Name']]\n",
    "image_df = dataset_bag.get_table_as_dataframe('Image')[['RID', 'Subject', 'URL']]\n",
    "metadata_df = subject_df.join(image_df, lsuffix=\"_subject\", rsuffix=\"_image\")\n",
    "display(metadata_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a251643b1b7389b2",
   "metadata": {},
   "source": [
    "For ths example, lets partition the data based on the name of the subject.  Of course in real examples, we would do a more complex analysis in deciding\n",
    "what subset goes into each data set."
   ]
  },
  {
   "cell_type": "code",
   "id": "fac3a1b3d7db6556",
   "metadata": {},
   "source": [
    "def thing_number(name: pd.Series) -> pd.Series:\n",
    "    return name.map(lambda n: int(n.replace('Thing','')))\n",
    "\n",
    "training_rids = metadata_df.loc[lambda df: thing_number(df['Name']) % 3 == 0]['RID_image'].tolist()\n",
    "testing_rids =  metadata_df.loc[lambda df: thing_number(df['Name']) % 3 == 1]['RID_image'].tolist()\n",
    "validation_rids = metadata_df.loc[lambda df: thing_number(df['Name']) % 3 == 2]['RID_image'].tolist()\n",
    "\n",
    "print(f'Training images: {training_rids}')\n",
    "print(f'Testing images: {testing_rids}')\n",
    "print(f'Validation images: {validation_rids}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "27e66ce59b0fb193",
   "metadata": {},
   "source": [
    "Now that we know what we want in each dataset, lets create datasets for each of our partitioned elements along with a nested dataset to track the entire collection."
   ]
  },
  {
   "cell_type": "code",
   "id": "1fcf179738fad9f2",
   "metadata": {},
   "source": [
    "nested_dataset = dataset_execution.create_dataset(['Partitioned', 'Image'], description='A nested dataset_table for machine learning')\n",
    "training_dataset = dataset_execution.create_dataset('Training', description='An image dataset_table for training')\n",
    "testing_dataset = dataset_execution.create_dataset('Testing', description='A image dataset_table for testing')\n",
    "validation_dataset = dataset_execution.create_dataset('Validation', description='A image dataset_table for validation')\n",
    "pd.DataFrame(ml_instance.find_datasets()).drop(columns=DerivaSystemColumns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ac71fa8b087340ec",
   "metadata": {},
   "source": [
    "And then fill the datasets with the appropriate members."
   ]
  },
  {
   "cell_type": "code",
   "id": "6e76bbdf-2441-4444-be7f-e55399bcc32a",
   "metadata": {},
   "source": [
    "ml_instance.add_dataset_members(dataset_rid=nested_dataset, members=[training_dataset, testing_dataset, validation_dataset])\n",
    "ml_instance.add_dataset_members(dataset_rid=training_dataset, members=training_rids)\n",
    "ml_instance.add_dataset_members(dataset_rid=testing_dataset, members=testing_rids)\n",
    "ml_instance.add_dataset_members(dataset_rid=validation_dataset, members=validation_rids)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9d67f5a8334cde9d",
   "metadata": {},
   "source": [
    "Ok, lets see what we have now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6b8ddd57688870",
   "metadata": {},
   "source": [
    "As our very last step, lets get a PID that will allow us to share and cite the dataset that we just created"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "display(\n",
    "    Markdown('## Nested Dataset'),\n",
    "    pd.DataFrame(ml_instance.list_dataset_members(nested_dataset)['Dataset']).drop(columns=DerivaSystemColumns),\n",
    "    Markdown('## Training Dataset'),\n",
    "    pd.DataFrame(ml_instance.list_dataset_members(training_dataset)['Image']).drop(columns=DerivaSystemColumns),\n",
    "    Markdown('## Testing Dataset'),\n",
    "    pd.DataFrame(ml_instance.list_dataset_members(testing_dataset)['Image']).drop(columns=DerivaSystemColumns),\n",
    "    Markdown('## Validation Dataset'),\n",
    "    pd.DataFrame(ml_instance.list_dataset_members(validation_dataset)['Image']).drop(columns=DerivaSystemColumns),)"
   ],
   "id": "93af8b022d6c7edf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f'Dataset parents: {ml_instance.list_dataset_parents(training_dataset)}')\n",
    "print(f'Dataset children: {ml_instance.list_dataset_children(nested_dataset)}')\n"
   ],
   "id": "ff010feb58c82b56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset_citation = ml_instance.cite(nested_dataset)\n",
    "display(\n",
    "    HTML(f'Nested dataset_table citation: <a href={dataset_citation}>{dataset_citation}</a>')\n",
    ")"
   ],
   "id": "85ab7754d6ec4fe5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "display(\n",
    "     Markdown('## Nested Dataset -- Recursive Listing'),\n",
    "    JSON(ml_instance.list_dataset_members(nested_dataset, recurse=True))\n",
    ")"
   ],
   "id": "ad69f39e723923d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Dataset Versions\n",
    "Datasets have a version number which can be retrieved or incremented.  We follow the equivalent of semantic versioning, but for data rather than code.  Note that datasets are also versioned by virtue of the fact that the dataset RID can include a catalog snapshot ID as well."
   ],
   "id": "3cbefa6d45d24c13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f'Current dataset_table version for training_dataset: {ml_instance.dataset_version(training_dataset)}')\n",
    "next_version = ml_instance.increment_dataset_version(training_dataset, SemanticVersion.minor)\n",
    "print(f'Next dataset_table version for training_dataset: {next_version}')"
   ],
   "id": "2e5362767a03f050",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display(HTML(f'<a href={ml_instance.chaise_url(\"Dataset\")}>Browse Datasets</a>'))",
   "id": "ee92ec2906eee6bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_catalog.delete_ermrest_catalog(really=True)",
   "id": "8d0290ecff16a65e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
