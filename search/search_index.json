{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to DerivaML","text":"<p>DerivaML is a library designed to improve the reproducibility of machine learning experiments.  Using DerivaML facilitates the ability to create data that is continously Findable, Accessable, Interoperable, and Reusable (FAIR).  </p> <p>More details about the principles of continous FAIRness can be found here.</p> <p>Dempsey, William, Ian Foster, Scott Fraser, and Carl Kesselman.  Sharing begins at home: how continuous and ubiquitous FAIRness can enhance research productivity and data reuse.  Harvard data science review 4, no. 3 (2022). PDF</p> <p>An overview of the design and operational aspects of DerivaML can be found in this paper.</p> <p>[Li, Zhiwei, Carl Kesselman, Mike D\u2019Arcy, Michael Pazzani, and Benjamin Yizing Xu.  Deriva-ML: A Continuous FAIRness Approach to Reproducible Machine Learning Models. In 2024 IEEE 20th International Conference on e-Science (e-Science), pp. 1-10. IEEE, 2024. PDF</p> <p>The data model design in Deriva Catalog:</p> <p></p>"},{"location":"release-notes/","title":"Release Notes","text":"<p>Version 1.2.0</p> <ul> <li>Dataset versioning with semantic versioning. Note that the current dataset version does NOT have the current catalog values, but rather the values at the time the dataset was created.  To get the current values you must increment the dataset version number.  Please consult online documentation for more information on dataset and versioning.</li> <li>Streamlined create_execution.  Now all datasets are automatically downloaded and instance variable has databag classes. You no longer need to explictly create dataset_bdbag. </li> <li>Significant performance improvement on cached dataset access and initial download</li> <li>Automatic creation of MINID for every dataset download</li> <li>Added method to restore an existing execution from local disk.</li> </ul> <p>Version 1.1.4 - Fixed error when creating DatasetBag on windows platform.</p> <p>Version 1.1.1</p> <ul> <li>Removed restriction on nested datasets so that now any level of nesting can be accomidated.</li> <li>Fixed bug in nested dataset download.</li> <li>Added additional methods to DatasetBag to make it easear to explore datasets.</li> <li>Added <code>datasets</code> instance variable to Execution object which has Dataset objects for all of the datasets listed in the configuration.</li> <li>Added option to DatasetBag init to provide a dataset RID or a path.  If the dataset has already been loaded, or the dataset is nested, this will return the assocated DatasetBag object.</li> </ul>"},{"location":"Notebooks/DerivaML%20Create%20Notes/","title":"DerivaML Create Notes","text":"<p>DerivaML is a class library built on the Deriva Scientific Asset management system that is designed to help simplify a number of the basic operations associated with building and testing ML libraries based on common toolkits such as TensorFlow.  This notebook reviews the basic features of the DerivaML library.</p> In\u00a0[\u00a0]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[\u00a0]: Copied! <pre>import builtins\nfrom demo_catalog import create_demo_catalog, DemoML\nfrom deriva.core.utils.globus_auth_utils import GlobusNativeLogin\nfrom IPython.display import display, Markdown, HTML, IFrame\n</pre> import builtins from demo_catalog import create_demo_catalog, DemoML from deriva.core.utils.globus_auth_utils import GlobusNativeLogin from IPython.display import display, Markdown, HTML, IFrame  <p>Set the details for the catalog we want and authenticate to the server if needed.</p> In\u00a0[\u00a0]: Copied! <pre>hostname = 'dev.eye-ai.org'\ndomain_schema = 'demo-schema'\n\ngnl = GlobusNativeLogin(host=hostname)\nif gnl.is_logged_in([hostname]):\n    print(\"You are already logged in.\")\nelse:\n    gnl.login([hostname], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n    print(\"Login Successful\")\n</pre> hostname = 'dev.eye-ai.org' domain_schema = 'demo-schema'  gnl = GlobusNativeLogin(host=hostname) if gnl.is_logged_in([hostname]):     print(\"You are already logged in.\") else:     gnl.login([hostname], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)     print(\"Login Successful\")  In\u00a0[\u00a0]: Copied! <pre>import json\nimport os\nimport ipykernel\nimport requests\n\ndef get_notebook_filename():\n    \"\"\"Returns the current Jupyter Notebook filename.\"\"\"\n    try:\n        # Get the kernel ID\n        kernel_id = ipykernel.connect.get_connection_file().split('-')[1].split('.')[0]\n        print(kernel_id)\n        # Get running notebook servers\n        response = requests.get('http://127.0.0.1:8888/api/sessions', headers={'Authorization': ''})\n        sessions = json.loads(response.text)\n        print(sessions)\n        # Match the kernel ID to find the notebook name\n        for session in sessions:\n            print(session)\n            if session['kernel']['id'] == kernel_id:\n                return session['name']  # Returns the filename\n    except Exception as e:\n        return f\"Error: {e}\"\n\n# Usage\nnotebook_file = get_notebook_filename()\nprint(f\"Current Notebook: {notebook_file}\")\n</pre> import json import os import ipykernel import requests  def get_notebook_filename():     \"\"\"Returns the current Jupyter Notebook filename.\"\"\"     try:         # Get the kernel ID         kernel_id = ipykernel.connect.get_connection_file().split('-')[1].split('.')[0]         print(kernel_id)         # Get running notebook servers         response = requests.get('http://127.0.0.1:8888/api/sessions', headers={'Authorization': ''})         sessions = json.loads(response.text)         print(sessions)         # Match the kernel ID to find the notebook name         for session in sessions:             print(session)             if session['kernel']['id'] == kernel_id:                 return session['name']  # Returns the filename     except Exception as e:         return f\"Error: {e}\"  # Usage notebook_file = get_notebook_filename() print(f\"Current Notebook: {notebook_file}\") In\u00a0[\u00a0]: Copied! <pre>test_catalog = create_demo_catalog(hostname, domain_schema)\nml_instance = DemoML(hostname, test_catalog.catalog_id)\n</pre> test_catalog = create_demo_catalog(hostname, domain_schema) ml_instance = DemoML(hostname, test_catalog.catalog_id) <p>Now using TestFeatureClass, we can create some instances of the feature and add it.  We must have a exeuction_rid in order to define the feature.</p> In\u00a0[\u00a0]: Copied! <pre>display(IFrame(ml_instance.chaise_url('Page'), 500, 500))\n</pre> display(IFrame(ml_instance.chaise_url('Page'), 500, 500))  In\u00a0[\u00a0]: Copied! <pre>ml_instance.chaise_url('Page')\n</pre> ml_instance.chaise_url('Page') In\u00a0[\u00a0]: Copied! <pre>test_catalog.delete_ermrest_catalog(really=True)\n</pre> test_catalog.delete_ermrest_catalog(really=True)"},{"location":"Notebooks/DerivaML%20Dataset/","title":"DerivaML Dataset","text":"In\u00a0[\u00a0]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[\u00a0]: Copied! <pre>from deriva.core.utils.globus_auth_utils import GlobusNativeLogin\nfrom deriva_ml.demo_catalog import create_demo_catalog, DemoML\nfrom deriva_ml import MLVocab, ExecutionConfiguration, Workflow, DerivaSystemColumns, VersionPart, DatasetSpec\nimport pandas as pd\nfrom IPython.display import display, Markdown, HTML, JSON\n</pre> from deriva.core.utils.globus_auth_utils import GlobusNativeLogin from deriva_ml.demo_catalog import create_demo_catalog, DemoML from deriva_ml import MLVocab, ExecutionConfiguration, Workflow, DerivaSystemColumns, VersionPart, DatasetSpec import pandas as pd from IPython.display import display, Markdown, HTML, JSON <p>Set the details for the catalog we want and authenticate to the server if needed.</p> In\u00a0[\u00a0]: parameters Copied! <pre>hostname = 'localhost'\ndomain_schema = 'demo-schema'\n</pre> hostname = 'localhost' domain_schema = 'demo-schema' In\u00a0[\u00a0]: Copied! <pre>gnl = GlobusNativeLogin(host=hostname)\nif gnl.is_logged_in([hostname]):\n    print(\"You are already logged in.\")\nelse:\n    gnl.login([hostname], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n    print(\"Login Successful\")\n</pre> gnl = GlobusNativeLogin(host=hostname) if gnl.is_logged_in([hostname]):     print(\"You are already logged in.\") else:     gnl.login([hostname], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)     print(\"Login Successful\")  <p>Create a test catalog and get an instance of the DemoML class.</p> In\u00a0[\u00a0]: Copied! <pre>test_catalog = create_demo_catalog(hostname, domain_schema)\nml_instance = DemoML(hostname, test_catalog.catalog_id, use_minid=False)\n</pre> test_catalog = create_demo_catalog(hostname, domain_schema) ml_instance = DemoML(hostname, test_catalog.catalog_id, use_minid=False) In\u00a0[\u00a0]: Copied! <pre>print(f\"Current dataset_table element types: {[a.name for a in ml_instance.list_dataset_element_types()]}\")\nml_instance.add_dataset_element_type(\"Subject\")\nml_instance.add_dataset_element_type(\"Image\")\nprint(f\"New dataset_table element types {[a.name for a in ml_instance.list_dataset_element_types()]}\")\n</pre> print(f\"Current dataset_table element types: {[a.name for a in ml_instance.list_dataset_element_types()]}\") ml_instance.add_dataset_element_type(\"Subject\") ml_instance.add_dataset_element_type(\"Image\") print(f\"New dataset_table element types {[a.name for a in ml_instance.list_dataset_element_types()]}\") <p>Now that we have configured our datasets, we need to identify the dataset types so we can distinguish between them.</p> In\u00a0[\u00a0]: Copied! <pre># Create a new dataset_table\nml_instance.add_term(MLVocab.dataset_type, \"DemoSet\", description=\"A test dataset_table\")\nml_instance.add_term(MLVocab.dataset_type, 'Partitioned', description=\"A partitioned dataset_table for ML training.\")\nml_instance.add_term(MLVocab.dataset_type, \"Subject\", description=\"A test dataset_table\")\nml_instance.add_term(MLVocab.dataset_type, \"Image\", description=\"A test dataset_table\")\nml_instance.add_term(MLVocab.dataset_type, \"Training\", description=\"Training dataset_table\")\nml_instance.add_term(MLVocab.dataset_type, \"Testing\", description=\"Training dataset_table\")\nml_instance.add_term(MLVocab.dataset_type, \"Validation\", description=\"Validation dataset_table\")\n\nml_instance.list_vocabulary_terms(MLVocab.dataset_type)\n</pre> # Create a new dataset_table ml_instance.add_term(MLVocab.dataset_type, \"DemoSet\", description=\"A test dataset_table\") ml_instance.add_term(MLVocab.dataset_type, 'Partitioned', description=\"A partitioned dataset_table for ML training.\") ml_instance.add_term(MLVocab.dataset_type, \"Subject\", description=\"A test dataset_table\") ml_instance.add_term(MLVocab.dataset_type, \"Image\", description=\"A test dataset_table\") ml_instance.add_term(MLVocab.dataset_type, \"Training\", description=\"Training dataset_table\") ml_instance.add_term(MLVocab.dataset_type, \"Testing\", description=\"Training dataset_table\") ml_instance.add_term(MLVocab.dataset_type, \"Validation\", description=\"Validation dataset_table\")  ml_instance.list_vocabulary_terms(MLVocab.dataset_type) <p>Now create datasets and populate with elements from the test catalogs.</p> In\u00a0[\u00a0]: Copied! <pre>ml_instance.add_term(MLVocab.workflow_type, \"Create Dataset Notebook\", description=\"A Workflow that creates a new dataset_table\")\n\n# Now lets create model configuration for our program.\napi_workflow = Workflow(\n    name=\"API Workflow\",\n    url=\"https://github.com/informatics-isi-edu/deriva-ml/blob/main/docs/Notebooks/DerivaML%20Dataset.ipynb\",\n    workflow_type=\"Create Dataset Notebook\"\n)\n\ndataset_execution = ml_instance.create_execution(\n    ExecutionConfiguration(\n        workflow=api_workflow,\n        description=\"Our Sample Workflow instance\")\n)\n</pre> ml_instance.add_term(MLVocab.workflow_type, \"Create Dataset Notebook\", description=\"A Workflow that creates a new dataset_table\")  # Now lets create model configuration for our program. api_workflow = Workflow(     name=\"API Workflow\",     url=\"https://github.com/informatics-isi-edu/deriva-ml/blob/main/docs/Notebooks/DerivaML%20Dataset.ipynb\",     workflow_type=\"Create Dataset Notebook\" )  dataset_execution = ml_instance.create_execution(     ExecutionConfiguration(         workflow=api_workflow,         description=\"Our Sample Workflow instance\") ) In\u00a0[\u00a0]: Copied! <pre>subject_dataset = dataset_execution.create_dataset(['DemoSet', 'Subject'], description=\"A subject dataset_table\")\nimage_dataset = dataset_execution.create_dataset(['DemoSet', 'Image'], description=\"A image training dataset_table\")\ndatasets = pd.DataFrame(ml_instance.find_datasets()).drop(columns=DerivaSystemColumns)\ndisplay(\n    Markdown('## Datasets'),\n    datasets)\n</pre> subject_dataset = dataset_execution.create_dataset(['DemoSet', 'Subject'], description=\"A subject dataset_table\") image_dataset = dataset_execution.create_dataset(['DemoSet', 'Image'], description=\"A image training dataset_table\") datasets = pd.DataFrame(ml_instance.find_datasets()).drop(columns=DerivaSystemColumns) display(     Markdown('## Datasets'),     datasets) <p>And now that we have defined some datasets, we can add elements of the appropriate type to them.  We can see what is in our new datasets by listing the dataset members.</p> In\u00a0[\u00a0]: Copied! <pre># Get list of subjects and images from the catalog using the DataPath API.\ndp = ml_instance.domain_path  # Each call returns a new path instance, so only call once...\nsubject_rids = [i['RID'] for i in dp.tables['Subject'].entities().fetch()]\nimage_rids = [i['RID'] for i in dp.tables['Image'].entities().fetch()]\n\nml_instance.add_dataset_members(dataset_rid=subject_dataset, members=subject_rids)\nml_instance.add_dataset_members(dataset_rid=image_dataset, members=image_rids)\n\n# List the contents of our datasets, and let's not include columns like modify time.\ndisplay(\n    Markdown('## Subject Dataset'),\n    pd.DataFrame(ml_instance.list_dataset_members(subject_dataset)['Subject']).drop(columns=DerivaSystemColumns),\n    Markdown('## Image Dataset'),\n    pd.DataFrame(ml_instance.list_dataset_members(image_dataset)['Image']).drop(columns=DerivaSystemColumns))\n</pre> # Get list of subjects and images from the catalog using the DataPath API. dp = ml_instance.domain_path  # Each call returns a new path instance, so only call once... subject_rids = [i['RID'] for i in dp.tables['Subject'].entities().fetch()] image_rids = [i['RID'] for i in dp.tables['Image'].entities().fetch()]  ml_instance.add_dataset_members(dataset_rid=subject_dataset, members=subject_rids) ml_instance.add_dataset_members(dataset_rid=image_dataset, members=image_rids)  # List the contents of our datasets, and let's not include columns like modify time. display(     Markdown('## Subject Dataset'),     pd.DataFrame(ml_instance.list_dataset_members(subject_dataset)['Subject']).drop(columns=DerivaSystemColumns),     Markdown('## Image Dataset'),     pd.DataFrame(ml_instance.list_dataset_members(image_dataset)['Image']).drop(columns=DerivaSystemColumns)) In\u00a0[\u00a0]: Copied! <pre>dataset_bag = ml_instance.download_dataset_bag(DatasetSpec(rid=subject_dataset, version=ml_instance.dataset_version(subject_dataset), materialize=False))\nprint(f\"Bag materialized\")\n</pre> dataset_bag = ml_instance.download_dataset_bag(DatasetSpec(rid=subject_dataset, version=ml_instance.dataset_version(subject_dataset), materialize=False)) print(f\"Bag materialized\") <p>The domain model has two objects: Subject and Images where an Image is associated with a subject, but a subject can have multiple images associated with it.  Let's look at the subjects and partition into test and training datasets.</p> In\u00a0[\u00a0]: Copied! <pre># Get information about the subjects.....\nsubject_df = dataset_bag.get_table_as_dataframe('Subject')[['RID', 'Name']]\nimage_df = dataset_bag.get_table_as_dataframe('Image')[['RID', 'Subject', 'URL']]\nmetadata_df = subject_df.join(image_df, lsuffix=\"_subject\", rsuffix=\"_image\")\ndisplay(metadata_df)\n</pre> # Get information about the subjects..... subject_df = dataset_bag.get_table_as_dataframe('Subject')[['RID', 'Name']] image_df = dataset_bag.get_table_as_dataframe('Image')[['RID', 'Subject', 'URL']] metadata_df = subject_df.join(image_df, lsuffix=\"_subject\", rsuffix=\"_image\") display(metadata_df) <p>For ths example, lets partition the data based on the name of the subject.  Of course in real examples, we would do a more complex analysis in deciding what subset goes into each data set.</p> In\u00a0[\u00a0]: Copied! <pre>def thing_number(name: pd.Series) -&gt; pd.Series:\n    return name.map(lambda n: int(n.replace('Thing','')))\n\ntraining_rids = metadata_df.loc[lambda df: thing_number(df['Name']) % 3 == 0]['RID_image'].tolist()\ntesting_rids =  metadata_df.loc[lambda df: thing_number(df['Name']) % 3 == 1]['RID_image'].tolist()\nvalidation_rids = metadata_df.loc[lambda df: thing_number(df['Name']) % 3 == 2]['RID_image'].tolist()\n\nprint(f'Training images: {training_rids}')\nprint(f'Testing images: {testing_rids}')\nprint(f'Validation images: {validation_rids}')\n</pre> def thing_number(name: pd.Series) -&gt; pd.Series:     return name.map(lambda n: int(n.replace('Thing','')))  training_rids = metadata_df.loc[lambda df: thing_number(df['Name']) % 3 == 0]['RID_image'].tolist() testing_rids =  metadata_df.loc[lambda df: thing_number(df['Name']) % 3 == 1]['RID_image'].tolist() validation_rids = metadata_df.loc[lambda df: thing_number(df['Name']) % 3 == 2]['RID_image'].tolist()  print(f'Training images: {training_rids}') print(f'Testing images: {testing_rids}') print(f'Validation images: {validation_rids}') <p>Now that we know what we want in each dataset, lets create datasets for each of our partitioned elements along with a nested dataset to track the entire collection.</p> In\u00a0[\u00a0]: Copied! <pre>nested_dataset = dataset_execution.create_dataset(['Partitioned', 'Image'], description='A nested dataset_table for machine learning')\ntraining_dataset = dataset_execution.create_dataset('Training', description='An image dataset_table for training')\ntesting_dataset = dataset_execution.create_dataset('Testing', description='A image dataset_table for testing')\nvalidation_dataset = dataset_execution.create_dataset('Validation', description='A image dataset_table for validation')\npd.DataFrame(ml_instance.find_datasets()).drop(columns=DerivaSystemColumns)\n</pre> nested_dataset = dataset_execution.create_dataset(['Partitioned', 'Image'], description='A nested dataset_table for machine learning') training_dataset = dataset_execution.create_dataset('Training', description='An image dataset_table for training') testing_dataset = dataset_execution.create_dataset('Testing', description='A image dataset_table for testing') validation_dataset = dataset_execution.create_dataset('Validation', description='A image dataset_table for validation') pd.DataFrame(ml_instance.find_datasets()).drop(columns=DerivaSystemColumns) <p>And then fill the datasets with the appropriate members.</p> In\u00a0[\u00a0]: Copied! <pre>ml_instance.add_dataset_members(dataset_rid=nested_dataset, members=[training_dataset, testing_dataset, validation_dataset])\nml_instance.add_dataset_members(dataset_rid=training_dataset, members=training_rids)\nml_instance.add_dataset_members(dataset_rid=testing_dataset, members=testing_rids)\nml_instance.add_dataset_members(dataset_rid=validation_dataset, members=validation_rids)\n</pre> ml_instance.add_dataset_members(dataset_rid=nested_dataset, members=[training_dataset, testing_dataset, validation_dataset]) ml_instance.add_dataset_members(dataset_rid=training_dataset, members=training_rids) ml_instance.add_dataset_members(dataset_rid=testing_dataset, members=testing_rids) ml_instance.add_dataset_members(dataset_rid=validation_dataset, members=validation_rids) <p>Ok, lets see what we have now.</p> <p>As our very last step, lets get a PID that will allow us to share and cite the dataset that we just created</p> In\u00a0[\u00a0]: Copied! <pre>display(\n    Markdown('## Nested Dataset'),\n    pd.DataFrame(ml_instance.list_dataset_members(nested_dataset)['Dataset']).drop(columns=DerivaSystemColumns),\n    Markdown('## Training Dataset'),\n    pd.DataFrame(ml_instance.list_dataset_members(training_dataset)['Image']).drop(columns=DerivaSystemColumns),\n    Markdown('## Testing Dataset'),\n    pd.DataFrame(ml_instance.list_dataset_members(testing_dataset)['Image']).drop(columns=DerivaSystemColumns),\n    Markdown('## Validation Dataset'),\n    pd.DataFrame(ml_instance.list_dataset_members(validation_dataset)['Image']).drop(columns=DerivaSystemColumns),)\n</pre> display(     Markdown('## Nested Dataset'),     pd.DataFrame(ml_instance.list_dataset_members(nested_dataset)['Dataset']).drop(columns=DerivaSystemColumns),     Markdown('## Training Dataset'),     pd.DataFrame(ml_instance.list_dataset_members(training_dataset)['Image']).drop(columns=DerivaSystemColumns),     Markdown('## Testing Dataset'),     pd.DataFrame(ml_instance.list_dataset_members(testing_dataset)['Image']).drop(columns=DerivaSystemColumns),     Markdown('## Validation Dataset'),     pd.DataFrame(ml_instance.list_dataset_members(validation_dataset)['Image']).drop(columns=DerivaSystemColumns),) In\u00a0[\u00a0]: Copied! <pre>print(f'Dataset parents: {ml_instance.list_dataset_parents(training_dataset)}')\nprint(f'Dataset children: {ml_instance.list_dataset_children(nested_dataset)}')\n</pre> print(f'Dataset parents: {ml_instance.list_dataset_parents(training_dataset)}') print(f'Dataset children: {ml_instance.list_dataset_children(nested_dataset)}')  In\u00a0[\u00a0]: Copied! <pre>dataset_citation = ml_instance.cite(nested_dataset)\ndisplay(\n    HTML(f'Nested dataset_table citation: &lt;a href={dataset_citation}&gt;{dataset_citation}&lt;/a&gt;')\n)\n</pre> dataset_citation = ml_instance.cite(nested_dataset) display(     HTML(f'Nested dataset_table citation: {dataset_citation}') ) In\u00a0[\u00a0]: Copied! <pre>display(\n     Markdown('## Nested Dataset -- Recursive Listing'),\n    JSON(ml_instance.list_dataset_members(nested_dataset, recurse=True))\n)\n</pre> display(      Markdown('## Nested Dataset -- Recursive Listing'),     JSON(ml_instance.list_dataset_members(nested_dataset, recurse=True)) ) In\u00a0[\u00a0]: Copied! <pre>print(f'Current dataset_table version for training_dataset: {ml_instance.dataset_version(training_dataset)}')\nnext_version = ml_instance.increment_dataset_version(training_dataset, VersionPart.minor)\nprint(f'Next dataset_table version for training_dataset: {next_version}')\n</pre> print(f'Current dataset_table version for training_dataset: {ml_instance.dataset_version(training_dataset)}') next_version = ml_instance.increment_dataset_version(training_dataset, VersionPart.minor) print(f'Next dataset_table version for training_dataset: {next_version}') In\u00a0[\u00a0]: Copied! <pre>display(HTML(f'&lt;a href={ml_instance.chaise_url(\"Dataset\")}&gt;Browse Datasets&lt;/a&gt;'))\n</pre> display(HTML(f'Browse Datasets')) In\u00a0[\u00a0]: Copied! <pre>test_catalog.delete_ermrest_catalog(really=True)\n</pre> test_catalog.delete_ermrest_catalog(really=True)"},{"location":"Notebooks/DerivaML%20Dataset/#derivaml-dataset","title":"DerivaML Dataset\u00b6","text":"<p>DerivaML is a class library built on the Deriva Scientific Asset management system that is designed to help simplify a number of the basic operations associated with building and testing ML libraries based on common toolkits such as TensorFlow.  This notebook reviews the basic features of the DerivaML library.</p>"},{"location":"Notebooks/DerivaML%20Dataset/#set-up-derivaml-for-test-case","title":"Set up DerivaML  for test case\u00b6","text":""},{"location":"Notebooks/DerivaML%20Dataset/#configure-derivaml-datasets","title":"Configure DerivaML Datasets\u00b6","text":"<p>In Deriva-ML a dataset is used to aggregate instances of entities.  However, before we can create any datasets, we must configure Deriva-ML for the specifics of the datasets.  The first stp is we need to tell Deriva-ML what types of use defined objects can be associated with a dataset.</p> <p>Note that out of the box, Deriva-ML is configured to allow datasets to contained dataset (i.e. nested datasets), so we don't need to do anything for that specific configuration.</p>"},{"location":"Notebooks/DerivaML%20Dataset/#create-partitioned-dataset","title":"Create partitioned dataset\u00b6","text":"<p>Now let's create some subsets of the original dataset based on subject level metadata. We are going to create the subsets based on the metadata values of the subjects. We will download the subject dataset and look at its metadata to figure out how to partition the original data. Since we are not going to look at the images, we use the materialize=False option to save some time.</p>"},{"location":"Notebooks/DerivaML%20Dataset/#dataset-versions","title":"Dataset Versions\u00b6","text":"<p>Datasets have a version number which can be retrieved or incremented.  We follow the equivalent of semantic versioning, but for data rather than code.  Note that datasets are also versioned by virtue of the fact that the dataset RID can include a catalog snapshot ID as well.</p>"},{"location":"Notebooks/DerivaML%20Execution/","title":"DerivaML Execution","text":"In\u00a0[\u00a0]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[\u00a0]: Copied! <pre>import builtins\nfrom deriva.core.utils.globus_auth_utils import GlobusNativeLogin\nfrom deriva_ml import ExecutionConfiguration, MLVocab, DerivaSystemColumns, DatasetSpec\nfrom deriva_ml.demo_catalog import create_demo_catalog, DemoML\nfrom IPython.display import display, Markdown, JSON\nimport itertools\nimport pandas as pd\n</pre> import builtins from deriva.core.utils.globus_auth_utils import GlobusNativeLogin from deriva_ml import ExecutionConfiguration, MLVocab, DerivaSystemColumns, DatasetSpec from deriva_ml.demo_catalog import create_demo_catalog, DemoML from IPython.display import display, Markdown, JSON import itertools import pandas as pd <p>Set the details for the catalog we want and authenticate to the server if needed.</p> In\u00a0[\u00a0]: Copied! <pre>hostname = 'dev.eye-ai.org'\ndomain_schema = 'demo-schema'\n\ngnl = GlobusNativeLogin(host=hostname)\nif gnl.is_logged_in([hostname]):\n    print(\"You are already logged in.\")\nelse:\n    gnl.login([hostname], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n    print(\"Login Successful\")\n</pre> hostname = 'dev.eye-ai.org' domain_schema = 'demo-schema'  gnl = GlobusNativeLogin(host=hostname) if gnl.is_logged_in([hostname]):     print(\"You are already logged in.\") else:     gnl.login([hostname], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)     print(\"Login Successful\")  <p>Create a test catalog and get an instance of the DerivaML class.  Use options so that we create some initial datasets and features.  Use the exploration API to find out what features and datasets we have.</p> In\u00a0[\u00a0]: Copied! <pre>test_catalog = create_demo_catalog(hostname, domain_schema, create_features=True, create_datasets=True)\nml_instance = DemoML(hostname, test_catalog.catalog_id)\nprint(f'Creating catalog at {ml_instance.catalog_id}')\n</pre> test_catalog = create_demo_catalog(hostname, domain_schema, create_features=True, create_datasets=True) ml_instance = DemoML(hostname, test_catalog.catalog_id) print(f'Creating catalog at {ml_instance.catalog_id}') In\u00a0[\u00a0]: Copied! <pre>display(\n    Markdown('## Datasets'),\n    pd.DataFrame(ml_instance.find_datasets()).drop(columns=DerivaSystemColumns),\n\n    Markdown('## Features'),\n    [f'{f.target_table.name}:{f.feature_name}' for f in ml_instance.find_features(\"Subject\")],\n    [f'{f.target_table.name}:{f.feature_name}' for f in ml_instance.find_features(\"Image\")]\n)\n</pre> display(     Markdown('## Datasets'),     pd.DataFrame(ml_instance.find_datasets()).drop(columns=DerivaSystemColumns),      Markdown('## Features'),     [f'{f.target_table.name}:{f.feature_name}' for f in ml_instance.find_features(\"Subject\")],     [f'{f.target_table.name}:{f.feature_name}' for f in ml_instance.find_features(\"Image\")] ) In\u00a0[\u00a0]: Copied! <pre>datasets = pd.DataFrame(ml_instance.find_datasets()).drop(columns=DerivaSystemColumns)\ntraining_dataset_rid = [ds['RID'] for ds in ml_instance.find_datasets() if 'Training' in ds['Dataset_Type']][0]\ntesting_dataset_rid = [ds['RID'] for ds in ml_instance.find_datasets() if 'Testing' in ds['Dataset_Type']][0]\n\ndisplay(\n    Markdown(f'Training Dataset: {training_dataset_rid}'),\n    Markdown('## Datasets'),\n    datasets)\n</pre> datasets = pd.DataFrame(ml_instance.find_datasets()).drop(columns=DerivaSystemColumns) training_dataset_rid = [ds['RID'] for ds in ml_instance.find_datasets() if 'Training' in ds['Dataset_Type']][0] testing_dataset_rid = [ds['RID'] for ds in ml_instance.find_datasets() if 'Testing' in ds['Dataset_Type']][0]  display(     Markdown(f'Training Dataset: {training_dataset_rid}'),     Markdown('## Datasets'),     datasets) In\u00a0[\u00a0]: Copied! <pre>ml_instance.add_term(MLVocab.workflow_type, \"Execution Notebook\", description=\"Notebook for demonstrating executions\")\nml_instance.add_term(MLVocab.asset_type, \"API_Model\", description=\"Model for our API workflow\")\n\napi_workflow = ml_instance.create_workflow(\n    name=\"Execution Notebook Workflow\",\n    workflow_type=\"Execution Notebook\",\n    description=\"Demonstration notebook\"\n)\n\nnotebook_execution = ml_instance.create_execution(ExecutionConfiguration( description=\"Sample Execution\", workflow=api_workflow))\n\n# Now lets create model configuration for our program.\nmodel_file = notebook_execution.asset_file_path(\"Execution_Asset\",'modelfile.txt', asset_types=\"API_Model\")\nwith builtins.open(model_file, \"w\") as fp:\n    fp.write(f\"My model\")\n\n# Now upload the file and retrieve the RID of the new asset from the returned results.\nuploaded_assets = notebook_execution.upload_execution_outputs()\ntraining_model_rid = [a.asset_rid  for a  in uploaded_assets['deriva-ml/Execution_Asset'] if 'API_Model' in a.asset_types][0]\n\ndisplay(\n    Markdown(f'## Training Model: {training_model_rid}'),\n    JSON(ml_instance.retrieve_rid(training_model_rid))\n)\n</pre> ml_instance.add_term(MLVocab.workflow_type, \"Execution Notebook\", description=\"Notebook for demonstrating executions\") ml_instance.add_term(MLVocab.asset_type, \"API_Model\", description=\"Model for our API workflow\")  api_workflow = ml_instance.create_workflow(     name=\"Execution Notebook Workflow\",     workflow_type=\"Execution Notebook\",     description=\"Demonstration notebook\" )  notebook_execution = ml_instance.create_execution(ExecutionConfiguration( description=\"Sample Execution\", workflow=api_workflow))  # Now lets create model configuration for our program. model_file = notebook_execution.asset_file_path(\"Execution_Asset\",'modelfile.txt', asset_types=\"API_Model\") with builtins.open(model_file, \"w\") as fp:     fp.write(f\"My model\")  # Now upload the file and retrieve the RID of the new asset from the returned results. uploaded_assets = notebook_execution.upload_execution_outputs() training_model_rid = [a.asset_rid  for a  in uploaded_assets['deriva-ml/Execution_Asset'] if 'API_Model' in a.asset_types][0]  display(     Markdown(f'## Training Model: {training_model_rid}'),     JSON(ml_instance.retrieve_rid(training_model_rid)) ) In\u00a0[\u00a0]: Copied! <pre>ml_instance.add_term(MLVocab.workflow_type, \"ML Demo\", description=\"A ML Workflow that uses Deriva ML API\")\n\nconfig = ExecutionConfiguration(\n        assets = [training_model_rid],\n    description=\"Notebook ML Execution\",\n    workflow=api_workflow,\n    datasets=[DatasetSpec(rid=training_dataset_rid, version=ml_instance.dataset_version(training_dataset_rid)),\n            DatasetSpec(rid=testing_dataset_rid, version=ml_instance.dataset_version(training_dataset_rid), materialize=False)],\n)\n\nml_execution = ml_instance.create_execution(config)\n</pre> ml_instance.add_term(MLVocab.workflow_type, \"ML Demo\", description=\"A ML Workflow that uses Deriva ML API\")  config = ExecutionConfiguration(         assets = [training_model_rid],     description=\"Notebook ML Execution\",     workflow=api_workflow,     datasets=[DatasetSpec(rid=training_dataset_rid, version=ml_instance.dataset_version(training_dataset_rid)),             DatasetSpec(rid=testing_dataset_rid, version=ml_instance.dataset_version(training_dataset_rid), materialize=False)], )  ml_execution = ml_instance.create_execution(config) In\u00a0[\u00a0]: Copied! <pre>ml_execution.asset_paths\n</pre> ml_execution.asset_paths In\u00a0[\u00a0]: Copied! <pre>with ml_execution.execute() as deriva_exec:\n    # Get the input datasets:\n    training_dataset = ml_execution.datasets[0]  # Input dataset\n    image_rids = training_dataset.get_table_as_dataframe('Image')['RID']\n\n    # Get input files\n    with open(ml_execution.asset_paths[0], 'rt') as model_file:\n        training_model = model_file.read()\n        print(f'Got model file: {training_model}')\n\n    # Put your ML code here....\n    pass\n\n    # Write a new model\n    model_file = ml_execution.asset_path('API_Model', 'modelfile.txt')\n    with open(model_file, 'w') as f:\n        f.write(\"Hello there a new model;\\n\")\n\n    # Create some new feature values.\n    bb_csv_path, bb_asset_paths = ml_execution.execution_asset_path('BoundingBox')\n    bounding_box_files = [bb_asset_paths['BoundingBox'] / f\"box{i}.txt\" for i in range(10)]\n    for i in range(10):\n        bounding_box_files.append(fn := bb_asset_paths['BoundingBox'] / f\"box{i}.txt\")\n        with builtins.open(fn, \"w\") as fp:\n            fp.write(f\"Hi there {i}\")\n\n    ImageBoundingboxFeature = ml_instance.feature_record_class(\"Image\", \"BoundingBox\")\n    image_bounding_box_feature_list = [ImageBoundingboxFeature(Image=image_rid,\n                                                               Execution=ml_execution.execution_rid,\n                                                               BoundingBox=asset_rid)\n                                       for image_rid, asset_rid in zip(image_rids, itertools.cycle(bounding_box_files))]\n\n    ml_execution.add_features(image_bounding_box_feature_list)\n\nupload_status = ml_execution.upload_execution_outputs()\n</pre> with ml_execution.execute() as deriva_exec:     # Get the input datasets:     training_dataset = ml_execution.datasets[0]  # Input dataset     image_rids = training_dataset.get_table_as_dataframe('Image')['RID']      # Get input files     with open(ml_execution.asset_paths[0], 'rt') as model_file:         training_model = model_file.read()         print(f'Got model file: {training_model}')      # Put your ML code here....     pass      # Write a new model     model_file = ml_execution.asset_path('API_Model', 'modelfile.txt')     with open(model_file, 'w') as f:         f.write(\"Hello there a new model;\\n\")      # Create some new feature values.     bb_csv_path, bb_asset_paths = ml_execution.execution_asset_path('BoundingBox')     bounding_box_files = [bb_asset_paths['BoundingBox'] / f\"box{i}.txt\" for i in range(10)]     for i in range(10):         bounding_box_files.append(fn := bb_asset_paths['BoundingBox'] / f\"box{i}.txt\")         with builtins.open(fn, \"w\") as fp:             fp.write(f\"Hi there {i}\")      ImageBoundingboxFeature = ml_instance.feature_record_class(\"Image\", \"BoundingBox\")     image_bounding_box_feature_list = [ImageBoundingboxFeature(Image=image_rid,                                                                Execution=ml_execution.execution_rid,                                                                BoundingBox=asset_rid)                                        for image_rid, asset_rid in zip(image_rids, itertools.cycle(bounding_box_files))]      ml_execution.add_features(image_bounding_box_feature_list)  upload_status = ml_execution.upload_execution_outputs() <p>Now lets check the assets produced by this execution to make sure that they are what we expect.</p> In\u00a0[\u00a0]: Copied! <pre># Get datapath to the ML schema.\nschema_path = ml_instance.pathBuilder.schemas[ml_instance.ml_schema]\n\n# Now get path to the execution table, and get our execution record.  We filter on the RID for the\n# execution we are looking for.\nexecutions = schema_path.Execution.filter(schema_path.Execution.RID == ml_execution.execution_rid)\nexecution_info = list(executions.entities().fetch())[0]\n\n# To get the assets for the execution, we need to go through the linking table to the assets.\nasset_path = executions.link(schema_path.Execution_Asset_Execution).link(schema_path.Execution_Asset)\npd.DataFrame(asset_path.entities().fetch()).drop(columns=DerivaSystemColumns + ['MD5'])\n\n# Now lets display our results.\ndisplay(\n    Markdown(f'### Execution: {ml_execution.execution_rid}'),\n    JSON(execution_info),\n    Markdown(f'### Execution Assets'),\n    pd.DataFrame(asset_path.entities().fetch()).drop(columns=DerivaSystemColumns + ['MD5']),\n)\n</pre> # Get datapath to the ML schema. schema_path = ml_instance.pathBuilder.schemas[ml_instance.ml_schema]  # Now get path to the execution table, and get our execution record.  We filter on the RID for the # execution we are looking for. executions = schema_path.Execution.filter(schema_path.Execution.RID == ml_execution.execution_rid) execution_info = list(executions.entities().fetch())[0]  # To get the assets for the execution, we need to go through the linking table to the assets. asset_path = executions.link(schema_path.Execution_Asset_Execution).link(schema_path.Execution_Asset) pd.DataFrame(asset_path.entities().fetch()).drop(columns=DerivaSystemColumns + ['MD5'])  # Now lets display our results. display(     Markdown(f'### Execution: {ml_execution.execution_rid}'),     JSON(execution_info),     Markdown(f'### Execution Assets'),     pd.DataFrame(asset_path.entities().fetch()).drop(columns=DerivaSystemColumns + ['MD5']), ) In\u00a0[\u00a0]: Copied! <pre>test_catalog.delete_ermrest_catalog(really=True)\n</pre> test_catalog.delete_ermrest_catalog(really=True)"},{"location":"Notebooks/DerivaML%20Execution/#derivaml-execution","title":"DerivaML Execution\u00b6","text":"<p>DerivaML is a class library built on the Deriva Scientific Asset management system that is designed to help simplify a number of the basic operations associated with building and testing ML libraries based on common toolkits such as TensorFlow.  This notebook reviews the basic features of the DerivaML library.</p>"},{"location":"Notebooks/DerivaML%20Execution/#initializing-the-environment-for-an-execution","title":"Initializing the environment for an execution\u00b6","text":"<p>In DerivaML, the catalog is the source of record for all of the data created and used by a machine learning experiment.  While we can use the Deriva API to interact directly with the catalog, DerivaML provides a much simpler way of retrieving and adding data to a catalog.</p> <p>The core concept in this process is an execution.  An execution can be the process of training a model, of executing a model, for running analysis scripts, or even a manual operation.  Every execution in DerivaML is uniquely identified by a resource identifier (RID).</p> <p>The steps involved in creating and using an execution are:</p> <ol> <li>Create an Execution configuration object that identifies the inputs, and code for the execution.</li> <li>Create a workflow object to represent the code/operation that you will perform</li> <li>Create an execution instance, which will download all of the required inputs from the catalog Locate the input files using methods in the execution instance</li> <li>Perform your computation, placing output files in locations provided by the execution instance methods</li> <li>Upload the results of the computation using the execution instance methods. This will upload all of your files and tag them with the execution RID so you know how they were generated.  In addition, and new tabular data in CSV format will be uploaded to corrisponding tables in the catalog.</li> </ol>"},{"location":"Notebooks/DerivaML%20Execution/#creating-an-exectutionconfiguration","title":"Creating an <code>ExectutionConfiguration</code>\u00b6","text":"<p>An execution can be described by the datasets and files that it needs, the code that it runs, and the resulting files that it creates. This information is captured in an ExecutionConfiguration object:</p> <pre><code>class ExecutionConfiguration:\n \"\"\"\n    Define the parameters that are used to configure a specific execution.\n\n    Arguments:\n        datasets: List of dataset RIDS, MINIDS for datasets to be downloaded prior to execution.  By default,\n                 all  the datasets are materialized. However, if the assets associated with a dataset are not\n                 needed, a dictionary that defines the rid and the materialization parameter for the\n                 download_dataset_bag method can be specified, e.g.  datasets=[{'rid': RID, 'materialize': True}].\n        assets: List of assets to be downloaded prior to execution.  The values must be RIDs in an asset table\n        workflow: A workflow instance.  Must have a name, URI to the workflow instance, and a type.\n        description: A description of the execution.  Can use markdown format.</code></pre>"},{"location":"Notebooks/DerivaML%20Execution/#creating-a-workflow","title":"Creating a <code>Workflow</code>\u00b6","text":"<p>The actual code that is being run is represented by a <code>Workflow</code> class.  A workflow class is intended to be quite general and could be a Python script, a Jupyter notebook, a manual process, or even a Airflow or some other type of workflow system.  In order to create a workflow class instance, we will need to have a name for the workflow, a URI to name the resource that the workflow is capturing, and a workflow type.</p> <p>The url for the workflow will depend on what the workflow is actually doing. In general, its a good idea to make the URL a reference to a tagged code or repository in GitHub. This will require some disiplane on your process to ensure that you always have workflows that are commited and tagged in a repo.</p> <p>The workflow type is a controlled vocabulary.  You can create new workflow types using the standard APIs for adding terms.</p>"},{"location":"Notebooks/DerivaML%20Execution/#setup-for-a-ml-run","title":"Setup for a ML run\u00b6","text":""},{"location":"Notebooks/DerivaML%20Features/","title":"DerivaML Features","text":"In\u00a0[\u00a0]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[\u00a0]: Copied! <pre>import builtins\nfrom deriva.core.utils.globus_auth_utils import GlobusNativeLogin\nfrom deriva_ml import ColumnDefinition, BuiltinTypes, MLVocab, DerivaSystemColumns\nfrom deriva_ml.demo_catalog import create_demo_catalog, DemoML\nfrom deriva_ml import ExecutionConfiguration\nfrom IPython.display import display, Markdown, HTML\nimport itertools\nimport pandas as pd\nimport random\n</pre> import builtins from deriva.core.utils.globus_auth_utils import GlobusNativeLogin from deriva_ml import ColumnDefinition, BuiltinTypes, MLVocab, DerivaSystemColumns from deriva_ml.demo_catalog import create_demo_catalog, DemoML from deriva_ml import ExecutionConfiguration from IPython.display import display, Markdown, HTML import itertools import pandas as pd import random <p>Set the details for the catalog we want and authenticate to the server if needed.</p> In\u00a0[\u00a0]: Copied! <pre>hostname = 'localhost'\ndomain_schema = 'demo-schema'\n\ngnl = GlobusNativeLogin(host=hostname)\nif gnl.is_logged_in([hostname]):\n    print(\"You are already logged in.\")\nelse:\n    gnl.login([hostname], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n    print(\"Login Successful\")\n</pre> hostname = 'localhost' domain_schema = 'demo-schema'  gnl = GlobusNativeLogin(host=hostname) if gnl.is_logged_in([hostname]):     print(\"You are already logged in.\") else:     gnl.login([hostname], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)     print(\"Login Successful\") <p>Create a test catalog and get an instance of the DerivaML class.</p> In\u00a0[\u00a0]: Copied! <pre>test_catalog = create_demo_catalog(hostname, domain_schema)\nml_instance = DemoML(hostname, test_catalog.catalog_id)\ndisplay(f\"Created demo catalog at {hostname}:{test_catalog.catalog_id}\")\n</pre> test_catalog = create_demo_catalog(hostname, domain_schema) ml_instance = DemoML(hostname, test_catalog.catalog_id) display(f\"Created demo catalog at {hostname}:{test_catalog.catalog_id}\") In\u00a0[\u00a0]: Copied! <pre># Prerequisites for our feature, which will include a CV term and asset.\n\n# Create a vocabulary and add a term to it to use in our features.\nml_instance.create_vocabulary(\"SubjectHealth\", \"A vocab\")\nml_instance.add_term(\"SubjectHealth\", \"Sick\", description=\"The subject self reports that they are sick\")\nml_instance.add_term(\"SubjectHealth\", \"Well\", description=\"The subject self reports that they feel well\")\n\nml_instance.create_vocabulary(\"ImageQuality\", \"Controlled vocabulary for image quality\")\nml_instance.add_term(\"ImageQuality\", \"Good\", description=\"The image is good\")\nml_instance.add_term(\"ImageQuality\", \"Bad\", description=\"The image is bad\")\n\nbox_asset = ml_instance.create_asset(\"BoundingBox\", comment=\"A file that contains a cropped version of a image\")\n</pre> # Prerequisites for our feature, which will include a CV term and asset.  # Create a vocabulary and add a term to it to use in our features. ml_instance.create_vocabulary(\"SubjectHealth\", \"A vocab\") ml_instance.add_term(\"SubjectHealth\", \"Sick\", description=\"The subject self reports that they are sick\") ml_instance.add_term(\"SubjectHealth\", \"Well\", description=\"The subject self reports that they feel well\")  ml_instance.create_vocabulary(\"ImageQuality\", \"Controlled vocabulary for image quality\") ml_instance.add_term(\"ImageQuality\", \"Good\", description=\"The image is good\") ml_instance.add_term(\"ImageQuality\", \"Bad\", description=\"The image is bad\")  box_asset = ml_instance.create_asset(\"BoundingBox\", comment=\"A file that contains a cropped version of a image\") <p>We are now ready to create our new features. Each feature will be associated with a table, have a name, and then the set of values that define the feature. After we create the features, we can list the features associated with each table type that we have.</p> In\u00a0[\u00a0]: Copied! <pre>ml_instance.create_feature(\"Subject\", \"Health\",\n                                        terms=[\"SubjectHealth\"],\n                                        metadata=[ColumnDefinition(name='Scale', type=BuiltinTypes.int2, nullok=True)],\n                           optional=['Scale'])\n\nml_instance.create_feature('Image', 'BoundingBox', assets=[box_asset])\nml_instance.create_feature('Image', 'Quality', terms=[\"ImageQuality\"])\n\ndisplay(\n    [f'{f.target_table.name}:{f.feature_name}' for f in ml_instance.find_features(\"Subject\")],\n    [f'{f.target_table.name}:{f.feature_name}' for f in ml_instance.find_features(\"Image\")]\n)\n</pre> ml_instance.create_feature(\"Subject\", \"Health\",                                         terms=[\"SubjectHealth\"],                                         metadata=[ColumnDefinition(name='Scale', type=BuiltinTypes.int2, nullok=True)],                            optional=['Scale'])  ml_instance.create_feature('Image', 'BoundingBox', assets=[box_asset]) ml_instance.create_feature('Image', 'Quality', terms=[\"ImageQuality\"])  display(     [f'{f.target_table.name}:{f.feature_name}' for f in ml_instance.find_features(\"Subject\")],     [f'{f.target_table.name}:{f.feature_name}' for f in ml_instance.find_features(\"Image\")] ) <p>Now we can add some features to our images.  To streamline the creation of new feature, we create a class that is specific to the arguments required to create it.</p> In\u00a0[\u00a0]: Copied! <pre>ImageQualityFeature = ml_instance.feature_record_class(\"Image\", \"Quality\")\nImageBoundingboxFeature = ml_instance.feature_record_class(\"Image\", \"BoundingBox\")\nSubjectWellnessFeature= ml_instance.feature_record_class(\"Subject\", \"Health\")\n\ndisplay(\n    Markdown('### SubjectWellnessFeature'),\n    Markdown(f'* feature_columns: ' f'```{[c.name for c in SubjectWellnessFeature.feature_columns()]}```'),\n    Markdown(f'* required columns: ' f'```{[c.name  for c in SubjectWellnessFeature.feature_columns() if not c.nullok]}```'),\n    Markdown(f'* term columns: ' f'```{[c.name for c in SubjectWellnessFeature.term_columns()]}```'),\n    Markdown(f'* value columns: ' f'```{[c.name for c in SubjectWellnessFeature.value_columns()]}```'),\n    Markdown(f'* asset columns: ' f'```{[c.name for c in SubjectWellnessFeature.asset_columns()]}```'),\n\n    Markdown('### ImageQualityFeature'),\n    Markdown( f'* feature_columns:* ' f'```{[c.name for c in ImageQualityFeature.feature_columns()]}```'),\n    Markdown(f'*  required columns:* ' f'```{[c.name  for c in ImageQualityFeature.feature_columns() if not c.nullok]}```'),\n    Markdown(f'* term columns: * ' f'```{[c.name for c in ImageQualityFeature.term_columns()]}```'),\n    Markdown(f'* value columns: * ' f'```{[c.name for c in ImageQualityFeature.value_columns()]}```'),\n    Markdown(f'* asset columns: * ' f'```{[c.name for c in ImageQualityFeature.asset_columns()]}```'),\n\n    Markdown('### ImageBoundingboxFeature'),\n    Markdown( f'* feature_columns:* ' f'```{[c.name for c in ImageBoundingboxFeature.feature_columns()]}```'),\n    Markdown(f'* required columns:* ' f'```{[c.name  for c in ImageBoundingboxFeature.feature_columns() if not c.nullok]}```'),\n    Markdown( f'* term columns:* ' f'```{[c.name for c in ImageBoundingboxFeature.term_columns()]}```'),\n    Markdown( f'* value columns:* ' f'```{[c.name for c in ImageBoundingboxFeature.value_columns()]}```'),\n    Markdown( f'* asset columns:* ' f'```{[c.name for c in ImageBoundingboxFeature.asset_columns()]}```'),\n)\n</pre> ImageQualityFeature = ml_instance.feature_record_class(\"Image\", \"Quality\") ImageBoundingboxFeature = ml_instance.feature_record_class(\"Image\", \"BoundingBox\") SubjectWellnessFeature= ml_instance.feature_record_class(\"Subject\", \"Health\")  display(     Markdown('### SubjectWellnessFeature'),     Markdown(f'* feature_columns: ' f'```{[c.name for c in SubjectWellnessFeature.feature_columns()]}```'),     Markdown(f'* required columns: ' f'```{[c.name  for c in SubjectWellnessFeature.feature_columns() if not c.nullok]}```'),     Markdown(f'* term columns: ' f'```{[c.name for c in SubjectWellnessFeature.term_columns()]}```'),     Markdown(f'* value columns: ' f'```{[c.name for c in SubjectWellnessFeature.value_columns()]}```'),     Markdown(f'* asset columns: ' f'```{[c.name for c in SubjectWellnessFeature.asset_columns()]}```'),      Markdown('### ImageQualityFeature'),     Markdown( f'* feature_columns:* ' f'```{[c.name for c in ImageQualityFeature.feature_columns()]}```'),     Markdown(f'*  required columns:* ' f'```{[c.name  for c in ImageQualityFeature.feature_columns() if not c.nullok]}```'),     Markdown(f'* term columns: * ' f'```{[c.name for c in ImageQualityFeature.term_columns()]}```'),     Markdown(f'* value columns: * ' f'```{[c.name for c in ImageQualityFeature.value_columns()]}```'),     Markdown(f'* asset columns: * ' f'```{[c.name for c in ImageQualityFeature.asset_columns()]}```'),      Markdown('### ImageBoundingboxFeature'),     Markdown( f'* feature_columns:* ' f'```{[c.name for c in ImageBoundingboxFeature.feature_columns()]}```'),     Markdown(f'* required columns:* ' f'```{[c.name  for c in ImageBoundingboxFeature.feature_columns() if not c.nullok]}```'),     Markdown( f'* term columns:* ' f'```{[c.name for c in ImageBoundingboxFeature.term_columns()]}```'),     Markdown( f'* value columns:* ' f'```{[c.name for c in ImageBoundingboxFeature.value_columns()]}```'),     Markdown( f'* asset columns:* ' f'```{[c.name for c in ImageBoundingboxFeature.asset_columns()]}```'), ) In\u00a0[\u00a0]: Copied! <pre>ml_instance.add_term(MLVocab.workflow_type, \"Feature Notebook Workflow\", description=\"A Workflow that uses Deriva ML API\")\nml_instance.add_term(MLVocab.asset_type, \"API_Model\", description=\"Model for our Notebook workflow\")\n\n# Get the workflow for this notebook\nnotebook_workflow = ml_instance.create_workflow(\n    name=\"API Workflow\", \n    workflow_type=\"Feature Notebook Workflow\"\n)\n\nfeature_execution = ml_instance.create_execution(\n    ExecutionConfiguration(\n        workflow=notebook_workflow,\n        description=\"Our Sample Workflow instance\")\n)\n</pre> ml_instance.add_term(MLVocab.workflow_type, \"Feature Notebook Workflow\", description=\"A Workflow that uses Deriva ML API\") ml_instance.add_term(MLVocab.asset_type, \"API_Model\", description=\"Model for our Notebook workflow\")  # Get the workflow for this notebook notebook_workflow = ml_instance.create_workflow(     name=\"API Workflow\",      workflow_type=\"Feature Notebook Workflow\" )  feature_execution = ml_instance.create_execution(     ExecutionConfiguration(         workflow=notebook_workflow,         description=\"Our Sample Workflow instance\") ) In\u00a0[\u00a0]: Copied! <pre># Get the IDs of al of the things that we are going to want to attach features to.\nsubject_rids = [i['RID'] for i in ml_instance.domain_path.tables['Subject'].entities().fetch()]\nimage_rids = [i['RID'] for i in ml_instance.domain_path.tables['Image'].entities().fetch()]\n</pre> # Get the IDs of al of the things that we are going to want to attach features to. subject_rids = [i['RID'] for i in ml_instance.domain_path.tables['Subject'].entities().fetch()] image_rids = [i['RID'] for i in ml_instance.domain_path.tables['Image'].entities().fetch()] <p>Now that we have the list of objects that we want to add features to, we can define the sets of feature values we want to record and then add these features in the catalog.</p> In\u00a0[\u00a0]: Copied! <pre># Create a new set of images.  For fun, lets wrap this in an execution so we get status updates\nimage_bounding_box_feature_list = []\nfor cnt, image_rid in enumerate(image_rids):\n    bounding_box_file = feature_execution.asset_file_path(\"BoundingBox\", f\"box{cnt}.txt\")\n    with open(bounding_box_file, \"w\") as fp:\n        fp.write(f\"Hi there {cnt}\")\n    image_bounding_box_feature_list.append(   ImageBoundingboxFeature(Image=image_rid, BoundingBox=bounding_box_file)\n    )\n\nimage_quality_feature_list = [\n    ImageQualityFeature(\n        Image=image_rid,\n        ImageQuality=[\"Good\", \"Bad\"][random.randint(0, 1)],\n    )\n    for image_rid in image_rids\n]\n\nsubject_feature_list = [\n    SubjectWellnessFeature(\n        Subject=subject_rid,\n        SubjectHealth=[\"Well\", \"Sick\"][random.randint(0, 1)],\n        Scale=random.randint(1, 10),\n    )\n    for subject_rid in subject_rids\n]\n\nwith feature_execution.execute() as execution:\n    feature_execution.add_features(image_bounding_box_feature_list)\n    feature_execution.add_features(image_quality_feature_list)\n    feature_execution.add_features(subject_feature_list)\n\n# Upload all of the new assets that we have created during the execution.\nfeature_execution.upload_execution_outputs()\n</pre> # Create a new set of images.  For fun, lets wrap this in an execution so we get status updates image_bounding_box_feature_list = [] for cnt, image_rid in enumerate(image_rids):     bounding_box_file = feature_execution.asset_file_path(\"BoundingBox\", f\"box{cnt}.txt\")     with open(bounding_box_file, \"w\") as fp:         fp.write(f\"Hi there {cnt}\")     image_bounding_box_feature_list.append(   ImageBoundingboxFeature(Image=image_rid, BoundingBox=bounding_box_file)     )  image_quality_feature_list = [     ImageQualityFeature(         Image=image_rid,         ImageQuality=[\"Good\", \"Bad\"][random.randint(0, 1)],     )     for image_rid in image_rids ]  subject_feature_list = [     SubjectWellnessFeature(         Subject=subject_rid,         SubjectHealth=[\"Well\", \"Sick\"][random.randint(0, 1)],         Scale=random.randint(1, 10),     )     for subject_rid in subject_rids ]  with feature_execution.execute() as execution:     feature_execution.add_features(image_bounding_box_feature_list)     feature_execution.add_features(image_quality_feature_list)     feature_execution.add_features(subject_feature_list)  # Upload all of the new assets that we have created during the execution. feature_execution.upload_execution_outputs() In\u00a0[\u00a0]: Copied! <pre>display(\n    Markdown('### Wellness'),\n    pd.DataFrame(ml_instance.list_feature_values(\"Subject\", \"Health\")).drop(columns=DerivaSystemColumns + ['Feature_Name']),\n    Markdown('### Image Quality'),\n    pd.DataFrame(ml_instance.list_feature_values(\"Image\", \"Quality\")).drop(columns=DerivaSystemColumns + ['Feature_Name']),\n    Markdown('### BoundingBox'),\n    pd.DataFrame(ml_instance.list_feature_values(\"Image\", \"BoundingBox\")).drop(columns=DerivaSystemColumns + ['Feature_Name']),\n)\n</pre> display(     Markdown('### Wellness'),     pd.DataFrame(ml_instance.list_feature_values(\"Subject\", \"Health\")).drop(columns=DerivaSystemColumns + ['Feature_Name']),     Markdown('### Image Quality'),     pd.DataFrame(ml_instance.list_feature_values(\"Image\", \"Quality\")).drop(columns=DerivaSystemColumns + ['Feature_Name']),     Markdown('### BoundingBox'),     pd.DataFrame(ml_instance.list_feature_values(\"Image\", \"BoundingBox\")).drop(columns=DerivaSystemColumns + ['Feature_Name']), ) In\u00a0[\u00a0]: Copied! <pre>display(HTML(f'&lt;a href={ml_instance.chaise_url(\"Subject\")}&gt;Browse Subject Table&lt;/a&gt;'))\n</pre> display(HTML(f'Browse Subject Table')) In\u00a0[\u00a0]: Copied! <pre>test_catalog.delete_ermrest_catalog(really=True)\n</pre> test_catalog.delete_ermrest_catalog(really=True)"},{"location":"Notebooks/DerivaML%20Features/#derivaml-features","title":"DerivaML Features\u00b6","text":"<p>DerivaML is a class library built on the Deriva Scientific Asset management system that is designed to help simplify a number of the basic operations associated with building and testing ML libraries based on common toolkits such as TensorFlow.  This notebook reviews the basic features of the DerivaML library.</p> <p>In DerivaML, \"features\" are the way we attach values to objects in the catalog. A feature could be a computed value that serves as input to a ML model, or it could be a label, that is the result of running a model.  A feature can be a controlled vocabulary term, an asset, or a value.</p> <p>Each feature in the catalog is distinguished by the name of the feature, the identity of the object that the feature is being attached to, and the execution RID of the process that generated the feature value</p>"},{"location":"Notebooks/DerivaML%20Features/#set-up-deriva-for-test-case","title":"Set up Deriva for test case\u00b6","text":""},{"location":"Notebooks/DerivaML%20Features/#define-features","title":"Define Features\u00b6","text":"<p>A feature is a set of values that are attached to a table in the DerivaML catalog. Instances of features are distinguished from one another by the ID of the execution that produced the feature value. The execution could be the result of a program, or it could be a manual process by which a person defines a set of values</p> <p>To create a new feature, we need to know the name of the feature, the table to which it is attached, and the set of values that make up the feature.  The values could be terms from a controlled vocabulary, a set of one or more file based assets, or other values, such as integers, or strings. However, use of strings outside of controlled vocabularies is discouraged.</p> <p>For our example, we are going to define three features.  Two of them will use values from a controlled vocabulary, which we need to create.  The third feature will consist of a file whose contents we will generate.  To start, we will need to create the controlled vocabularies, and create an asset table for the feature values.</p>"},{"location":"Notebooks/DerivaML%20Features/#add-feature-values","title":"Add feature values\u00b6","text":"<p>Now using feature classes, we can create some instances of the feature and add them.  We must have a execution_rid in order to define the feature. In our example, we will assume that the execution that calculates the feature values will use a model file to configure it, so ww will need to create and upload the file before we can start the execution.</p>"},{"location":"Notebooks/DerivaML%20Ingest/","title":"DerivaML Ingest","text":"In\u00a0[\u00a0]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[\u00a0]: Copied! <pre>from deriva.core.utils.globus_auth_utils import GlobusNativeLogin\nfrom deriva_ml.demo_catalog import create_demo_catalog, DemoML\nfrom deriva_ml import MLVocab, ExecutionConfiguration, Workflow, DerivaSystemColumns, VersionPart, DatasetSpec, FileSpec\nfrom IPython.display import display, Markdown, HTML, JSON\n</pre> from deriva.core.utils.globus_auth_utils import GlobusNativeLogin from deriva_ml.demo_catalog import create_demo_catalog, DemoML from deriva_ml import MLVocab, ExecutionConfiguration, Workflow, DerivaSystemColumns, VersionPart, DatasetSpec, FileSpec from IPython.display import display, Markdown, HTML, JSON <p>Set the details for the catalog we want and authenticate to the server if needed.</p> In\u00a0[\u00a0]: parameters Copied! <pre>hostname = 'dev.eye-ai.org'\ndomain_schema = 'demo-schema'\n</pre> hostname = 'dev.eye-ai.org' domain_schema = 'demo-schema' In\u00a0[\u00a0]: Copied! <pre>gnl = GlobusNativeLogin(host=hostname)\nif gnl.is_logged_in([hostname]):\n    print(\"You are already logged in.\")\nelse:\n    gnl.login([hostname], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n    print(\"Login Successful\")\n</pre> gnl = GlobusNativeLogin(host=hostname) if gnl.is_logged_in([hostname]):     print(\"You are already logged in.\") else:     gnl.login([hostname], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)     print(\"Login Successful\")  <p>Create a test catalog and get an instance of the DemoML class.</p> In\u00a0[\u00a0]: Copied! <pre>test_catalog = create_demo_catalog(hostname, domain_schema)\nml_instance = DemoML(hostname, test_catalog.catalog_id, use_minid=False)\n</pre> test_catalog = create_demo_catalog(hostname, domain_schema) ml_instance = DemoML(hostname, test_catalog.catalog_id, use_minid=False) <p>Now that we have configured our datasets, we need to identify the dataset types so we can distinguish between them.</p> <p>Now create datasets and populate with elements from the test catalogs.</p> In\u00a0[\u00a0]: Copied! <pre>ml_instance.add_term(MLVocab.workflow_type, \"Data Ingest Notebook\", description=\"A Workflow that Ingests data into a catalog\")\n\n# Now lets create model configuration for our program.\napi_workflow = ml_instance.create_workflow(\n    name=\"Data Ingest\",\n    workflow_type=\"Data Ingest Notebook\",\n    description=\"An example of how to use the file table\"\n)\n\ningest_execution = ml_instance.create_execution(\n    ExecutionConfiguration(\n        workflow=api_workflow,\n        description=\"Our Sample Workflow instance\")\n)\n</pre> ml_instance.add_term(MLVocab.workflow_type, \"Data Ingest Notebook\", description=\"A Workflow that Ingests data into a catalog\")  # Now lets create model configuration for our program. api_workflow = ml_instance.create_workflow(     name=\"Data Ingest\",     workflow_type=\"Data Ingest Notebook\",     description=\"An example of how to use the file table\" )  ingest_execution = ml_instance.create_execution(     ExecutionConfiguration(         workflow=api_workflow,         description=\"Our Sample Workflow instance\") ) In\u00a0[\u00a0]: Copied! <pre>with ingest_execution.execute() as exe:\n    files = FileSpec.create_filespecs('/Users/carl/Repos/Projects/deriva-ml/src', 'my stuff')\n    exe.add_files(files, file_types=[])\n</pre> with ingest_execution.execute() as exe:     files = FileSpec.create_filespecs('/Users/carl/Repos/Projects/deriva-ml/src', 'my stuff')     exe.add_files(files, file_types=[]) <p>And now that we have defined some datasets, we can add elements of the appropriate type to them.  We can see what is in our new datasets by listing the dataset members.</p> In\u00a0[\u00a0]: Copied! <pre># Get list of subjects and images from the catalog using the DataPath API.\nml_instance.list_files()\n</pre> # Get list of subjects and images from the catalog using the DataPath API. ml_instance.list_files() <p>For ths example, lets partition the data based on the name of the subject.  Of course in real examples, we would do a more complex analysis in deciding what subset goes into each data set.</p> In\u00a0[\u00a0]: Copied! <pre>display(HTML(f'&lt;a href={ml_instance.chaise_url(\"Dataset\")}&gt;Browse Datasets&lt;/a&gt;'))\n</pre> display(HTML(f'Browse Datasets')) In\u00a0[\u00a0]: Copied! <pre>test_catalog.delete_ermrest_catalog(really=True)\n</pre> test_catalog.delete_ermrest_catalog(really=True)"},{"location":"Notebooks/DerivaML%20Ingest/#derivaml-ingest","title":"DerivaML Ingest\u00b6","text":"<p>DerivaML is a class library built on the Deriva Scientific Asset management system that is designed to help simplify a number of the basic operations associated with building and testing ML libraries based on common toolkits such as TensorFlow.  This notebook reviews the basic features of the DerivaML library.</p>"},{"location":"Notebooks/DerivaML%20Ingest/#set-up-derivaml-for-test-case","title":"Set up DerivaML  for test case\u00b6","text":""},{"location":"Notebooks/DerivaML%20Ingest/#configure-derivaml-datasets","title":"Configure DerivaML Datasets\u00b6","text":"<p>In Deriva-ML a dataset is used to aggregate instances of entities.  However, before we can create any datasets, we must configure Deriva-ML for the specifics of the datasets.  The first stp is we need to tell Deriva-ML what types of use defined objects can be associated with a dataset.</p> <p>Note that out of the box, Deriva-ML is configured to allow datasets to contained dataset (i.e. nested datasets), so we don't need to do anything for that specific configuration.</p>"},{"location":"Notebooks/DerivaML%20Vocabulary/","title":"DerivaML Vocabulary","text":"In\u00a0[1]: Copied! <pre>from IPython.display import display, Markdown, HTML\nimport pandas as pd\nfrom deriva.core.utils.globus_auth_utils import GlobusNativeLogin\nfrom deriva_ml.demo_catalog import create_demo_catalog, DemoML\nfrom deriva_ml import MLVocab\n</pre> from IPython.display import display, Markdown, HTML import pandas as pd from deriva.core.utils.globus_auth_utils import GlobusNativeLogin from deriva_ml.demo_catalog import create_demo_catalog, DemoML from deriva_ml import MLVocab In\u00a0[2]: Copied! <pre>hostname = 'dev.eye-ai.org'   # This needs to be changed.\n\ngnl = GlobusNativeLogin(host=hostname)\nif gnl.is_logged_in([hostname]):\n    print(\"You are already logged in.\")\nelse:\n    gnl.login([hostname], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n    print(\"Login Successful\")\n</pre> hostname = 'dev.eye-ai.org'   # This needs to be changed.  gnl = GlobusNativeLogin(host=hostname) if gnl.is_logged_in([hostname]):     print(\"You are already logged in.\") else:     gnl.login([hostname], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)     print(\"Login Successful\") <pre>You are already logged in.\n</pre> In\u00a0[3]: Copied! <pre>test_catalog = create_demo_catalog(hostname)\nml_instance = DemoML(hostname, test_catalog.catalog_id)\n</pre> test_catalog = create_demo_catalog(hostname) ml_instance = DemoML(hostname, test_catalog.catalog_id) <pre>2025-06-06 14:12:47,103 - deriva_ml.WARNING - File /Users/carl/Repos/Projects/deriva-ml/docs/Notebooks/DerivaML Vocabulary.ipynb has been modified since last commit. Consider commiting before executing\n</pre> <p>Execution RID: https://dev.eye-ai.org/id/2060/3SC@33D-VDH5-6N1W</p> In\u00a0[4]: Copied! <pre>ml_instance.find_vocabularies()\n</pre> ml_instance.find_vocabularies() <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[4], line 1\n----&gt; 1 ml_instance.find_vocabularies()\n\nAttributeError: 'DemoML' object has no attribute 'find_vocabularies'</pre> <p>Let's look at the contents of one of the predefined vocabularies in the DerivaML library.  We can make this look nicer with a Panda. Many of the datatypes in DerivaML are represented by Pydantic data classes.  These have a number of methods that can make it easy to operate on them.  The one we are going to use here is <code>model_dump()</code>, which converts a dataclass into a dictionary.</p> In\u00a0[\u00a0]: Copied! <pre>display(\n    Markdown(f\"#### Contents of controlled vocabulary {MLVocab.execution_metadata_type}\"),\n    pd.DataFrame([v.model_dump() for v in ml_instance.list_vocabulary_terms(MLVocab.execution_metadata_type)])\n)\n</pre> display(     Markdown(f\"#### Contents of controlled vocabulary {MLVocab.execution_metadata_type}\"),     pd.DataFrame([v.model_dump() for v in ml_instance.list_vocabulary_terms(MLVocab.execution_metadata_type)]) ) In\u00a0[\u00a0]: Copied! <pre>ml_instance.create_vocabulary(\"My term set\", comment=\"Terms to use for generating tests\")\n</pre> ml_instance.create_vocabulary(\"My term set\", comment=\"Terms to use for generating tests\") In\u00a0[\u00a0]: Copied! <pre>ml_instance.find_vocabularies()\n</pre> ml_instance.find_vocabularies() In\u00a0[\u00a0]: Copied! <pre>for i in range(5):\n    ml_instance.add_term(\"My term set\", f\"Term{i}\", description=f\"My term {i}\", synonyms=[f\"t{i}\", f\"T{i}\"])\n</pre> for i in range(5):     ml_instance.add_term(\"My term set\", f\"Term{i}\", description=f\"My term {i}\", synonyms=[f\"t{i}\", f\"T{i}\"]) In\u00a0[2]: Copied! <pre>display(\n    Markdown('#### Contents of controlled vocabulary \"My term set'),\n    pd.DataFrame([v.model_dump() for v in ml_instance.list_vocabulary_terms(\"My term set\")])\n)\n</pre> display(     Markdown('#### Contents of controlled vocabulary \"My term set'),     pd.DataFrame([v.model_dump() for v in ml_instance.list_vocabulary_terms(\"My term set\")]) ) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[2], line 3\n      1 display(\n      2     Markdown('#### Contents of controlled vocabulary \"My term set'),\n----&gt; 3     pd.DataFrame([v.model_dump() for v in ml_instance.list_vocabulary_terms(\"My term set\")])\n      4 )\n\nNameError: name 'ml_instance' is not defined</pre> In\u00a0[\u00a0]: Copied! <pre>display(\n    ml_instance.lookup_term(\"My term set\", \"Term0\"),\n    ml_instance.lookup_term(\"My term set\", \"Term2\"),\n    ml_instance.lookup_term('My term set', 'T3'),\n)\n</pre> display(     ml_instance.lookup_term(\"My term set\", \"Term0\"),     ml_instance.lookup_term(\"My term set\", \"Term2\"),     ml_instance.lookup_term('My term set', 'T3'), ) In\u00a0[\u00a0]: Copied! <pre>display(HTML(f'&lt;a href={ml_instance.chaise_url(\"My term set\")}&gt;Browse vocabulary: My term set&lt;/a&gt;'))\n</pre> display(HTML(f'Browse vocabulary: My term set')) In\u00a0[\u00a0]: Copied! <pre>test_catalog.delete_ermrest_catalog(really=True)\n</pre> test_catalog.delete_ermrest_catalog(really=True)"},{"location":"Notebooks/DerivaML%20Vocabulary/#derivaml-vocabulary","title":"DerivaML Vocabulary\u00b6","text":"<p>DerivaML is a class library built on the Deriva Scientific Asset management system that is designed to help simplify a number of the basic operations associated with building and testing ML libraries based on common toolkits such as TensorFlow.  This notebook reviews the basic features of the DerivaML library.</p> <p>A core aspect of DerivaML is the extensive use of controlled vocabulary terms.  A vocabulary term may be something defined outside of the study, for example from an ontology like Uberon or Schema.org, or it can be a term that is defined and used locally by the ML team.  The purpose of using controlled vocabulary is that it makes it easier to find data and can help ensure that proper communication is taking place between members of the ML team.</p>"},{"location":"Notebooks/DerivaML%20Vocabulary/#preliminaries","title":"Preliminaries.\u00b6","text":"<p>To start, we will do some preliminaries, loading needed modules and making sure we are logged into the DerivaML server.</p>"},{"location":"Notebooks/DerivaML%20Vocabulary/#create-a-test-catalog","title":"Create a test catalog.\u00b6","text":"<p>Create a test catalog and get an instance of the DerivaML class.  This will take around 30 seconds, so be patient.</p>"},{"location":"Notebooks/DerivaML%20Vocabulary/#explore-existing-vocabularies","title":"Explore existing vocabularies.\u00b6","text":"<p>Get a list of all the currently defined controlled vocabularies</p>"},{"location":"Notebooks/DerivaML%20Vocabulary/#creating-a-new-controlled-vocabulary","title":"Creating a new controlled vocabulary.\u00b6","text":"<p>Now let's create a new controlled vocabulary to house terms that are specific to the problem we are working on.</p>"},{"location":"Notebooks/DerivaML%20Vocabulary/#adding-terms","title":"Adding terms\u00b6","text":"<p>Given our new controlled vocabulary, we can add terms to it.  A term has a name, that should uniquely identify it within the vocabulary, a description of what the term means, and finally a list of synonyms. Each term is assigned a resource identifier (RID) by the deriva platform.  There are other additional features of terms that facilitate integration from preexisting vocabularies that are beyond the scope of this notebook.  You can look at the class documentation for these details.</p>"},{"location":"Notebooks/DerivaML%20Vocabulary/#looking-up-terms","title":"Looking up terms\u00b6","text":"<p>We can also look up individual terms, either by their name, or by a synonym</p>"},{"location":"Notebooks/DerivaML%20Vocabulary/#browsing-terms-in-the-user-interface","title":"Browsing terms in the user interface\u00b6","text":"<p>All the terms we define in the API are of course visible via the Chaise use interface.</p>"},{"location":"code-docs/dataset/","title":"Dataset","text":""},{"location":"code-docs/dataset/#deriva_ml.dataset.Dataset","title":"Dataset","text":"<p>Manages dataset operations in a Deriva catalog.</p> <p>The Dataset class provides functionality for creating, modifying, and tracking datasets in a Deriva catalog. It handles versioning, relationships between datasets, and data export.</p> <p>Attributes:</p> Name Type Description <code>dataset_table</code> <code>Table</code> <p>ERMrest table storing dataset information.</p> <code>_model</code> <code>DerivaModel</code> <p>Catalog model instance.</p> <code>_ml_schema</code> <code>str</code> <p>Schema name for ML-specific tables.</p> <code>_cache_dir</code> <code>Path</code> <p>Directory for caching downloaded datasets.</p> <code>_working_dir</code> <code>Path</code> <p>Directory for working data.</p> <code>_use_minid</code> <code>bool</code> <p>Whether to use MINID service for dataset identification.</p> Note <p>This class is typically used as a base class, with its methods accessed through DerivaML class instances rather than directly.</p> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>class Dataset:\n    \"\"\"Manages dataset operations in a Deriva catalog.\n\n    The Dataset class provides functionality for creating, modifying, and tracking datasets\n    in a Deriva catalog. It handles versioning, relationships between datasets, and data export.\n\n    Attributes:\n        dataset_table (Table): ERMrest table storing dataset information.\n        _model (DerivaModel): Catalog model instance.\n        _ml_schema (str): Schema name for ML-specific tables.\n        _cache_dir (Path): Directory for caching downloaded datasets.\n        _working_dir (Path): Directory for working data.\n        _use_minid (bool): Whether to use MINID service for dataset identification.\n\n    Note:\n        This class is typically used as a base class, with its methods accessed through\n        DerivaML class instances rather than directly.\n    \"\"\"\n\n    _Logger = logging.getLogger(\"deriva_ml\")\n\n    def __init__(\n        self,\n        model: DerivaModel,\n        cache_dir: Path,\n        working_dir: Path,\n        use_minid: bool = True,\n    ):\n        \"\"\"Initializes a Dataset instance.\n\n        Args:\n            model: DerivaModel instance representing the catalog.\n            cache_dir: Directory path for caching downloaded datasets.\n            working_dir: Directory path for working data.\n            use_minid: Whether to use MINID service for dataset identification.\n        \"\"\"\n        self._model = model\n        self._ml_schema = ML_SCHEMA\n        self._cache_dir = cache_dir\n        self._working_dir = working_dir\n        self._logger = logging.getLogger(\"deriva_ml\")\n        self._use_minid = use_minid\n\n    @property\n    def _dataset_table(self):\n        return self._model.schemas[self._ml_schema].tables[\"Dataset\"]\n\n    def _is_dataset_rid(self, dataset_rid: RID, deleted: bool = False) -&gt; bool:\n        try:\n            rid_info = self._model.catalog.resolve_rid(dataset_rid, self._model.model)\n        except KeyError as _e:\n            raise DerivaMLException(f\"Invalid RID {dataset_rid}\")\n        if rid_info.table != self._dataset_table:\n            return False\n        elif deleted:\n            # Got a dataset rid. Now check to see if its deleted or not.\n            return True\n        else:\n            return not list(rid_info.datapath.entities().fetch())[0][\"Deleted\"]\n\n    def _insert_dataset_versions(\n        self,\n        dataset_list: list[DatasetSpec],\n        description: str | None = \"\",\n        execution_rid: RID | None = None,\n    ) -&gt; None:\n        schema_path = self._model.catalog.getPathBuilder().schemas[self._ml_schema]\n        # determine snapshot after changes were made\n\n        # Construct version records for insert\n        version_records = schema_path.tables[\"Dataset_Version\"].insert(\n            [\n                {\n                    \"Dataset\": dataset.rid,\n                    \"Version\": str(dataset.version),\n                    \"Description\": description,\n                    \"Execution\": execution_rid,\n                }\n                for dataset in dataset_list\n            ]\n        )\n        version_records = list(version_records)\n        snap = self._model.catalog.get(\"/\").json()[\"snaptime\"]\n        schema_path.tables[\"Dataset_Version\"].update(\n            [{\"RID\": v[\"RID\"], \"Dataset\": v[\"Dataset\"], \"Snapshot\": snap} for v in version_records]\n        )\n\n        # And update the dataset records.\n        schema_path.tables[\"Dataset\"].update([{\"Version\": v[\"RID\"], \"RID\": v[\"Dataset\"]} for v in version_records])\n\n    def _bootstrap_versions(self):\n        datasets = [ds[\"RID\"] for ds in self.find_datasets()]\n        ds_version = [\n            {\n                \"Dataset\": d,\n                \"Version\": \"0.1.0\",\n                \"Description\": \"Dataset at the time of conversion to versioned datasets\",\n            }\n            for d in datasets\n        ]\n        schema_path = self._model.catalog.getPathBuilder().schemas[self._ml_schema]\n        version_path = schema_path.tables[\"Dataset_Version\"]\n        dataset_path = schema_path.tables[\"Dataset\"]\n        history = list(version_path.insert(ds_version))\n        dataset_versions = [{\"RID\": h[\"Dataset\"], \"Version\": h[\"Version\"]} for h in history]\n        dataset_path.update(dataset_versions)\n\n    def _synchronize_dataset_versions(self):\n        datasets = [ds[\"RID\"] for ds in self.find_datasets()]\n        for ds in datasets:\n            self.dataset_version(ds)\n        schema_path = self._model.catalog.getPathBuilder().schemas[self._ml_schema]\n        dataset_version_path = schema_path.tables[\"Dataset_Version\"]\n        # Get the maximum version number for each dataset.\n        versions = {}\n        for v in dataset_version_path.entities().fetch():\n            if v[\"Version\"] &gt; versions.get(\"Dataset\", DatasetVersion(0, 0, 0)):\n                versions[v[\"Dataset\"]] = v\n        dataset_path = schema_path.tables[\"Dataset\"]\n\n        dataset_path.update([{\"RID\": dataset, \"Version\": version[\"RID\"]} for dataset, version in versions.items()])\n\n    def _set_version_snapshot(self):\n        \"\"\"Update the Snapshot column of the Dataset_Version table to the correct time.\"\"\"\n        dataset_version_path = self._model.catalog.getPathBuilder().schemas[self._ml_schema].tables[\"Dataset_Version\"]\n        versions = dataset_version_path.entities().fetch()\n        dataset_version_path.update(\n            [{\"RID\": h[\"RID\"], \"Snapshot\": iso_to_snap(h[\"RCT\"])} for h in versions if not h[\"Snapshot\"]]\n        )\n\n    def dataset_history(self, dataset_rid: RID) -&gt; list[DatasetHistory]:\n        \"\"\"Retrieves the version history of a dataset.\n\n        Returns a chronological list of dataset versions, including their version numbers,\n        creation times, and associated metadata.\n\n        Args:\n            dataset_rid: Resource Identifier of the dataset.\n\n        Returns:\n            list[DatasetHistory]: List of history entries, each containing:\n                - dataset_version: Version number (major.minor.patch)\n                - minid: Minimal Viable Identifier\n                - snapshot: Catalog snapshot time\n                - dataset_rid: Dataset Resource Identifier\n                - version_rid: Version Resource Identifier\n                - description: Version description\n                - execution_rid: Associated execution RID\n\n        Raises:\n            DerivaMLException: If dataset_rid is not a valid dataset RID.\n\n        Example:\n            &gt;&gt;&gt; history = ml.dataset_history(\"1-abc123\")\n            &gt;&gt;&gt; for entry in history:\n            ...     print(f\"Version {entry.dataset_version}: {entry.description}\")\n        \"\"\"\n\n        if not self._is_dataset_rid(dataset_rid):\n            raise DerivaMLException(f\"RID is not for a data set: {dataset_rid}\")\n        version_path = self._model.catalog.getPathBuilder().schemas[self._ml_schema].tables[\"Dataset_Version\"]\n        return [\n            DatasetHistory(\n                dataset_version=DatasetVersion.parse(v[\"Version\"]),\n                minid=v[\"Minid\"],\n                snapshot=v[\"Snapshot\"],\n                dataset_rid=dataset_rid,\n                version_rid=v[\"RID\"],\n                description=v[\"Description\"],\n                execution_rid=v[\"Execution\"],\n            )\n            for v in version_path.filter(version_path.Dataset == dataset_rid).entities().fetch()\n        ]\n\n    @validate_call\n    def dataset_version(self, dataset_rid: RID) -&gt; DatasetVersion:\n        \"\"\"Retrieve the current version of the specified dataset_table.\n\n        Given a rid, return the most recent version of the dataset. It is important to remember that this version\n        captures the state of the catalog at the time the version was created, not the current state of the catalog.\n        This means that its possible that the values associated with an object in the catalog may be different\n        from the values of that object in the dataset.\n\n        Args:\n            dataset_rid: The RID of the dataset to retrieve the version for.\n\n        Returns:\n            A tuple with the semantic version of the dataset_table.\n        \"\"\"\n        history = self.dataset_history(dataset_rid)\n        if not history:\n            return DatasetVersion(0, 1, 0)\n        else:\n            # Ensure we return a DatasetVersion, not a string\n            versions = [h.dataset_version for h in history]\n            return max(versions) if versions else DatasetVersion(0, 1, 0)\n\n    def _build_dataset_graph(self, dataset_rid: RID) -&gt; Iterable[RID]:\n        ts: TopologicalSorter = TopologicalSorter()\n        self._build_dataset_graph_1(dataset_rid, ts, set())\n        return ts.static_order()\n\n    def _build_dataset_graph_1(self, dataset_rid: RID, ts: TopologicalSorter, visited) -&gt; None:\n        \"\"\"Use topological sort to return bottom up list of nested datasets\"\"\"\n        ts.add(dataset_rid)\n        if dataset_rid not in visited:\n            visited.add(dataset_rid)\n            children = self.list_dataset_children(dataset_rid=dataset_rid)\n            parents = self.list_dataset_parents(dataset_rid=dataset_rid)\n            for parent in parents:\n                # Convert string to RID type\n                self._build_dataset_graph_1(RID(parent), ts, visited)\n            for child in children:\n                self._build_dataset_graph_1(child, ts, visited)\n\n    @validate_call(config=ConfigDict(arbitrary_types_allowed=True))\n    def increment_dataset_version(\n        self,\n        dataset_rid: RID,\n        component: VersionPart,\n        description: str | None = \"\",\n        execution_rid: RID | None = None,\n    ) -&gt; DatasetVersion:\n        \"\"\"Increments a dataset's version number.\n\n        Creates a new version of the dataset by incrementing the specified version component\n        (major, minor, or patch). The new version is recorded with an optional description\n        and execution reference.\n\n        Args:\n            dataset_rid: Resource Identifier of the dataset to version.\n            component: Which version component to increment ('major', 'minor', or 'patch').\n            description: Optional description of the changes in this version.\n            execution_rid: Optional execution RID to associate with this version.\n\n        Returns:\n            DatasetVersion: The new version number.\n\n        Raises:\n            DerivaMLException: If dataset_rid is invalid or version increment fails.\n\n        Example:\n            &gt;&gt;&gt; new_version = ml.increment_dataset_version(\n            ...     dataset_rid=\"1-abc123\",\n            ...     component=\"minor\",\n            ...     description=\"Added new samples\"\n            ... )\n            &gt;&gt;&gt; print(f\"New version: {new_version}\")  # e.g., \"1.2.0\"\n        \"\"\"\n\n        # Find all the datasets that are reachable from this dataset and determine their new version numbers.\n        related_datasets = list(self._build_dataset_graph(dataset_rid=dataset_rid))\n        version_update_list = [\n            DatasetSpec(\n                rid=ds_rid,\n                version=self.dataset_version(ds_rid).increment_version(component),\n            )\n            for ds_rid in related_datasets\n        ]\n        self._insert_dataset_versions(version_update_list, description=description, execution_rid=execution_rid)\n        return next((d.version for d in version_update_list if d.rid == dataset_rid))\n\n    @validate_call(config=ConfigDict(arbitrary_types_allowed=True))\n    def create_dataset(\n        self,\n        dataset_types: str | list[str] | None = None,\n        description: str = \"\",\n        execution_rid: RID | None = None,\n        version: DatasetVersion | None = None,\n    ) -&gt; RID:\n        \"\"\"Creates a new dataset in the catalog.\n\n        Creates a dataset with specified types and description. The dataset can be associated\n        with an execution and initialized with a specific version.\n\n        Args:\n            dataset_types: One or more dataset type terms from Dataset_Type vocabulary.\n            description: Description of the dataset's purpose and contents.\n            execution_rid: Optional execution RID to associate with dataset creation.\n            version: Optional initial version number. Defaults to 0.1.0.\n\n        Returns:\n            RID: Resource Identifier of the newly created dataset.\n\n        Raises:\n            DerivaMLException: If dataset_types are invalid or creation fails.\n\n        Example:\n            &gt;&gt;&gt; rid = ml.create_dataset(\n            ...     dataset_types=[\"experiment\", \"raw_data\"],\n            ...     description=\"RNA sequencing experiment data\",\n            ...     version=DatasetVersion(1, 0, 0)\n            ... )\n        \"\"\"\n\n        version = version or DatasetVersion(0, 1, 0)\n        dataset_types = dataset_types or []\n\n        type_path = self._model.catalog.getPathBuilder().schemas[self._ml_schema].tables[MLVocab.dataset_type.value]\n        defined_types = list(type_path.entities().fetch())\n\n        def check_dataset_type(dtype: str) -&gt; bool:\n            for term in defined_types:\n                if dtype == term[\"Name\"] or (term[\"Synonyms\"] and ds_type in term[\"Synonyms\"]):\n                    return True\n            return False\n\n        # Create the entry for the new dataset_table and get its RID.\n        ds_types = [dataset_types] if isinstance(dataset_types, str) else dataset_types\n        pb = self._model.catalog.getPathBuilder()\n        for ds_type in ds_types:\n            if not check_dataset_type(ds_type):\n                raise DerivaMLException(\"Dataset type must be a vocabulary term.\")\n        dataset_table_path = pb.schemas[self._dataset_table.schema.name].tables[self._dataset_table.name]\n        dataset_rid = dataset_table_path.insert(\n            [\n                {\n                    \"Description\": description,\n                    \"Deleted\": False,\n                }\n            ]\n        )[0][\"RID\"]\n\n        # Get the name of the association table between dataset_table and dataset_type.\n        associations = list(self._model.schemas[self._ml_schema].tables[MLVocab.dataset_type].find_associations())\n        atable = associations[0].name if associations else None\n        pb.schemas[self._ml_schema].tables[atable].insert(\n            [{MLVocab.dataset_type: ds_type, \"Dataset\": dataset_rid} for ds_type in ds_types]\n        )\n        if execution_rid is not None:\n            pb.schemas[self._ml_schema].Dataset_Execution.insert([{\"Dataset\": dataset_rid, \"Execution\": execution_rid}])\n        self._insert_dataset_versions(\n            [DatasetSpec(rid=dataset_rid, version=version)],\n            execution_rid=execution_rid,\n            description=\"Initial dataset creation.\",\n        )\n        return dataset_rid\n\n    @validate_call\n    def delete_dataset(self, dataset_rid: RID, recurse: bool = False) -&gt; None:\n        \"\"\"Delete a dataset_table from the catalog.\n\n        Args:\n            dataset_rid: RID of the dataset_table to delete.\n            recurse: If True, delete the dataset_table along with any nested datasets. (Default value = False)\n        \"\"\"\n        # Get association table entries for this dataset_table\n        # Delete association table entries\n        if not self._is_dataset_rid(dataset_rid):\n            raise DerivaMLException(\"Dataset_rid is not a dataset.\")\n\n        if parents := self.list_dataset_parents(dataset_rid):\n            raise DerivaMLException(f'Dataset_rid \"{dataset_rid}\" is in a nested dataset: {parents}.')\n\n        pb = self._model.catalog.getPathBuilder()\n        dataset_path = pb.schemas[self._dataset_table.schema.name].tables[self._dataset_table.name]\n\n        rid_list = [dataset_rid] + (self.list_dataset_children(dataset_rid=dataset_rid) if recurse else [])\n        dataset_path.update([{\"RID\": r, \"Deleted\": True} for r in rid_list])\n\n    def find_datasets(self, deleted: bool = False) -&gt; Iterable[dict[str, Any]]:\n        \"\"\"Returns a list of currently available datasets.\n\n        Arguments:\n            deleted: If True, included the datasets that have been deleted.\n\n        Returns:\n             list of currently available datasets.\n        \"\"\"\n        # Get datapath to all the tables we will need: Dataset, DatasetType and the association table.\n        pb = self._model.catalog.getPathBuilder()\n        dataset_path = pb.schemas[self._dataset_table.schema.name].tables[self._dataset_table.name]\n        associations = list(self._model.schemas[self._ml_schema].tables[MLVocab.dataset_type].find_associations())\n        atable = associations[0].name if associations else None\n        ml_path = pb.schemas[self._ml_schema]\n        atable_path = ml_path.tables[atable]\n\n        if deleted:\n            filtered_path = dataset_path\n        else:\n            filtered_path = dataset_path.filter(\n                (dataset_path.Deleted == False) | (dataset_path.Deleted == None)  # noqa: E711, E712\n            )\n\n        # Get a list of all the dataset_type values associated with this dataset_table.\n        datasets = []\n        for dataset in filtered_path.entities().fetch():\n            ds_types = (\n                atable_path.filter(atable_path.Dataset == dataset[\"RID\"]).attributes(atable_path.Dataset_Type).fetch()\n            )\n            datasets.append(dataset | {MLVocab.dataset_type: [ds[MLVocab.dataset_type] for ds in ds_types]})\n        return datasets\n\n    def list_dataset_element_types(self) -&gt; Iterable[Table]:\n        \"\"\"List the types of entities that can be added to a dataset_table.\n\n        Returns:\n          :return: An iterable of Table objects that can be included as an element of a dataset_table.\n        \"\"\"\n\n        def domain_table(table: Table) -&gt; bool:\n            return table.schema.name == self._model.domain_schema or table.name == self._dataset_table.name\n\n        return [t for a in self._dataset_table.find_associations() if domain_table(t := a.other_fkeys.pop().pk_table)]\n\n    @validate_call(config=ConfigDict(arbitrary_types_allowed=True))\n    def add_dataset_element_type(self, element: str | Table) -&gt; Table:\n        \"\"\"A dataset_table is a heterogeneous collection of objects, each of which comes from a different table. This\n        routine makes it possible to add objects from the specified table to a dataset_table.\n\n        Args:\n            element: Name of the table or table object that is to be added to the dataset_table.\n\n        Returns:\n            The table object that was added to the dataset_table.\n        \"\"\"\n        # Add table to map\n        element_table = self._model.name_to_table(element)\n        atable_def = Table.define_association([self._dataset_table, element_table])\n        try:\n            table = self._model.schemas[self._model.domain_schema].create_table(atable_def)\n        except ValueError as e:\n            if \"already exists\" in str(e):\n                table = self._model.name_to_table(atable_def[\"table_name\"])\n            else:\n                raise e\n\n        # self.model = self.catalog.getCatalogModel()\n        self._dataset_table.annotations.update(self._generate_dataset_download_annotations())\n        self._model.model.apply()\n        return table\n\n    # @validate_call\n    def list_dataset_members(\n        self, dataset_rid: RID, recurse: bool = False, limit: int | None = None\n    ) -&gt; dict[str, list[dict[str, Any]]]:\n        \"\"\"Lists members of a dataset.\n\n        Returns a dictionary mapping member types to lists of member records. Can optionally\n        recurse through nested datasets and limit the number of results.\n\n        Args:\n            dataset_rid: Resource Identifier of the dataset.\n            recurse: Whether to include members of nested datasets. Defaults to False.\n            limit: Maximum number of members to return per type. None for no limit.\n\n        Returns:\n            dict[str, list[dict[str, Any]]]: Dictionary mapping member types to lists of members.\n                Each member is a dictionary containing the record's attributes.\n\n        Raises:\n            DerivaMLException: If dataset_rid is invalid.\n\n        Example:\n            &gt;&gt;&gt; members = ml.list_dataset_members(\"1-abc123\", recurse=True)\n            &gt;&gt;&gt; for type_name, records in members.items():\n            ...     print(f\"{type_name}: {len(records)} records\")\n        \"\"\"\n\n        if not self._is_dataset_rid(dataset_rid):\n            raise DerivaMLException(f\"RID is not for a dataset_table: {dataset_rid}\")\n\n        # Look at each of the element types that might be in the dataset_table and get the list of rid for them from\n        # the appropriate association table.\n        members = defaultdict(list)\n        pb = self._model.catalog.getPathBuilder()\n        for assoc_table in self._dataset_table.find_associations():\n            other_fkey = assoc_table.other_fkeys.pop()\n            target_table = other_fkey.pk_table\n            member_table = assoc_table.table\n\n            # Look at domain tables and nested datasets.\n            if target_table.schema.name != self._model.domain_schema and not (\n                target_table == self._dataset_table or target_table.name == \"File\"\n            ):\n                continue\n            member_column = (\n                \"Nested_Dataset\" if target_table == self._dataset_table else other_fkey.foreign_key_columns[0].name\n            )\n\n            target_path = pb.schemas[target_table.schema.name].tables[target_table.name]\n            member_path = pb.schemas[member_table.schema.name].tables[member_table.name]\n\n            path = member_path.filter(member_path.Dataset == dataset_rid).link(\n                target_path,\n                on=(member_path.columns[member_column] == target_path.columns[\"RID\"]),\n            )\n            target_entities = list(path.entities().fetch(limit=limit) if limit else path.entities().fetch())\n            members[target_table.name].extend(target_entities)\n            if recurse and target_table == self._dataset_table:\n                # Get the members for all the nested datasets and add to the member list.\n                nested_datasets = [d[\"RID\"] for d in target_entities]\n                for ds in nested_datasets:\n                    for k, v in self.list_dataset_members(ds, recurse=recurse).items():\n                        members[k].extend(v)\n        return dict(members)\n\n    @validate_call\n    def add_dataset_members(\n        self,\n        dataset_rid: RID,\n        members: list[RID] | dict[str, list[RID]],\n        validate: bool = True,\n        description: str | None = \"\",\n        execution_rid: RID | None = None,\n    ) -&gt; None:\n        \"\"\"Adds members to a dataset.\n\n        Associates one or more records with a dataset. Can optionally validate member types\n        and create a new dataset version to track the changes.\n\n        Args:\n            dataset_rid: Resource Identifier of the dataset.\n            members: List of RIDs to add as dataset members. Can be orginized into a dictionary that indicates the\n                table that the member rids belong to.\n            validate: Whether to validate member types. Defaults to True.\n            description: Optional description of the member additions.\n            execution_rid: Optional execution RID to associate with changes.\n\n        Raises:\n            DerivaMLException: If:\n                - dataset_rid is invalid\n                - members are invalid or of wrong type\n                - adding members would create a cycle\n                - validation fails\n\n        Example:\n            &gt;&gt;&gt; ml.add_dataset_members(\n            ...     dataset_rid=\"1-abc123\",\n            ...     members=[\"1-def456\", \"1-ghi789\"],\n            ...     description=\"Added sample data\"\n            ... )\n        \"\"\"\n        description = description or \"Updated dataset via add_dataset_members\"\n\n        def check_dataset_cycle(member_rid, path=None):\n            \"\"\"\n\n            Args:\n              member_rid:\n              path: (Default value = None)\n\n            Returns:\n\n            \"\"\"\n            path = path or set(dataset_rid)\n            return member_rid in path\n\n        if validate:\n            existing_rids = set(m[\"RID\"] for ms in self.list_dataset_members(dataset_rid).values() for m in ms)\n            if overlap := set(existing_rids).intersection(members):\n                raise DerivaMLException(f\"Attempting to add existing member to dataset_table {dataset_rid}: {overlap}\")\n\n        # Now go through every rid to be added to the data set and sort them based on what association table entries\n        # need to be made.\n        dataset_elements = {}\n        association_map = {\n            a.other_fkeys.pop().pk_table.name: a.table.name for a in self._dataset_table.find_associations()\n        }\n\n        # Get a list of all the object types that can be linked to a dataset_table.\n        if type(members) is list:\n            members = set(members)\n            for m in members:\n                try:\n                    rid_info = self._model.catalog.resolve_rid(m)\n                except KeyError:\n                    raise DerivaMLException(f\"Invalid RID: {m}\")\n                if rid_info.table.name not in association_map:\n                    raise DerivaMLException(f\"RID table: {rid_info.table.name} not part of dataset_table\")\n                if rid_info.table == self._dataset_table and check_dataset_cycle(rid_info.rid):\n                    raise DerivaMLException(\"Creating cycle of datasets is not allowed\")\n                dataset_elements.setdefault(rid_info.table.name, []).append(rid_info.rid)\n        else:\n            dataset_elements = {t: set(ms) for t, ms in members.items()}\n        # Now make the entries into the association tables.\n        pb = self._model.catalog.getPathBuilder()\n        for table, elements in dataset_elements.items():\n            schema_path = pb.schemas[\n                self._ml_schema if (table == \"Dataset\" or table == \"File\") else self._model.domain_schema\n            ]\n            fk_column = \"Nested_Dataset\" if table == \"Dataset\" else table\n            if len(elements):\n                # Find out the name of the column in the association table.\n                schema_path.tables[association_map[table]].insert(\n                    [{\"Dataset\": dataset_rid, fk_column: e} for e in elements]\n                )\n        self.increment_dataset_version(\n            dataset_rid,\n            VersionPart.minor,\n            description=description,\n            execution_rid=execution_rid,\n        )\n\n    @validate_call\n    def delete_dataset_members(\n        self,\n        dataset_rid: RID,\n        members: list[RID],\n        description: str = \"\",\n        execution_rid: RID | None = None,\n    ) -&gt; None:\n        \"\"\"Remove elements to an existing dataset_table.\n\n        Delete elements from an existing dataset. In addition to deleting members, the minor version number of the\n        dataset is incremented and the description, if provide is applied to that new version.\n\n        Args:\n            dataset_rid: RID of dataset_table to extend or None if a new dataset_table is to be created.\n            members: List of member RIDs to add to the dataset_table.\n            description: Markdown description of the updated dataset.\n            execution_rid: Optional RID of execution associated with this operation.\n        \"\"\"\n\n        members = set(members)\n        description = description or \"Deletes dataset members\"\n\n        # Now go through every rid to be added to the data set and sort them based on what association table entries\n        # need to be made.\n        dataset_elements = {}\n        association_map = {\n            a.other_fkeys.pop().pk_table.name: a.table.name for a in self._dataset_table.find_associations()\n        }\n        # Get a list of all the object types that can be linked to a dataset_table.\n        for m in members:\n            try:\n                rid_info = self._model.catalog.resolve_rid(m)\n            except KeyError:\n                raise DerivaMLException(f\"Invalid RID: {m}\")\n            if rid_info.table.name not in association_map:\n                raise DerivaMLException(f\"RID table: {rid_info.table.name} not part of dataset_table\")\n            dataset_elements.setdefault(rid_info.table.name, []).append(rid_info.rid)\n        # Now make the entries into the association tables.\n        pb = self._model.catalog.getPathBuilder()\n        for table, elements in dataset_elements.items():\n            schema_path = pb.schemas[self._ml_schema if table == \"Dataset\" else self._model.domain_schema]\n            fk_column = \"Nested_Dataset\" if table == \"Dataset\" else table\n\n            if len(elements):\n                atable_path = schema_path.tables[association_map[table]]\n                # Find out the name of the column in the association table.\n                for e in elements:\n                    entity = atable_path.filter(\n                        (atable_path.Dataset == dataset_rid) &amp; (atable_path.columns[fk_column] == e),\n                    )\n                    entity.delete()\n        self.increment_dataset_version(\n            dataset_rid,\n            VersionPart.minor,\n            description=description,\n            execution_rid=execution_rid,\n        )\n\n    @validate_call\n    def list_dataset_parents(self, dataset_rid: RID) -&gt; list[str]:\n        \"\"\"Given a dataset_table RID, return a list of RIDs of the parent datasets if this is included in a\n        nested dataset.\n\n        Args:\n            dataset_rid: return: RID of the parent dataset_table.\n\n        Returns:\n            RID of the parent dataset_table.\n        \"\"\"\n        if not self._is_dataset_rid(dataset_rid):\n            raise DerivaMLException(f\"RID: {dataset_rid} does not belong to dataset_table {self._dataset_table.name}\")\n        # Get association table for nested datasets\n        pb = self._model.catalog.getPathBuilder()\n        atable_path = pb.schemas[self._ml_schema].Dataset_Dataset\n        return [p[\"Dataset\"] for p in atable_path.filter(atable_path.Nested_Dataset == dataset_rid).entities().fetch()]\n\n    @validate_call\n    def list_dataset_children(self, dataset_rid: RID, recurse: bool = False) -&gt; list[RID]:\n        \"\"\"Given a dataset_table RID, return a list of RIDs for any nested datasets.\n\n        Args:\n            dataset_rid: A dataset_table RID.\n            recurse: If True, return a list of nested datasets RIDs.\n\n        Returns:\n          list of nested dataset RIDs.\n\n        \"\"\"\n        dataset_dataset_path = self._model.catalog.getPathBuilder().schemas[self._ml_schema].tables[\"Dataset_Dataset\"]\n        nested_datasets = list(dataset_dataset_path.entities().fetch())\n\n        def find_children(rid: RID):\n            children = [child[\"Nested_Dataset\"] for child in nested_datasets if child[\"Dataset\"] == rid]\n            if recurse:\n                for child in children.copy():\n                    children.extend(find_children(child))\n            return children\n\n        return find_children(dataset_rid)\n\n    def _export_vocabulary(self, writer: Callable[[str, str, Table], list[dict[str, Any]]]) -&gt; list[dict[str, Any]]:\n        \"\"\"\n\n        Args:\n          writer: Callable[[list[Table]]: list[dict[str: Any]]]:\n\n        Returns:\n\n        \"\"\"\n        vocabs = [\n            table\n            for s in self._model.schemas.values()\n            for table in s.tables.values()\n            if self._model.is_vocabulary(table)\n        ]\n        return [o for table in vocabs for o in writer(f\"{table.schema.name}:{table.name}\", table.name, table)]\n\n    def _table_paths(\n        self,\n        dataset: DatasetSpec | None = None,\n        snapshot_catalog: DerivaML | None = None,\n    ) -&gt; Iterator[tuple[str, str, Table]]:\n        paths = self._collect_paths(dataset and dataset.rid, snapshot_catalog)\n\n        def source_path(path: tuple[Table, ...]) -&gt; list[str]:\n            \"\"\"Convert a tuple representing a path into a source path component with FK linkage\"\"\"\n            path = list(path)\n            p = [f\"{self._model.ml_schema}:Dataset/RID={{RID}}\"]\n            for table in path[1:]:\n                if table.name == \"Dataset_Dataset\":\n                    p.append(\"(RID)=(deriva-ml:Dataset_Dataset:Dataset)\")\n                elif table.name == \"Dataset\":\n                    p.append(\"(Nested_Dataset)=(deriva-ml:Dataset:RID)\")\n                elif table.name == \"Dataset_Version\":\n                    p.append(f\"(RID)=({self._model.ml_schema}:Dataset_Version:Dataset)\")\n                else:\n                    p.append(f\"{table.schema.name}:{table.name}\")\n            return p\n\n        src_paths = [\"/\".join(source_path(p)) for p in paths]\n        dest_paths = [\"/\".join([t.name for t in p]) for p in paths]\n        target_tables = [p[-1] for p in paths]\n        return zip(src_paths, dest_paths, target_tables)\n\n    def _collect_paths(\n        self,\n        dataset_rid: RID | None = None,\n        snapshot: Dataset | None = None,\n        dataset_nesting_depth: int | None = None,\n    ) -&gt; set[tuple[Table, ...]]:\n        snapshot_catalog = snapshot if snapshot else self\n\n        dataset_table = snapshot_catalog._model.schemas[self._ml_schema].tables[\"Dataset\"]\n        dataset_dataset = snapshot_catalog._model.schemas[self._ml_schema].tables[\"Dataset_Dataset\"]\n\n        # Figure out what types of elements the dataset contains.\n        dataset_associations = [\n            a\n            for a in self._dataset_table.find_associations()\n            if a.table.schema.name != self._ml_schema or a.table.name == \"Dataset_Dataset\"\n        ]\n        if dataset_rid:\n            # Get a list of the members of the dataset so we can figure out which tables to query.\n            dataset_elements = [\n                snapshot_catalog._model.name_to_table(e)\n                for e, m in snapshot_catalog.list_dataset_members(\n                    dataset_rid=dataset_rid,  #  limit=1 Limit seems to make things run slow.\n                ).items()\n                if m\n            ]\n            included_associations = [\n                a.table for a in dataset_table.find_associations() if a.other_fkeys.pop().pk_table in dataset_elements\n            ]\n        else:\n            included_associations = dataset_associations\n\n        # Get the paths through the schema and filter out all the dataset paths not used by this dataset.\n        paths = {\n            tuple(p)\n            for p in snapshot_catalog._model._schema_to_paths()\n            if (len(p) == 1)\n            or (p[1] not in dataset_associations)  # Tables in the domain schema\n            or (p[1] in included_associations)  # Tables that include members of the dataset\n        }\n        # Now get paths for nested datasets\n        nested_paths = set()\n        if dataset_rid:\n            for c in snapshot_catalog.list_dataset_children(dataset_rid=dataset_rid):\n                nested_paths |= self._collect_paths(c, snapshot=snapshot_catalog)\n        else:\n            # Initialize nesting depth if not already provided.\n            dataset_nesting_depth = (\n                self._dataset_nesting_depth() if dataset_nesting_depth is None else dataset_nesting_depth\n            )\n            if dataset_nesting_depth:\n                nested_paths = self._collect_paths(dataset_nesting_depth=dataset_nesting_depth - 1)\n        if nested_paths:\n            paths |= {\n                tuple([dataset_table]),\n                (dataset_table, dataset_dataset),\n            }\n        paths |= {(self._dataset_table, dataset_dataset) + p for p in nested_paths}\n        return paths\n\n    def _dataset_nesting_depth(self, dataset_rid: RID | None = None) -&gt; int:\n        \"\"\"Determine the maximum dataset nesting depth in the current catalog.\n\n        Returns:\n\n        \"\"\"\n\n        def children_depth(dataset_rid: RID, nested_datasets: dict[str, list[str]]) -&gt; int:\n            \"\"\"Return the number of nested datasets for the dataset_rid if provided, otherwise in the current catalog\"\"\"\n            try:\n                children = nested_datasets[dataset_rid]\n                return max(map(lambda x: children_depth(x, nested_datasets), children)) + 1 if children else 1\n            except KeyError:\n                return 0\n\n        # Build up the dataset_table nesting graph...\n        pb = self._model.catalog.getPathBuilder().schemas[self._ml_schema].tables[\"Dataset_Dataset\"]\n        dataset_children = (\n            [\n                {\n                    \"Dataset\": dataset_rid,\n                    \"Nested_Dataset\": c,\n                }  # Make uniform with return from datapath\n                for c in self.list_dataset_children(dataset_rid=dataset_rid)\n            ]\n            if dataset_rid\n            else pb.entities().fetch()\n        )\n        nested_dataset = defaultdict(list)\n        for ds in dataset_children:\n            nested_dataset[ds[\"Dataset\"]].append(ds[\"Nested_Dataset\"])\n        return max(map(lambda d: children_depth(d, dict(nested_dataset)), nested_dataset)) if nested_dataset else 0\n\n    def _dataset_specification(\n        self,\n        writer: Callable[[str, str, Table], list[dict[str, Any]]],\n        dataset: DatasetSpec | None = None,\n        snapshot_catalog: DerivaML | None = None,\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"Output a download/export specification for a dataset_table.  Each element of the dataset_table\n        will be placed in its own directory.\n        The top level data directory of the resulting BDBag will have one subdirectory for element type.\n        The subdirectory will contain the CSV indicating which elements of that type are present in the\n        dataset_table, and then there will be a subdirectory for each object that is reachable from the\n        dataset_table members.\n\n        To simplify reconstructing the relationship between tables, the CVS for each element is included.\n        The top level data directory will also contain a subdirectory for any controlled vocabularies used in\n        the dataset_table. All assets will be placed into a directory named asset in a subdirectory with the\n        asset table name.\n\n        For example, consider a dataset_table that consists of two element types, T1 and T2. T1 has foreign\n        key relationships to objects in tables T3 and T4. There are also two controlled vocabularies, CV1 and\n        CV2. T2 is an asset table which has two assets in it. The layout of the resulting bdbag would be:\n              data\n                CV1/\n                    cv1.csv\n                CV2/\n                    cv2.csv\n                Dataset/\n                    T1/\n                        t1.csv\n                        T3/\n                            t3.csv\n                        T4/\n                            t4.csv\n                    T2/\n                        t2.csv\n                asset/\n                  T2\n                    f1\n                    f2\n\n        Args:\n          writer: Callable[[list[Table]]: list[dict[str:  Any]]]:\n\n        Returns:\n            A dataset_table specification.\n        \"\"\"\n        element_spec = self._export_vocabulary(writer)\n        for path in self._table_paths(dataset=dataset, snapshot_catalog=snapshot_catalog):\n            element_spec.extend(writer(*path))\n        return element_spec\n\n    def _download_dataset_bag(\n        self,\n        dataset: DatasetSpec,\n        execution_rid: RID | None = None,\n        snapshot_catalog: DerivaML | None = None,\n    ) -&gt; DatasetBag:\n        \"\"\"Download a dataset onto the local file system.  Create a MINID for the dataset if one doesn't already exist.\n\n        Args:\n            dataset: Specification of the dataset to be downloaded.\n            execution_rid: Execution RID for the dataset.\n            snapshot_catalog: Snapshot catalog for the dataset version if specified.\n\n        Returns:\n            Tuple consisting of the path to the dataset, the RID of the dataset that was downloaded and the MINID\n            for the dataset.\n        \"\"\"\n        if (\n            execution_rid\n            and execution_rid != DRY_RUN_RID\n            and self._model.catalog.resolve_rid(execution_rid).table.name != \"Execution\"\n        ):\n            raise DerivaMLException(f\"RID {execution_rid} is not an execution\")\n        minid = self._get_dataset_minid(dataset, snapshot_catalog=snapshot_catalog)\n\n        bag_path = (\n            self._materialize_dataset_bag(minid, execution_rid=execution_rid)\n            if dataset.materialize\n            else self._download_dataset_minid(minid)\n        )\n        return DatabaseModel(minid, bag_path, self._working_dir).get_dataset()\n\n    def _version_snapshot(self, dataset: DatasetSpec) -&gt; str:\n        \"\"\"Return a catalog with snapshot for the specified dataset version\"\"\"\n        try:\n            version_record = next(\n                h for h in self.dataset_history(dataset_rid=dataset.rid) if h.dataset_version == dataset.version\n            )\n        except StopIteration:\n            raise DerivaMLException(f\"Dataset version {dataset.version} not found for dataset {dataset.rid}\")\n        return f\"{self._model.catalog.catalog_id}@{version_record.snapshot}\"\n\n    def _create_dataset_minid(self, dataset: DatasetSpec, snapshot_catalog: DerivaML | None = None) -&gt; str:\n        with TemporaryDirectory() as tmp_dir:\n            # Generate a download specification file for the current catalog schema. By default, this spec\n            # will generate a minid and place the bag into S3 storage.\n            spec_file = Path(tmp_dir) / \"download_spec.json\"\n            with spec_file.open(\"w\", encoding=\"utf-8\") as ds:\n                json.dump(self._generate_dataset_download_spec(dataset, snapshot_catalog), ds)\n            try:\n                self._logger.info(\n                    \"Downloading dataset %s for catalog: %s@%s\"\n                    % (\n                        \"minid\" if self._use_minid else \"bag\",\n                        dataset.rid,\n                        str(dataset.version),\n                    )\n                )\n                # Generate the bag and put into S3 storage.\n                exporter = DerivaExport(\n                    host=self._model.catalog.deriva_server.server,\n                    config_file=spec_file,\n                    output_dir=tmp_dir,\n                    defer_download=True,\n                    timeout=(10, 610),\n                    envars={\"RID\": dataset.rid},\n                )\n                minid_page_url = exporter.export()[0]  # Get the MINID launch page\n\n            except (\n                DerivaDownloadError,\n                DerivaDownloadConfigurationError,\n                DerivaDownloadAuthenticationError,\n                DerivaDownloadAuthorizationError,\n                DerivaDownloadTimeoutError,\n            ) as e:\n                raise DerivaMLException(format_exception(e))\n            # Update version table with MINID.\n            if self._use_minid:\n                version_path = self._model.catalog.getPathBuilder().schemas[self._ml_schema].tables[\"Dataset_Version\"]\n                version_rid = [\n                    h for h in self.dataset_history(dataset_rid=dataset.rid) if h.dataset_version == dataset.version\n                ][0].version_rid\n                version_path.update([{\"RID\": version_rid, \"Minid\": minid_page_url}])\n        return minid_page_url\n\n    def _get_dataset_minid(\n        self,\n        dataset: DatasetSpec,\n        snapshot_catalog: DerivaML | None = None,\n        create: bool = True,\n    ) -&gt; DatasetMinid | None:\n        \"\"\"Return a MINID for the specified dataset. If no version is specified, use the latest.\n\n        Args:\n            dataset: Specification of the dataset.\n            snapshot_catalog: Snapshot catalog for the dataset version if specified.\n            create: Create a new MINID if one doesn't already exist.\n\n        Returns:\n            New or existing MINID for the dataset.\n        \"\"\"\n        rid = dataset.rid\n\n        # Case 1: RID is already a MINID or direct URL\n        if rid.startswith(\"minid\"):\n            return self._fetch_minid_metadata(f\"https://identifiers.org/{rid}\", dataset.version)\n        if rid.startswith(\"http\"):\n            return self._fetch_minid_metadata(rid, dataset.version)\n\n        # Case 2: RID is a dataset RID \u2013 validate existence\n        if not any(rid == ds[\"RID\"] for ds in self.find_datasets()):\n            raise DerivaMLTableTypeError(\"Dataset\", rid)\n\n        # Find dataset version record\n        version_str = str(dataset.version)\n        history = self.dataset_history(rid)\n        try:\n            version_record = next(v for v in history if v.dataset_version == version_str)\n        except StopIteration:\n            raise DerivaMLException(f\"Version {version_str} does not exist for RID {rid}\")\n\n        # Check or create MINID\n        minid_url = version_record.minid\n        if not minid_url:\n            if not create:\n                raise DerivaMLException(f\"Minid for dataset {rid} doesn't exist\")\n            if self._use_minid:\n                self._logger.info(\"Creating new MINID for dataset %s\", rid)\n            minid_url = self._create_dataset_minid(dataset, snapshot_catalog)\n\n        # Return based on MINID usage\n        if self._use_minid:\n            return self._fetch_minid_metadata(minid_url, dataset.version)\n\n        return DatasetMinid(\n            dataset_version=dataset.version,\n            RID=f\"{rid}@{version_record.snapshot}\",\n            location=minid_url,\n        )\n\n    def _fetch_minid_metadata(self, url: str, version: DatasetVersion) -&gt; DatasetMinid:\n        r = requests.get(url, headers={\"accept\": \"application/json\"})\n        r.raise_for_status()\n        return DatasetMinid(dataset_version=version, **r.json())\n\n    def _download_dataset_minid(self, minid: DatasetMinid) -&gt; Path:\n        \"\"\"Given a RID to a dataset_table, or a MINID to an existing bag, download the bag file, extract it, and\n        validate that all the metadata is correct\n\n        Args:\n            minid: The RID of a dataset_table or a minid to an existing bag.\n        Returns:\n            the location of the unpacked and validated dataset_table bag and the RID of the bag and the bag MINID\n        \"\"\"\n\n        # Check to see if we have an existing idempotent materialization of the desired bag. If so, then reuse\n        # it.  If not, then we need to extract the contents of the archive into our cache directory.\n        bag_dir = self._cache_dir / f\"{minid.dataset_rid}_{minid.checksum}\"\n        if bag_dir.exists():\n            self._logger.info(f\"Using cached bag for  {minid.dataset_rid} Version:{minid.dataset_version}\")\n            return Path(bag_dir / f\"Dataset_{minid.dataset_rid}\")\n\n        # Either bag hasn't been downloaded yet, or we are not using a Minid, so we don't know the checksum yet.\n        with TemporaryDirectory() as tmp_dir:\n            if self._use_minid:\n                # Get bag from S3\n                archive_path = fetch_single_file(minid.bag_url, output_path=tmp_dir)\n            else:\n                exporter = DerivaExport(host=self._model.catalog.deriva_server.server, output_dir=tmp_dir)\n                archive_path = exporter.retrieve_file(minid.bag_url)\n                hashes = hash_utils.compute_file_hashes(archive_path, hashes=[\"md5\", \"sha256\"])\n                checksum = hashes[\"sha256\"][0]\n                bag_dir = self._cache_dir / f\"{minid.dataset_rid}_{checksum}\"\n                if bag_dir.exists():\n                    self._logger.info(f\"Using cached bag for  {minid.dataset_rid} Version:{minid.dataset_version}\")\n                    return Path(bag_dir / f\"Dataset_{minid.dataset_rid}\")\n            bag_path = bdb.extract_bag(archive_path, bag_dir.as_posix())\n        bdb.validate_bag_structure(bag_path)\n        return Path(bag_path)\n\n    def _materialize_dataset_bag(\n        self,\n        minid: DatasetMinid,\n        execution_rid: RID | None = None,\n    ) -&gt; Path:\n        \"\"\"Materialize a dataset_table bag into a local directory\n\n        Args:\n            minid: A MINID to an existing bag or a RID of the dataset_table that should be downloaded.\n\n        Returns:\n            A tuple containing the path to the bag, the RID of the bag, and the MINID to the bag.\n        \"\"\"\n\n        def update_status(status: Status, msg: str) -&gt; None:\n            \"\"\"Update the current status for this execution in the catalog\"\"\"\n            if execution_rid and execution_rid != DRY_RUN_RID:\n                self._model.catalog.getPathBuilder().schemas[self._ml_schema].Execution.update(\n                    [\n                        {\n                            \"RID\": execution_rid,\n                            \"Status\": status.value,\n                            \"Status_Detail\": msg,\n                        }\n                    ]\n                )\n            self._logger.info(msg)\n\n        def fetch_progress_callback(current, total):\n            msg = f\"Materializing bag: {current} of {total} file(s) downloaded.\"\n            if execution_rid:\n                update_status(Status.running, msg)\n            return True\n\n        def validation_progress_callback(current, total):\n            msg = f\"Validating bag: {current} of {total} file(s) validated.\"\n            if execution_rid:\n                update_status(Status.running, msg)\n            return True\n\n        # request metadata\n        bag_path = self._download_dataset_minid(minid)\n        bag_dir = bag_path.parent\n        validated_check = bag_dir / \"validated_check.txt\"\n\n        # If this bag has already been validated, our work is done.  Otherwise, materialize the bag.\n        if not validated_check.exists():\n            self._logger.info(f\"Materializing bag {minid.dataset_rid} Version:{minid.dataset_version}\")\n            bdb.materialize(\n                bag_path.as_posix(),\n                fetch_callback=fetch_progress_callback,\n                validation_callback=validation_progress_callback,\n            )\n            validated_check.touch()\n        return Path(bag_path)\n\n    def _export_annotation(\n        self,\n        snapshot_catalog: DerivaML | None = None,\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"Return and output specification for the datasets in the provided model\n\n        Returns:\n          An export specification suitable for Chaise.\n        \"\"\"\n\n        # Export specification is a specification for the datasets, plus any controlled vocabulary\n        return [\n            {\n                \"source\": {\"api\": False, \"skip_root_path\": True},\n                \"destination\": {\"type\": \"env\", \"params\": {\"query_keys\": [\"snaptime\"]}},\n            },\n            {\n                \"source\": {\"api\": \"entity\"},\n                \"destination\": {\n                    \"type\": \"env\",\n                    \"params\": {\"query_keys\": [\"RID\", \"Description\"]},\n                },\n            },\n            {\n                \"source\": {\"api\": \"schema\", \"skip_root_path\": True},\n                \"destination\": {\"type\": \"json\", \"name\": \"schema\"},\n            },\n        ] + self._dataset_specification(\n            self._export_annotation_dataset_element,\n            None,\n            snapshot_catalog=snapshot_catalog,\n        )\n\n    def _export_specification(\n        self, dataset: DatasetSpec, snapshot_catalog: DerivaML | None = None\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Generate a specification for export engine for specific dataset.\n\n        Returns:\n          a download specification for the datasets in the provided model.\n\n        \"\"\"\n\n        # Download spec is the spec for any controlled vocabulary and for the dataset_table.\n        return [\n            {\n                \"processor\": \"json\",\n                \"processor_params\": {\"query_path\": \"/schema\", \"output_path\": \"schema\"},\n            }\n        ] + self._dataset_specification(self._export_specification_dataset_element, dataset, snapshot_catalog)\n\n    @staticmethod\n    def _export_specification_dataset_element(spath: str, dpath: str, table: Table) -&gt; list[dict[str, Any]]:\n        \"\"\"Return the download specification for the data object indicated by a path through the data model.\n\n        Args:\n          spath: Source path\n          dpath: Destination path\n          table: Table referenced to by the path\n\n        Returns:\n          The download specification that will retrieve that data from the catalog and place it into a BDBag.\n        \"\"\"\n        exports = [\n            {\n                \"processor\": \"csv\",\n                \"processor_params\": {\n                    \"query_path\": f\"/entity/{spath}\",\n                    \"output_path\": dpath,\n                },\n            }\n        ]\n\n        # If this table is an asset table, then we need to output the files associated with the asset.\n        asset_columns = {\"Filename\", \"URL\", \"Length\", \"MD5\", \"Description\"}\n        if asset_columns.issubset({c.name for c in table.columns}):\n            exports.append(\n                {\n                    \"processor\": \"fetch\",\n                    \"processor_params\": {\n                        \"query_path\": f\"/attribute/{spath}/!(URL::null::)/url:=URL,length:=Length,filename:=Filename,md5:=MD5,asset_rid:=RID\",\n                        \"output_path\": \"asset/{asset_rid}/\" + table.name,\n                    },\n                }\n            )\n        return exports\n\n    def _export_annotation_dataset_element(self, spath: str, dpath: str, table: Table) -&gt; list[dict[str, Any]]:\n        \"\"\"Given a path in the data model, output an export specification for the path taken to get to the\n        current table.\n\n        Args:\n          spath: Source path\n          dpath: Destination path\n          table: Table referenced to by the path\n\n        Returns:\n          The export specification that will retrieve that data from the catalog and place it into a BDBag.\n        \"\"\"\n        # The table is the last element of the path.  Generate the ERMRest query by converting the list of tables\n        # into a path in the form of /S:T1/S:T2/S:Table\n        # Generate the destination path in the file system using just the table names.\n\n        skip_root_path = False\n        if spath.startswith(f\"{self._ml_schema}:Dataset/\"):\n            # Chaise will add table name and RID filter, so strip it off.\n            spath = \"/\".join(spath.split(\"/\")[2:])\n            if spath == \"\":\n                # This path is to just the dataset table.\n                return []\n        else:\n            # A vocabulary table, so we don't want the root_path.\n            skip_root_path = True\n        exports = [\n            {\n                \"source\": {\n                    \"api\": \"entity\",\n                    \"path\": spath,\n                    \"skip_root_path\": skip_root_path,\n                },\n                \"destination\": {\"name\": dpath, \"type\": \"csv\"},\n            }\n        ]\n\n        # If this table is an asset table, then we need to output the files associated with the asset.\n        asset_columns = {\"Filename\", \"URL\", \"Length\", \"MD5\", \"Description\"}\n        if asset_columns.issubset({c.name for c in table.columns}):\n            exports.append(\n                {\n                    \"source\": {\n                        \"skip_root_path\": False,\n                        \"api\": \"attribute\",\n                        \"path\": f\"{spath}/!(URL::null::)/url:=URL,length:=Length,filename:=Filename,md5:=MD5, asset_rid:=RID\",\n                    },\n                    \"destination\": {\"name\": \"asset/{asset_rid}/\" + table.name, \"type\": \"fetch\"},\n                }\n            )\n        return exports\n\n    def _generate_dataset_download_spec(\n        self, dataset: DatasetSpec, snapshot_catalog: DerivaML | None = None\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Generate a specification for downloading a specific dataset.\n\n        This routine creates a download specification that can be used by the Deriva export processor to download\n        a specific dataset as a MINID.\n        Returns:\n        \"\"\"\n        s3_target = \"s3://eye-ai-shared\"\n        minid_test = False\n\n        catalog_id = self._version_snapshot(dataset)\n        post_processors = (\n            {\n                \"post_processors\": [\n                    {\n                        \"processor\": \"cloud_upload\",\n                        \"processor_params\": {\n                            \"acl\": \"public-read\",\n                            \"target_url\": s3_target,\n                        },\n                    },\n                    {\n                        \"processor\": \"identifier\",\n                        \"processor_params\": {\n                            \"test\": minid_test,\n                            \"env_column_map\": {\n                                \"RID\": \"{RID}@{snaptime}\",\n                                \"Description\": \"{Description}\",\n                            },\n                        },\n                    },\n                ]\n            }\n            if self._use_minid\n            else {}\n        )\n        return post_processors | {\n            \"env\": {\"RID\": \"{RID}\"},\n            \"bag\": {\n                \"bag_name\": \"Dataset_{RID}\",\n                \"bag_algorithms\": [\"md5\"],\n                \"bag_archiver\": \"zip\",\n                \"bag_metadata\": {},\n                \"bag_idempotent\": True,\n            },\n            \"catalog\": {\n                \"host\": f\"{self._model.catalog.deriva_server.scheme}://{self._model.catalog.deriva_server.server}\",\n                \"catalog_id\": catalog_id,\n                \"query_processors\": [\n                    {\n                        \"processor\": \"env\",\n                        \"processor_params\": {\n                            \"output_path\": \"Dataset\",\n                            \"query_keys\": [\"snaptime\"],\n                            \"query_path\": \"/\",\n                        },\n                    },\n                    {\n                        \"processor\": \"env\",\n                        \"processor_params\": {\n                            \"query_path\": \"/entity/M:=deriva-ml:Dataset/RID={RID}\",\n                            \"output_path\": \"Dataset\",\n                            \"query_keys\": [\"RID\", \"Description\"],\n                        },\n                    },\n                ]\n                + self._export_specification(dataset, snapshot_catalog),\n            },\n        }\n\n    def _generate_dataset_download_annotations(self) -&gt; dict[str, Any]:\n        post_processors = (\n            {\n                \"type\": \"BAG\",\n                \"outputs\": [{\"fragment_key\": \"dataset_export_outputs\"}],\n                \"displayname\": \"BDBag to Cloud\",\n                \"bag_idempotent\": True,\n                \"postprocessors\": [\n                    {\n                        \"processor\": \"cloud_upload\",\n                        \"processor_params\": {\n                            \"acl\": \"public-read\",\n                            \"target_url\": \"s3://eye-ai-shared/\",\n                        },\n                    },\n                    {\n                        \"processor\": \"identifier\",\n                        \"processor_params\": {\n                            \"test\": False,\n                            \"env_column_map\": {\n                                \"RID\": \"{RID}@{snaptime}\",\n                                \"Description\": \"{Description}\",\n                            },\n                        },\n                    },\n                ],\n            }\n            if self._use_minid\n            else {}\n        )\n        return {\n            deriva_tags.export_fragment_definitions: {\"dataset_export_outputs\": self._export_annotation()},\n            deriva_tags.visible_foreign_keys: self._dataset_visible_fkeys(),\n            deriva_tags.export_2019: {\n                \"detailed\": {\n                    \"templates\": [\n                        {\n                            \"type\": \"BAG\",\n                            \"outputs\": [{\"fragment_key\": \"dataset_export_outputs\"}],\n                            \"displayname\": \"BDBag Download\",\n                            \"bag_idempotent\": True,\n                        }\n                        | post_processors\n                    ]\n                }\n            },\n        }\n\n    def _dataset_visible_fkeys(self) -&gt; dict[str, Any]:\n        def fkey_name(fk):\n            return [fk.name[0].name, fk.name[1]]\n\n        dataset_table = self._model.schemas[\"deriva-ml\"].tables[\"Dataset\"]\n\n        source_list = [\n            {\n                \"source\": [\n                    {\"inbound\": [\"deriva-ml\", \"Dataset_Version_Dataset_fkey\"]},\n                    \"RID\",\n                ],\n                \"markdown_name\": \"Previous Versions\",\n                \"entity\": True,\n            },\n            {\n                \"source\": [\n                    {\"inbound\": [\"deriva-ml\", \"Dataset_Dataset_Nested_Dataset_fkey\"]},\n                    {\"outbound\": [\"deriva-ml\", \"Dataset_Dataset_Dataset_fkey\"]},\n                    \"RID\",\n                ],\n                \"markdown_name\": \"Parent Datasets\",\n            },\n            {\n                \"source\": [\n                    {\"inbound\": [\"deriva-ml\", \"Dataset_Dataset_Dataset_fkey\"]},\n                    {\"outbound\": [\"deriva-ml\", \"Dataset_Dataset_Nested_Dataset_fkey\"]},\n                    \"RID\",\n                ],\n                \"markdown_name\": \"Child Datasets\",\n            },\n        ]\n        source_list.extend(\n            [\n                {\n                    \"source\": [\n                        {\"inbound\": fkey_name(fkey.self_fkey)},\n                        {\"outbound\": fkey_name(other_fkey := fkey.other_fkeys.pop())},\n                        \"RID\",\n                    ],\n                    \"markdown_name\": other_fkey.pk_table.name,\n                }\n                for fkey in dataset_table.find_associations(max_arity=3, pure=False)\n            ]\n        )\n        return {\"detailed\": source_list}\n</code></pre>"},{"location":"code-docs/dataset/#deriva_ml.dataset.Dataset.__init__","title":"__init__","text":"<pre><code>__init__(\n    model: DerivaModel,\n    cache_dir: Path,\n    working_dir: Path,\n    use_minid: bool = True,\n)\n</code></pre> <p>Initializes a Dataset instance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>DerivaModel</code> <p>DerivaModel instance representing the catalog.</p> required <code>cache_dir</code> <code>Path</code> <p>Directory path for caching downloaded datasets.</p> required <code>working_dir</code> <code>Path</code> <p>Directory path for working data.</p> required <code>use_minid</code> <code>bool</code> <p>Whether to use MINID service for dataset identification.</p> <code>True</code> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>def __init__(\n    self,\n    model: DerivaModel,\n    cache_dir: Path,\n    working_dir: Path,\n    use_minid: bool = True,\n):\n    \"\"\"Initializes a Dataset instance.\n\n    Args:\n        model: DerivaModel instance representing the catalog.\n        cache_dir: Directory path for caching downloaded datasets.\n        working_dir: Directory path for working data.\n        use_minid: Whether to use MINID service for dataset identification.\n    \"\"\"\n    self._model = model\n    self._ml_schema = ML_SCHEMA\n    self._cache_dir = cache_dir\n    self._working_dir = working_dir\n    self._logger = logging.getLogger(\"deriva_ml\")\n    self._use_minid = use_minid\n</code></pre>"},{"location":"code-docs/dataset/#deriva_ml.dataset.Dataset.add_dataset_element_type","title":"add_dataset_element_type","text":"<pre><code>add_dataset_element_type(\n    element: str | Table,\n) -&gt; Table\n</code></pre> <p>A dataset_table is a heterogeneous collection of objects, each of which comes from a different table. This routine makes it possible to add objects from the specified table to a dataset_table.</p> <p>Parameters:</p> Name Type Description Default <code>element</code> <code>str | Table</code> <p>Name of the table or table object that is to be added to the dataset_table.</p> required <p>Returns:</p> Type Description <code>Table</code> <p>The table object that was added to the dataset_table.</p> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>@validate_call(config=ConfigDict(arbitrary_types_allowed=True))\ndef add_dataset_element_type(self, element: str | Table) -&gt; Table:\n    \"\"\"A dataset_table is a heterogeneous collection of objects, each of which comes from a different table. This\n    routine makes it possible to add objects from the specified table to a dataset_table.\n\n    Args:\n        element: Name of the table or table object that is to be added to the dataset_table.\n\n    Returns:\n        The table object that was added to the dataset_table.\n    \"\"\"\n    # Add table to map\n    element_table = self._model.name_to_table(element)\n    atable_def = Table.define_association([self._dataset_table, element_table])\n    try:\n        table = self._model.schemas[self._model.domain_schema].create_table(atable_def)\n    except ValueError as e:\n        if \"already exists\" in str(e):\n            table = self._model.name_to_table(atable_def[\"table_name\"])\n        else:\n            raise e\n\n    # self.model = self.catalog.getCatalogModel()\n    self._dataset_table.annotations.update(self._generate_dataset_download_annotations())\n    self._model.model.apply()\n    return table\n</code></pre>"},{"location":"code-docs/dataset/#deriva_ml.dataset.Dataset.add_dataset_members","title":"add_dataset_members","text":"<pre><code>add_dataset_members(\n    dataset_rid: RID,\n    members: list[RID]\n    | dict[str, list[RID]],\n    validate: bool = True,\n    description: str | None = \"\",\n    execution_rid: RID | None = None,\n) -&gt; None\n</code></pre> <p>Adds members to a dataset.</p> <p>Associates one or more records with a dataset. Can optionally validate member types and create a new dataset version to track the changes.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_rid</code> <code>RID</code> <p>Resource Identifier of the dataset.</p> required <code>members</code> <code>list[RID] | dict[str, list[RID]]</code> <p>List of RIDs to add as dataset members. Can be orginized into a dictionary that indicates the table that the member rids belong to.</p> required <code>validate</code> <code>bool</code> <p>Whether to validate member types. Defaults to True.</p> <code>True</code> <code>description</code> <code>str | None</code> <p>Optional description of the member additions.</p> <code>''</code> <code>execution_rid</code> <code>RID | None</code> <p>Optional execution RID to associate with changes.</p> <code>None</code> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If: - dataset_rid is invalid - members are invalid or of wrong type - adding members would create a cycle - validation fails</p> Example <p>ml.add_dataset_members( ...     dataset_rid=\"1-abc123\", ...     members=[\"1-def456\", \"1-ghi789\"], ...     description=\"Added sample data\" ... )</p> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>@validate_call\ndef add_dataset_members(\n    self,\n    dataset_rid: RID,\n    members: list[RID] | dict[str, list[RID]],\n    validate: bool = True,\n    description: str | None = \"\",\n    execution_rid: RID | None = None,\n) -&gt; None:\n    \"\"\"Adds members to a dataset.\n\n    Associates one or more records with a dataset. Can optionally validate member types\n    and create a new dataset version to track the changes.\n\n    Args:\n        dataset_rid: Resource Identifier of the dataset.\n        members: List of RIDs to add as dataset members. Can be orginized into a dictionary that indicates the\n            table that the member rids belong to.\n        validate: Whether to validate member types. Defaults to True.\n        description: Optional description of the member additions.\n        execution_rid: Optional execution RID to associate with changes.\n\n    Raises:\n        DerivaMLException: If:\n            - dataset_rid is invalid\n            - members are invalid or of wrong type\n            - adding members would create a cycle\n            - validation fails\n\n    Example:\n        &gt;&gt;&gt; ml.add_dataset_members(\n        ...     dataset_rid=\"1-abc123\",\n        ...     members=[\"1-def456\", \"1-ghi789\"],\n        ...     description=\"Added sample data\"\n        ... )\n    \"\"\"\n    description = description or \"Updated dataset via add_dataset_members\"\n\n    def check_dataset_cycle(member_rid, path=None):\n        \"\"\"\n\n        Args:\n          member_rid:\n          path: (Default value = None)\n\n        Returns:\n\n        \"\"\"\n        path = path or set(dataset_rid)\n        return member_rid in path\n\n    if validate:\n        existing_rids = set(m[\"RID\"] for ms in self.list_dataset_members(dataset_rid).values() for m in ms)\n        if overlap := set(existing_rids).intersection(members):\n            raise DerivaMLException(f\"Attempting to add existing member to dataset_table {dataset_rid}: {overlap}\")\n\n    # Now go through every rid to be added to the data set and sort them based on what association table entries\n    # need to be made.\n    dataset_elements = {}\n    association_map = {\n        a.other_fkeys.pop().pk_table.name: a.table.name for a in self._dataset_table.find_associations()\n    }\n\n    # Get a list of all the object types that can be linked to a dataset_table.\n    if type(members) is list:\n        members = set(members)\n        for m in members:\n            try:\n                rid_info = self._model.catalog.resolve_rid(m)\n            except KeyError:\n                raise DerivaMLException(f\"Invalid RID: {m}\")\n            if rid_info.table.name not in association_map:\n                raise DerivaMLException(f\"RID table: {rid_info.table.name} not part of dataset_table\")\n            if rid_info.table == self._dataset_table and check_dataset_cycle(rid_info.rid):\n                raise DerivaMLException(\"Creating cycle of datasets is not allowed\")\n            dataset_elements.setdefault(rid_info.table.name, []).append(rid_info.rid)\n    else:\n        dataset_elements = {t: set(ms) for t, ms in members.items()}\n    # Now make the entries into the association tables.\n    pb = self._model.catalog.getPathBuilder()\n    for table, elements in dataset_elements.items():\n        schema_path = pb.schemas[\n            self._ml_schema if (table == \"Dataset\" or table == \"File\") else self._model.domain_schema\n        ]\n        fk_column = \"Nested_Dataset\" if table == \"Dataset\" else table\n        if len(elements):\n            # Find out the name of the column in the association table.\n            schema_path.tables[association_map[table]].insert(\n                [{\"Dataset\": dataset_rid, fk_column: e} for e in elements]\n            )\n    self.increment_dataset_version(\n        dataset_rid,\n        VersionPart.minor,\n        description=description,\n        execution_rid=execution_rid,\n    )\n</code></pre>"},{"location":"code-docs/dataset/#deriva_ml.dataset.Dataset.create_dataset","title":"create_dataset","text":"<pre><code>create_dataset(\n    dataset_types: str\n    | list[str]\n    | None = None,\n    description: str = \"\",\n    execution_rid: RID | None = None,\n    version: DatasetVersion\n    | None = None,\n) -&gt; RID\n</code></pre> <p>Creates a new dataset in the catalog.</p> <p>Creates a dataset with specified types and description. The dataset can be associated with an execution and initialized with a specific version.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_types</code> <code>str | list[str] | None</code> <p>One or more dataset type terms from Dataset_Type vocabulary.</p> <code>None</code> <code>description</code> <code>str</code> <p>Description of the dataset's purpose and contents.</p> <code>''</code> <code>execution_rid</code> <code>RID | None</code> <p>Optional execution RID to associate with dataset creation.</p> <code>None</code> <code>version</code> <code>DatasetVersion | None</code> <p>Optional initial version number. Defaults to 0.1.0.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>RID</code> <code>RID</code> <p>Resource Identifier of the newly created dataset.</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If dataset_types are invalid or creation fails.</p> Example <p>rid = ml.create_dataset( ...     dataset_types=[\"experiment\", \"raw_data\"], ...     description=\"RNA sequencing experiment data\", ...     version=DatasetVersion(1, 0, 0) ... )</p> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>@validate_call(config=ConfigDict(arbitrary_types_allowed=True))\ndef create_dataset(\n    self,\n    dataset_types: str | list[str] | None = None,\n    description: str = \"\",\n    execution_rid: RID | None = None,\n    version: DatasetVersion | None = None,\n) -&gt; RID:\n    \"\"\"Creates a new dataset in the catalog.\n\n    Creates a dataset with specified types and description. The dataset can be associated\n    with an execution and initialized with a specific version.\n\n    Args:\n        dataset_types: One or more dataset type terms from Dataset_Type vocabulary.\n        description: Description of the dataset's purpose and contents.\n        execution_rid: Optional execution RID to associate with dataset creation.\n        version: Optional initial version number. Defaults to 0.1.0.\n\n    Returns:\n        RID: Resource Identifier of the newly created dataset.\n\n    Raises:\n        DerivaMLException: If dataset_types are invalid or creation fails.\n\n    Example:\n        &gt;&gt;&gt; rid = ml.create_dataset(\n        ...     dataset_types=[\"experiment\", \"raw_data\"],\n        ...     description=\"RNA sequencing experiment data\",\n        ...     version=DatasetVersion(1, 0, 0)\n        ... )\n    \"\"\"\n\n    version = version or DatasetVersion(0, 1, 0)\n    dataset_types = dataset_types or []\n\n    type_path = self._model.catalog.getPathBuilder().schemas[self._ml_schema].tables[MLVocab.dataset_type.value]\n    defined_types = list(type_path.entities().fetch())\n\n    def check_dataset_type(dtype: str) -&gt; bool:\n        for term in defined_types:\n            if dtype == term[\"Name\"] or (term[\"Synonyms\"] and ds_type in term[\"Synonyms\"]):\n                return True\n        return False\n\n    # Create the entry for the new dataset_table and get its RID.\n    ds_types = [dataset_types] if isinstance(dataset_types, str) else dataset_types\n    pb = self._model.catalog.getPathBuilder()\n    for ds_type in ds_types:\n        if not check_dataset_type(ds_type):\n            raise DerivaMLException(\"Dataset type must be a vocabulary term.\")\n    dataset_table_path = pb.schemas[self._dataset_table.schema.name].tables[self._dataset_table.name]\n    dataset_rid = dataset_table_path.insert(\n        [\n            {\n                \"Description\": description,\n                \"Deleted\": False,\n            }\n        ]\n    )[0][\"RID\"]\n\n    # Get the name of the association table between dataset_table and dataset_type.\n    associations = list(self._model.schemas[self._ml_schema].tables[MLVocab.dataset_type].find_associations())\n    atable = associations[0].name if associations else None\n    pb.schemas[self._ml_schema].tables[atable].insert(\n        [{MLVocab.dataset_type: ds_type, \"Dataset\": dataset_rid} for ds_type in ds_types]\n    )\n    if execution_rid is not None:\n        pb.schemas[self._ml_schema].Dataset_Execution.insert([{\"Dataset\": dataset_rid, \"Execution\": execution_rid}])\n    self._insert_dataset_versions(\n        [DatasetSpec(rid=dataset_rid, version=version)],\n        execution_rid=execution_rid,\n        description=\"Initial dataset creation.\",\n    )\n    return dataset_rid\n</code></pre>"},{"location":"code-docs/dataset/#deriva_ml.dataset.Dataset.dataset_history","title":"dataset_history","text":"<pre><code>dataset_history(\n    dataset_rid: RID,\n) -&gt; list[DatasetHistory]\n</code></pre> <p>Retrieves the version history of a dataset.</p> <p>Returns a chronological list of dataset versions, including their version numbers, creation times, and associated metadata.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_rid</code> <code>RID</code> <p>Resource Identifier of the dataset.</p> required <p>Returns:</p> Type Description <code>list[DatasetHistory]</code> <p>list[DatasetHistory]: List of history entries, each containing: - dataset_version: Version number (major.minor.patch) - minid: Minimal Viable Identifier - snapshot: Catalog snapshot time - dataset_rid: Dataset Resource Identifier - version_rid: Version Resource Identifier - description: Version description - execution_rid: Associated execution RID</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If dataset_rid is not a valid dataset RID.</p> Example <p>history = ml.dataset_history(\"1-abc123\") for entry in history: ...     print(f\"Version {entry.dataset_version}: {entry.description}\")</p> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>def dataset_history(self, dataset_rid: RID) -&gt; list[DatasetHistory]:\n    \"\"\"Retrieves the version history of a dataset.\n\n    Returns a chronological list of dataset versions, including their version numbers,\n    creation times, and associated metadata.\n\n    Args:\n        dataset_rid: Resource Identifier of the dataset.\n\n    Returns:\n        list[DatasetHistory]: List of history entries, each containing:\n            - dataset_version: Version number (major.minor.patch)\n            - minid: Minimal Viable Identifier\n            - snapshot: Catalog snapshot time\n            - dataset_rid: Dataset Resource Identifier\n            - version_rid: Version Resource Identifier\n            - description: Version description\n            - execution_rid: Associated execution RID\n\n    Raises:\n        DerivaMLException: If dataset_rid is not a valid dataset RID.\n\n    Example:\n        &gt;&gt;&gt; history = ml.dataset_history(\"1-abc123\")\n        &gt;&gt;&gt; for entry in history:\n        ...     print(f\"Version {entry.dataset_version}: {entry.description}\")\n    \"\"\"\n\n    if not self._is_dataset_rid(dataset_rid):\n        raise DerivaMLException(f\"RID is not for a data set: {dataset_rid}\")\n    version_path = self._model.catalog.getPathBuilder().schemas[self._ml_schema].tables[\"Dataset_Version\"]\n    return [\n        DatasetHistory(\n            dataset_version=DatasetVersion.parse(v[\"Version\"]),\n            minid=v[\"Minid\"],\n            snapshot=v[\"Snapshot\"],\n            dataset_rid=dataset_rid,\n            version_rid=v[\"RID\"],\n            description=v[\"Description\"],\n            execution_rid=v[\"Execution\"],\n        )\n        for v in version_path.filter(version_path.Dataset == dataset_rid).entities().fetch()\n    ]\n</code></pre>"},{"location":"code-docs/dataset/#deriva_ml.dataset.Dataset.dataset_version","title":"dataset_version","text":"<pre><code>dataset_version(\n    dataset_rid: RID,\n) -&gt; DatasetVersion\n</code></pre> <p>Retrieve the current version of the specified dataset_table.</p> <p>Given a rid, return the most recent version of the dataset. It is important to remember that this version captures the state of the catalog at the time the version was created, not the current state of the catalog. This means that its possible that the values associated with an object in the catalog may be different from the values of that object in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_rid</code> <code>RID</code> <p>The RID of the dataset to retrieve the version for.</p> required <p>Returns:</p> Type Description <code>DatasetVersion</code> <p>A tuple with the semantic version of the dataset_table.</p> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>@validate_call\ndef dataset_version(self, dataset_rid: RID) -&gt; DatasetVersion:\n    \"\"\"Retrieve the current version of the specified dataset_table.\n\n    Given a rid, return the most recent version of the dataset. It is important to remember that this version\n    captures the state of the catalog at the time the version was created, not the current state of the catalog.\n    This means that its possible that the values associated with an object in the catalog may be different\n    from the values of that object in the dataset.\n\n    Args:\n        dataset_rid: The RID of the dataset to retrieve the version for.\n\n    Returns:\n        A tuple with the semantic version of the dataset_table.\n    \"\"\"\n    history = self.dataset_history(dataset_rid)\n    if not history:\n        return DatasetVersion(0, 1, 0)\n    else:\n        # Ensure we return a DatasetVersion, not a string\n        versions = [h.dataset_version for h in history]\n        return max(versions) if versions else DatasetVersion(0, 1, 0)\n</code></pre>"},{"location":"code-docs/dataset/#deriva_ml.dataset.Dataset.delete_dataset","title":"delete_dataset","text":"<pre><code>delete_dataset(\n    dataset_rid: RID,\n    recurse: bool = False,\n) -&gt; None\n</code></pre> <p>Delete a dataset_table from the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_rid</code> <code>RID</code> <p>RID of the dataset_table to delete.</p> required <code>recurse</code> <code>bool</code> <p>If True, delete the dataset_table along with any nested datasets. (Default value = False)</p> <code>False</code> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>@validate_call\ndef delete_dataset(self, dataset_rid: RID, recurse: bool = False) -&gt; None:\n    \"\"\"Delete a dataset_table from the catalog.\n\n    Args:\n        dataset_rid: RID of the dataset_table to delete.\n        recurse: If True, delete the dataset_table along with any nested datasets. (Default value = False)\n    \"\"\"\n    # Get association table entries for this dataset_table\n    # Delete association table entries\n    if not self._is_dataset_rid(dataset_rid):\n        raise DerivaMLException(\"Dataset_rid is not a dataset.\")\n\n    if parents := self.list_dataset_parents(dataset_rid):\n        raise DerivaMLException(f'Dataset_rid \"{dataset_rid}\" is in a nested dataset: {parents}.')\n\n    pb = self._model.catalog.getPathBuilder()\n    dataset_path = pb.schemas[self._dataset_table.schema.name].tables[self._dataset_table.name]\n\n    rid_list = [dataset_rid] + (self.list_dataset_children(dataset_rid=dataset_rid) if recurse else [])\n    dataset_path.update([{\"RID\": r, \"Deleted\": True} for r in rid_list])\n</code></pre>"},{"location":"code-docs/dataset/#deriva_ml.dataset.Dataset.delete_dataset_members","title":"delete_dataset_members","text":"<pre><code>delete_dataset_members(\n    dataset_rid: RID,\n    members: list[RID],\n    description: str = \"\",\n    execution_rid: RID | None = None,\n) -&gt; None\n</code></pre> <p>Remove elements to an existing dataset_table.</p> <p>Delete elements from an existing dataset. In addition to deleting members, the minor version number of the dataset is incremented and the description, if provide is applied to that new version.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_rid</code> <code>RID</code> <p>RID of dataset_table to extend or None if a new dataset_table is to be created.</p> required <code>members</code> <code>list[RID]</code> <p>List of member RIDs to add to the dataset_table.</p> required <code>description</code> <code>str</code> <p>Markdown description of the updated dataset.</p> <code>''</code> <code>execution_rid</code> <code>RID | None</code> <p>Optional RID of execution associated with this operation.</p> <code>None</code> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>@validate_call\ndef delete_dataset_members(\n    self,\n    dataset_rid: RID,\n    members: list[RID],\n    description: str = \"\",\n    execution_rid: RID | None = None,\n) -&gt; None:\n    \"\"\"Remove elements to an existing dataset_table.\n\n    Delete elements from an existing dataset. In addition to deleting members, the minor version number of the\n    dataset is incremented and the description, if provide is applied to that new version.\n\n    Args:\n        dataset_rid: RID of dataset_table to extend or None if a new dataset_table is to be created.\n        members: List of member RIDs to add to the dataset_table.\n        description: Markdown description of the updated dataset.\n        execution_rid: Optional RID of execution associated with this operation.\n    \"\"\"\n\n    members = set(members)\n    description = description or \"Deletes dataset members\"\n\n    # Now go through every rid to be added to the data set and sort them based on what association table entries\n    # need to be made.\n    dataset_elements = {}\n    association_map = {\n        a.other_fkeys.pop().pk_table.name: a.table.name for a in self._dataset_table.find_associations()\n    }\n    # Get a list of all the object types that can be linked to a dataset_table.\n    for m in members:\n        try:\n            rid_info = self._model.catalog.resolve_rid(m)\n        except KeyError:\n            raise DerivaMLException(f\"Invalid RID: {m}\")\n        if rid_info.table.name not in association_map:\n            raise DerivaMLException(f\"RID table: {rid_info.table.name} not part of dataset_table\")\n        dataset_elements.setdefault(rid_info.table.name, []).append(rid_info.rid)\n    # Now make the entries into the association tables.\n    pb = self._model.catalog.getPathBuilder()\n    for table, elements in dataset_elements.items():\n        schema_path = pb.schemas[self._ml_schema if table == \"Dataset\" else self._model.domain_schema]\n        fk_column = \"Nested_Dataset\" if table == \"Dataset\" else table\n\n        if len(elements):\n            atable_path = schema_path.tables[association_map[table]]\n            # Find out the name of the column in the association table.\n            for e in elements:\n                entity = atable_path.filter(\n                    (atable_path.Dataset == dataset_rid) &amp; (atable_path.columns[fk_column] == e),\n                )\n                entity.delete()\n    self.increment_dataset_version(\n        dataset_rid,\n        VersionPart.minor,\n        description=description,\n        execution_rid=execution_rid,\n    )\n</code></pre>"},{"location":"code-docs/dataset/#deriva_ml.dataset.Dataset.find_datasets","title":"find_datasets","text":"<pre><code>find_datasets(\n    deleted: bool = False,\n) -&gt; Iterable[dict[str, Any]]\n</code></pre> <p>Returns a list of currently available datasets.</p> <p>Parameters:</p> Name Type Description Default <code>deleted</code> <code>bool</code> <p>If True, included the datasets that have been deleted.</p> <code>False</code> <p>Returns:</p> Type Description <code>Iterable[dict[str, Any]]</code> <p>list of currently available datasets.</p> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>def find_datasets(self, deleted: bool = False) -&gt; Iterable[dict[str, Any]]:\n    \"\"\"Returns a list of currently available datasets.\n\n    Arguments:\n        deleted: If True, included the datasets that have been deleted.\n\n    Returns:\n         list of currently available datasets.\n    \"\"\"\n    # Get datapath to all the tables we will need: Dataset, DatasetType and the association table.\n    pb = self._model.catalog.getPathBuilder()\n    dataset_path = pb.schemas[self._dataset_table.schema.name].tables[self._dataset_table.name]\n    associations = list(self._model.schemas[self._ml_schema].tables[MLVocab.dataset_type].find_associations())\n    atable = associations[0].name if associations else None\n    ml_path = pb.schemas[self._ml_schema]\n    atable_path = ml_path.tables[atable]\n\n    if deleted:\n        filtered_path = dataset_path\n    else:\n        filtered_path = dataset_path.filter(\n            (dataset_path.Deleted == False) | (dataset_path.Deleted == None)  # noqa: E711, E712\n        )\n\n    # Get a list of all the dataset_type values associated with this dataset_table.\n    datasets = []\n    for dataset in filtered_path.entities().fetch():\n        ds_types = (\n            atable_path.filter(atable_path.Dataset == dataset[\"RID\"]).attributes(atable_path.Dataset_Type).fetch()\n        )\n        datasets.append(dataset | {MLVocab.dataset_type: [ds[MLVocab.dataset_type] for ds in ds_types]})\n    return datasets\n</code></pre>"},{"location":"code-docs/dataset/#deriva_ml.dataset.Dataset.increment_dataset_version","title":"increment_dataset_version","text":"<pre><code>increment_dataset_version(\n    dataset_rid: RID,\n    component: VersionPart,\n    description: str | None = \"\",\n    execution_rid: RID | None = None,\n) -&gt; DatasetVersion\n</code></pre> <p>Increments a dataset's version number.</p> <p>Creates a new version of the dataset by incrementing the specified version component (major, minor, or patch). The new version is recorded with an optional description and execution reference.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_rid</code> <code>RID</code> <p>Resource Identifier of the dataset to version.</p> required <code>component</code> <code>VersionPart</code> <p>Which version component to increment ('major', 'minor', or 'patch').</p> required <code>description</code> <code>str | None</code> <p>Optional description of the changes in this version.</p> <code>''</code> <code>execution_rid</code> <code>RID | None</code> <p>Optional execution RID to associate with this version.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DatasetVersion</code> <code>DatasetVersion</code> <p>The new version number.</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If dataset_rid is invalid or version increment fails.</p> Example <p>new_version = ml.increment_dataset_version( ...     dataset_rid=\"1-abc123\", ...     component=\"minor\", ...     description=\"Added new samples\" ... ) print(f\"New version: {new_version}\")  # e.g., \"1.2.0\"</p> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>@validate_call(config=ConfigDict(arbitrary_types_allowed=True))\ndef increment_dataset_version(\n    self,\n    dataset_rid: RID,\n    component: VersionPart,\n    description: str | None = \"\",\n    execution_rid: RID | None = None,\n) -&gt; DatasetVersion:\n    \"\"\"Increments a dataset's version number.\n\n    Creates a new version of the dataset by incrementing the specified version component\n    (major, minor, or patch). The new version is recorded with an optional description\n    and execution reference.\n\n    Args:\n        dataset_rid: Resource Identifier of the dataset to version.\n        component: Which version component to increment ('major', 'minor', or 'patch').\n        description: Optional description of the changes in this version.\n        execution_rid: Optional execution RID to associate with this version.\n\n    Returns:\n        DatasetVersion: The new version number.\n\n    Raises:\n        DerivaMLException: If dataset_rid is invalid or version increment fails.\n\n    Example:\n        &gt;&gt;&gt; new_version = ml.increment_dataset_version(\n        ...     dataset_rid=\"1-abc123\",\n        ...     component=\"minor\",\n        ...     description=\"Added new samples\"\n        ... )\n        &gt;&gt;&gt; print(f\"New version: {new_version}\")  # e.g., \"1.2.0\"\n    \"\"\"\n\n    # Find all the datasets that are reachable from this dataset and determine their new version numbers.\n    related_datasets = list(self._build_dataset_graph(dataset_rid=dataset_rid))\n    version_update_list = [\n        DatasetSpec(\n            rid=ds_rid,\n            version=self.dataset_version(ds_rid).increment_version(component),\n        )\n        for ds_rid in related_datasets\n    ]\n    self._insert_dataset_versions(version_update_list, description=description, execution_rid=execution_rid)\n    return next((d.version for d in version_update_list if d.rid == dataset_rid))\n</code></pre>"},{"location":"code-docs/dataset/#deriva_ml.dataset.Dataset.list_dataset_children","title":"list_dataset_children","text":"<pre><code>list_dataset_children(\n    dataset_rid: RID,\n    recurse: bool = False,\n) -&gt; list[RID]\n</code></pre> <p>Given a dataset_table RID, return a list of RIDs for any nested datasets.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_rid</code> <code>RID</code> <p>A dataset_table RID.</p> required <code>recurse</code> <code>bool</code> <p>If True, return a list of nested datasets RIDs.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[RID]</code> <p>list of nested dataset RIDs.</p> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>@validate_call\ndef list_dataset_children(self, dataset_rid: RID, recurse: bool = False) -&gt; list[RID]:\n    \"\"\"Given a dataset_table RID, return a list of RIDs for any nested datasets.\n\n    Args:\n        dataset_rid: A dataset_table RID.\n        recurse: If True, return a list of nested datasets RIDs.\n\n    Returns:\n      list of nested dataset RIDs.\n\n    \"\"\"\n    dataset_dataset_path = self._model.catalog.getPathBuilder().schemas[self._ml_schema].tables[\"Dataset_Dataset\"]\n    nested_datasets = list(dataset_dataset_path.entities().fetch())\n\n    def find_children(rid: RID):\n        children = [child[\"Nested_Dataset\"] for child in nested_datasets if child[\"Dataset\"] == rid]\n        if recurse:\n            for child in children.copy():\n                children.extend(find_children(child))\n        return children\n\n    return find_children(dataset_rid)\n</code></pre>"},{"location":"code-docs/dataset/#deriva_ml.dataset.Dataset.list_dataset_element_types","title":"list_dataset_element_types","text":"<pre><code>list_dataset_element_types() -&gt; (\n    Iterable[Table]\n)\n</code></pre> <p>List the types of entities that can be added to a dataset_table.</p> <p>Returns:</p> Type Description <code>Iterable[Table]</code> <p>return: An iterable of Table objects that can be included as an element of a dataset_table.</p> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>def list_dataset_element_types(self) -&gt; Iterable[Table]:\n    \"\"\"List the types of entities that can be added to a dataset_table.\n\n    Returns:\n      :return: An iterable of Table objects that can be included as an element of a dataset_table.\n    \"\"\"\n\n    def domain_table(table: Table) -&gt; bool:\n        return table.schema.name == self._model.domain_schema or table.name == self._dataset_table.name\n\n    return [t for a in self._dataset_table.find_associations() if domain_table(t := a.other_fkeys.pop().pk_table)]\n</code></pre>"},{"location":"code-docs/dataset/#deriva_ml.dataset.Dataset.list_dataset_members","title":"list_dataset_members","text":"<pre><code>list_dataset_members(\n    dataset_rid: RID,\n    recurse: bool = False,\n    limit: int | None = None,\n) -&gt; dict[str, list[dict[str, Any]]]\n</code></pre> <p>Lists members of a dataset.</p> <p>Returns a dictionary mapping member types to lists of member records. Can optionally recurse through nested datasets and limit the number of results.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_rid</code> <code>RID</code> <p>Resource Identifier of the dataset.</p> required <code>recurse</code> <code>bool</code> <p>Whether to include members of nested datasets. Defaults to False.</p> <code>False</code> <code>limit</code> <code>int | None</code> <p>Maximum number of members to return per type. None for no limit.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, list[dict[str, Any]]]</code> <p>dict[str, list[dict[str, Any]]]: Dictionary mapping member types to lists of members. Each member is a dictionary containing the record's attributes.</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If dataset_rid is invalid.</p> Example <p>members = ml.list_dataset_members(\"1-abc123\", recurse=True) for type_name, records in members.items(): ...     print(f\"{type_name}: {len(records)} records\")</p> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>def list_dataset_members(\n    self, dataset_rid: RID, recurse: bool = False, limit: int | None = None\n) -&gt; dict[str, list[dict[str, Any]]]:\n    \"\"\"Lists members of a dataset.\n\n    Returns a dictionary mapping member types to lists of member records. Can optionally\n    recurse through nested datasets and limit the number of results.\n\n    Args:\n        dataset_rid: Resource Identifier of the dataset.\n        recurse: Whether to include members of nested datasets. Defaults to False.\n        limit: Maximum number of members to return per type. None for no limit.\n\n    Returns:\n        dict[str, list[dict[str, Any]]]: Dictionary mapping member types to lists of members.\n            Each member is a dictionary containing the record's attributes.\n\n    Raises:\n        DerivaMLException: If dataset_rid is invalid.\n\n    Example:\n        &gt;&gt;&gt; members = ml.list_dataset_members(\"1-abc123\", recurse=True)\n        &gt;&gt;&gt; for type_name, records in members.items():\n        ...     print(f\"{type_name}: {len(records)} records\")\n    \"\"\"\n\n    if not self._is_dataset_rid(dataset_rid):\n        raise DerivaMLException(f\"RID is not for a dataset_table: {dataset_rid}\")\n\n    # Look at each of the element types that might be in the dataset_table and get the list of rid for them from\n    # the appropriate association table.\n    members = defaultdict(list)\n    pb = self._model.catalog.getPathBuilder()\n    for assoc_table in self._dataset_table.find_associations():\n        other_fkey = assoc_table.other_fkeys.pop()\n        target_table = other_fkey.pk_table\n        member_table = assoc_table.table\n\n        # Look at domain tables and nested datasets.\n        if target_table.schema.name != self._model.domain_schema and not (\n            target_table == self._dataset_table or target_table.name == \"File\"\n        ):\n            continue\n        member_column = (\n            \"Nested_Dataset\" if target_table == self._dataset_table else other_fkey.foreign_key_columns[0].name\n        )\n\n        target_path = pb.schemas[target_table.schema.name].tables[target_table.name]\n        member_path = pb.schemas[member_table.schema.name].tables[member_table.name]\n\n        path = member_path.filter(member_path.Dataset == dataset_rid).link(\n            target_path,\n            on=(member_path.columns[member_column] == target_path.columns[\"RID\"]),\n        )\n        target_entities = list(path.entities().fetch(limit=limit) if limit else path.entities().fetch())\n        members[target_table.name].extend(target_entities)\n        if recurse and target_table == self._dataset_table:\n            # Get the members for all the nested datasets and add to the member list.\n            nested_datasets = [d[\"RID\"] for d in target_entities]\n            for ds in nested_datasets:\n                for k, v in self.list_dataset_members(ds, recurse=recurse).items():\n                    members[k].extend(v)\n    return dict(members)\n</code></pre>"},{"location":"code-docs/dataset/#deriva_ml.dataset.Dataset.list_dataset_parents","title":"list_dataset_parents","text":"<pre><code>list_dataset_parents(\n    dataset_rid: RID,\n) -&gt; list[str]\n</code></pre> <p>Given a dataset_table RID, return a list of RIDs of the parent datasets if this is included in a nested dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_rid</code> <code>RID</code> <p>return: RID of the parent dataset_table.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>RID of the parent dataset_table.</p> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>@validate_call\ndef list_dataset_parents(self, dataset_rid: RID) -&gt; list[str]:\n    \"\"\"Given a dataset_table RID, return a list of RIDs of the parent datasets if this is included in a\n    nested dataset.\n\n    Args:\n        dataset_rid: return: RID of the parent dataset_table.\n\n    Returns:\n        RID of the parent dataset_table.\n    \"\"\"\n    if not self._is_dataset_rid(dataset_rid):\n        raise DerivaMLException(f\"RID: {dataset_rid} does not belong to dataset_table {self._dataset_table.name}\")\n    # Get association table for nested datasets\n    pb = self._model.catalog.getPathBuilder()\n    atable_path = pb.schemas[self._ml_schema].Dataset_Dataset\n    return [p[\"Dataset\"] for p in atable_path.filter(atable_path.Nested_Dataset == dataset_rid).entities().fetch()]\n</code></pre>"},{"location":"code-docs/dataset/#deriva_ml.dataset.DatasetSpec","title":"DatasetSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represent a dataset_table in an execution configuration dataset_table list</p> <p>Attributes:</p> Name Type Description <code>rid</code> <code>RID</code> <p>A dataset_table RID</p> <code>materialize</code> <code>bool</code> <p>If False do not materialize datasets, only download table data, no assets.  Defaults to True</p> <code>version</code> <code>DatasetVersion</code> <p>The version of the dataset.  Should follow semantic versioning.</p> Source code in <code>src/deriva_ml/dataset/aux_classes.py</code> <pre><code>class DatasetSpec(BaseModel):\n    \"\"\"Represent a dataset_table in an execution configuration dataset_table list\n\n    Attributes:\n        rid (RID): A dataset_table RID\n        materialize (bool): If False do not materialize datasets, only download table data, no assets.  Defaults to True\n        version (DatasetVersion): The version of the dataset.  Should follow semantic versioning.\n    \"\"\"\n\n    rid: RID\n    materialize: bool = True\n    version: DatasetVersion | conlist(item_type=int, min_length=3, max_length=3) | tuple[int, int, int] | str\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @field_validator(\"version\", mode=\"before\")\n    @classmethod\n    def version_field_validator(cls, v: Any) -&gt; Any:\n        if isinstance(v, dict):\n            return DatasetVersion(**v)\n        elif isinstance(v, str):\n            return DatasetVersion.parse(v)\n        elif (isinstance(v, list) or isinstance(v, tuple)) and len(v) == 3:\n            return DatasetVersion(int(v[0]), int(v[1]), int(v[2]))\n        else:\n            return v\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def _check_bare_rid(cls, data: Any) -&gt; dict[str, str | bool]:\n        # If you are just given a string, assume it's a rid and put into dict for further validation.\n        return {\"rid\": data} if isinstance(data, str) else data\n\n    @field_serializer(\"version\")\n    def serialize_version(self, version: DatasetVersion) -&gt; dict[str, Any]:\n        return version.to_dict()\n</code></pre>"},{"location":"code-docs/dataset_aux_classes/","title":"DatasetBag Class","text":"<p>THis module defines the DataSet class with is used to manipulate n</p>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.DatasetHistory","title":"DatasetHistory","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class representing a dataset history.</p> <p>Attributes:</p> Name Type Description <code>dataset_version</code> <code>DatasetVersion</code> <p>A DatasetVersion object which captures the semantic versioning of the dataset.</p> <code>dataset_rid</code> <code>RID</code> <p>The RID of the dataset.</p> <code>version_rid</code> <code>RID</code> <p>The RID of the version record for the dataset in the Dataset_Version table.</p> <code>minid</code> <code>str</code> <p>The URL that represents the handle of the dataset bag.  This will be None if a MINID has not          been created yet.</p> <code>snapshot</code> <code>str</code> <p>Catalog snapshot ID of when the version record was created.</p> Source code in <code>src/deriva_ml/dataset/aux_classes.py</code> <pre><code>class DatasetHistory(BaseModel):\n    \"\"\"\n    Class representing a dataset history.\n\n    Attributes:\n        dataset_version (DatasetVersion): A DatasetVersion object which captures the semantic versioning of the dataset.\n        dataset_rid (RID): The RID of the dataset.\n        version_rid (RID): The RID of the version record for the dataset in the Dataset_Version table.\n        minid (str): The URL that represents the handle of the dataset bag.  This will be None if a MINID has not\n                     been created yet.\n        snapshot (str): Catalog snapshot ID of when the version record was created.\n    \"\"\"\n\n    dataset_version: DatasetVersion\n    dataset_rid: RID\n    version_rid: RID\n    execution_rid: Optional[RID] = None\n    description: str = \"\"\n    minid: Optional[str] = None\n    snapshot: Optional[str] = None\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.DatasetMinid","title":"DatasetMinid","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represent information about a MINID that refers to a dataset</p> <p>Attributes:</p> Name Type Description <code>dataset_version</code> <code>DatasetVersion</code> <p>A DatasetVersion object which captures the semantic versioning of the dataset.</p> <code>metadata</code> <code>dict</code> <p>A dictionary containing metadata from the MINID landing page.</p> <code>minid</code> <code>str</code> <p>The URL that represents the handle of the MINID associated with the dataset.</p> <code>bag_url</code> <code>str</code> <p>The URL to the dataset bag</p> <code>identifier</code> <code>str</code> <p>The identifier of the MINID in CURI form</p> <code>landing_page</code> <code>str</code> <p>The URL to the landing page of the MINID</p> <code>version_rid</code> <code>str</code> <p>RID of the dataset version.</p> <code>checksum</code> <code>str</code> <p>The checksum of the MINID in SHA256 form</p> Source code in <code>src/deriva_ml/dataset/aux_classes.py</code> <pre><code>class DatasetMinid(BaseModel):\n    \"\"\"Represent information about a MINID that refers to a dataset\n\n    Attributes:\n        dataset_version (DatasetVersion): A DatasetVersion object which captures the semantic versioning of the dataset.\n        metadata (dict): A dictionary containing metadata from the MINID landing page.\n        minid (str): The URL that represents the handle of the MINID associated with the dataset.\n        bag_url (str): The URL to the dataset bag\n        identifier (str): The identifier of the MINID in CURI form\n        landing_page (str): The URL to the landing page of the MINID\n        version_rid (str): RID of the dataset version.\n        checksum (str): The checksum of the MINID in SHA256 form\n\n    \"\"\"\n\n    dataset_version: DatasetVersion\n    metadata: dict[str, str | int] = {}\n    minid: str = Field(alias=\"compact_uri\", default=None)\n    bag_url: str = Field(alias=\"location\")\n    identifier: Optional[str] = None\n    landing_page: Optional[str] = None\n    version_rid: RID = Field(alias=\"RID\")\n    checksum: str = Field(alias=\"checksums\", default=\"\")\n\n    @computed_field\n    @property\n    def dataset_rid(self) -&gt; str:\n        rid_parts = self.version_rid.split(\"@\")\n        return rid_parts[0]\n\n    @computed_field\n    @property\n    def dataset_snapshot(self) -&gt; str:\n        return self.version_rid.split(\"@\")[1]\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def insert_metadata(cls, data: Any) -&gt; Any:\n        if isinstance(data, dict):\n            if \"metadata\" in data:\n                data = data | data[\"metadata\"]\n        return data\n\n    @field_validator(\"bag_url\", mode=\"before\")\n    @classmethod\n    def convert_location_to_str(cls, value: list[str] | str) -&gt; str:\n        return value[0] if isinstance(value, list) else value\n\n    @field_validator(\"checksum\", mode=\"before\")\n    @classmethod\n    def convert_checksum_to_value(cls, checksums: list[dict]) -&gt; str:\n        checksum_value = \"\"\n        for checksum in checksums:\n            if checksum.get(\"function\") == \"sha256\":\n                checksum_value = checksum.get(\"value\")\n                break\n        return checksum_value\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.DatasetSpec","title":"DatasetSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represent a dataset_table in an execution configuration dataset_table list</p> <p>Attributes:</p> Name Type Description <code>rid</code> <code>RID</code> <p>A dataset_table RID</p> <code>materialize</code> <code>bool</code> <p>If False do not materialize datasets, only download table data, no assets.  Defaults to True</p> <code>version</code> <code>DatasetVersion</code> <p>The version of the dataset.  Should follow semantic versioning.</p> Source code in <code>src/deriva_ml/dataset/aux_classes.py</code> <pre><code>class DatasetSpec(BaseModel):\n    \"\"\"Represent a dataset_table in an execution configuration dataset_table list\n\n    Attributes:\n        rid (RID): A dataset_table RID\n        materialize (bool): If False do not materialize datasets, only download table data, no assets.  Defaults to True\n        version (DatasetVersion): The version of the dataset.  Should follow semantic versioning.\n    \"\"\"\n\n    rid: RID\n    materialize: bool = True\n    version: DatasetVersion | conlist(item_type=int, min_length=3, max_length=3) | tuple[int, int, int] | str\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @field_validator(\"version\", mode=\"before\")\n    @classmethod\n    def version_field_validator(cls, v: Any) -&gt; Any:\n        if isinstance(v, dict):\n            return DatasetVersion(**v)\n        elif isinstance(v, str):\n            return DatasetVersion.parse(v)\n        elif (isinstance(v, list) or isinstance(v, tuple)) and len(v) == 3:\n            return DatasetVersion(int(v[0]), int(v[1]), int(v[2]))\n        else:\n            return v\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def _check_bare_rid(cls, data: Any) -&gt; dict[str, str | bool]:\n        # If you are just given a string, assume it's a rid and put into dict for further validation.\n        return {\"rid\": data} if isinstance(data, str) else data\n\n    @field_serializer(\"version\")\n    def serialize_version(self, version: DatasetVersion) -&gt; dict[str, Any]:\n        return version.to_dict()\n</code></pre>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.DatasetVersion","title":"DatasetVersion","text":"<p>               Bases: <code>Version</code></p> <p>Represent the version associated with a dataset using semantic versioning.</p> <p>Methods:</p> Name Description <code>replace</code> <p>Replace the major and minor versions</p> Source code in <code>src/deriva_ml/dataset/aux_classes.py</code> <pre><code>class DatasetVersion(Version):\n    \"\"\"Represent the version associated with a dataset using semantic versioning.\n\n    Methods:\n        replace(major, minor, patch): Replace the major and minor versions\n    \"\"\"\n\n    def __init__(self, major: SupportsInt, minor: SupportsInt = 0, patch: SupportsInt = 0):\n        \"\"\"Initialize a DatasetVersion object.\n\n        Args:\n            major: Major version number. Used to indicate schema changes.\n            minor: Minor version number.  Used to indicate additional members added, or change in member values.\n            patch: Patch number of the dataset.  Used to indicate minor clean-up and edits\n        \"\"\"\n        super().__init__(major, minor, patch)\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"\n\n        Returns:\n            dictionary of version information\n\n        \"\"\"\n        return {\"major\": self.major, \"minor\": self.minor, \"patch\": self.patch}\n\n    def to_tuple(self) -&gt; tuple[int, int, int]:\n        \"\"\"\n\n        Returns:\n            tuple of version information\n\n        \"\"\"\n        return self.major, self.minor, self.patch\n\n    @classmethod\n    def parse(cls, version: str, optional_minor_an_path=False) -&gt; \"DatasetVersion\":\n        v = Version.parse(version)\n        return DatasetVersion(v.major, v.minor, v.patch)\n\n    def increment_version(self, component: VersionPart) -&gt; \"DatasetVersion\":\n        match component:\n            case VersionPart.major:\n                return self.bump_major()\n            case VersionPart.minor:\n                return self.bump_minor()\n            case VersionPart.patch:\n                return self.bump_patch()\n            case _:\n                return self\n</code></pre>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.DatasetVersion.build","title":"build  <code>property</code> <code>writable</code>","text":"<pre><code>build: Optional[str]\n</code></pre> <p>The build part of a version (read-only).</p>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.DatasetVersion.major","title":"major  <code>property</code> <code>writable</code>","text":"<pre><code>major: int\n</code></pre> <p>The major part of a version (read-only).</p>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.DatasetVersion.minor","title":"minor  <code>property</code> <code>writable</code>","text":"<pre><code>minor: int\n</code></pre> <p>The minor part of a version (read-only).</p>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.DatasetVersion.patch","title":"patch  <code>property</code> <code>writable</code>","text":"<pre><code>patch: int\n</code></pre> <p>The patch part of a version (read-only).</p>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.DatasetVersion.prerelease","title":"prerelease  <code>property</code> <code>writable</code>","text":"<pre><code>prerelease: Optional[str]\n</code></pre> <p>The prerelease part of a version (read-only).</p>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.DatasetVersion.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(\n    index: Union[int, slice],\n) -&gt; Union[\n    int,\n    Optional[str],\n    Tuple[Union[int, str], ...],\n]\n</code></pre> <p>self.getitem(index) &lt;==&gt; self[index] Implement getitem.</p> <p>If the part  requested is undefined, or a part of the range requested is undefined, it will throw an index error. Negative indices are not supported.</p> <p>:param index: a positive integer indicating the        offset or a :func:<code>slice</code> object :raises IndexError: if index is beyond the range or a part is None :return: the requested part of the version at position index</p> <p>ver = semver.Version.parse(\"3.4.5\") ver[0], ver[1], ver[2] (3, 4, 5)</p> Source code in <code>semver/version.py</code> <pre><code>def __getitem__(\n    self, index: Union[int, slice]\n) -&gt; Union[int, Optional[str], Tuple[Union[int, str], ...]]:\n    \"\"\"\n    self.__getitem__(index) &lt;==&gt; self[index] Implement getitem.\n\n    If the part  requested is undefined, or a part of the range requested\n    is undefined, it will throw an index error.\n    Negative indices are not supported.\n\n    :param index: a positive integer indicating the\n           offset or a :func:`slice` object\n    :raises IndexError: if index is beyond the range or a part is None\n    :return: the requested part of the version at position index\n\n    &gt;&gt;&gt; ver = semver.Version.parse(\"3.4.5\")\n    &gt;&gt;&gt; ver[0], ver[1], ver[2]\n    (3, 4, 5)\n    \"\"\"\n    if isinstance(index, int):\n        index = slice(index, index + 1)\n    index = cast(slice, index)\n\n    if (\n        isinstance(index, slice)\n        and (index.start is not None and index.start &lt; 0)\n        or (index.stop is not None and index.stop &lt; 0)\n    ):\n        raise IndexError(\"Version index cannot be negative\")\n\n    part = tuple(\n        filter(lambda p: p is not None, cast(Iterable, self.to_tuple()[index]))\n    )\n\n    if len(part) == 1:\n        return part[0]\n    elif not part:\n        raise IndexError(\"Version part undefined\")\n    return part\n</code></pre>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.DatasetVersion.__init__","title":"__init__","text":"<pre><code>__init__(\n    major: SupportsInt,\n    minor: SupportsInt = 0,\n    patch: SupportsInt = 0,\n)\n</code></pre> <p>Initialize a DatasetVersion object.</p> <p>Parameters:</p> Name Type Description Default <code>major</code> <code>SupportsInt</code> <p>Major version number. Used to indicate schema changes.</p> required <code>minor</code> <code>SupportsInt</code> <p>Minor version number.  Used to indicate additional members added, or change in member values.</p> <code>0</code> <code>patch</code> <code>SupportsInt</code> <p>Patch number of the dataset.  Used to indicate minor clean-up and edits</p> <code>0</code> Source code in <code>src/deriva_ml/dataset/aux_classes.py</code> <pre><code>def __init__(self, major: SupportsInt, minor: SupportsInt = 0, patch: SupportsInt = 0):\n    \"\"\"Initialize a DatasetVersion object.\n\n    Args:\n        major: Major version number. Used to indicate schema changes.\n        minor: Minor version number.  Used to indicate additional members added, or change in member values.\n        patch: Patch number of the dataset.  Used to indicate minor clean-up and edits\n    \"\"\"\n    super().__init__(major, minor, patch)\n</code></pre>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.DatasetVersion.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; VersionIterator\n</code></pre> <p>Return iter(self).</p> Source code in <code>semver/version.py</code> <pre><code>def __iter__(self) -&gt; VersionIterator:\n    \"\"\"Return iter(self).\"\"\"\n    yield from self.to_tuple()\n</code></pre>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.DatasetVersion.bump_build","title":"bump_build","text":"<pre><code>bump_build(\n    token: Optional[str] = \"build\",\n) -&gt; Version\n</code></pre> <p>Raise the build part of the version, return a new object but leave self untouched.</p> <p>:param token: defaults to <code>'build'</code> :return: new :class:<code>Version</code> object with the raised build part.     The original object is not modified.</p> <p>ver = semver.parse(\"3.4.5-rc.1+build.9\") ver.bump_build() Version(major=3, minor=4, patch=5, prerelease='rc.1', build='build.10')</p> Source code in <code>semver/version.py</code> <pre><code>    def bump_build(self, token: Optional[str] = \"build\") -&gt; \"Version\":\n        \"\"\"\n        Raise the build part of the version, return a new object but leave self\n        untouched.\n\n        :param token: defaults to ``'build'``\n        :return: new :class:`Version` object with the raised build part.\n            The original object is not modified.\n\n        &gt;&gt;&gt; ver = semver.parse(\"3.4.5-rc.1+build.9\")\n        &gt;&gt;&gt; ver.bump_build()\n        Version(major=3, minor=4, patch=5, prerelease='rc.1', \\\nbuild='build.10')\n        \"\"\"\n        cls = type(self)\n        if self._build is not None:\n            build = self._build\n        elif token == \"\":\n            build = \"0\"\n        elif token is None:\n            build = \"build.0\"\n        else:\n            build = str(token) + \".0\"\n\n        # self._build or (token or \"build\") + \".0\"\n        build = cls._increment_string(build)\n        if self._build is not None:\n            build = self._build\n        elif token == \"\":\n            build = \"0\"\n        elif token is None:\n            build = \"build.0\"\n        else:\n            build = str(token) + \".0\"\n\n        # self._build or (token or \"build\") + \".0\"\n        build = cls._increment_string(build)\n        return cls(self._major, self._minor, self._patch, self._prerelease, build)\n</code></pre>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.DatasetVersion.bump_major","title":"bump_major","text":"<pre><code>bump_major() -&gt; Version\n</code></pre> <p>Raise the major part of the version, return a new object but leave self untouched.</p> <p>:return: new object with the raised major part</p> <p>ver = semver.parse(\"3.4.5\") ver.bump_major() Version(major=4, minor=0, patch=0, prerelease=None, build=None)</p> Source code in <code>semver/version.py</code> <pre><code>def bump_major(self) -&gt; \"Version\":\n    \"\"\"\n    Raise the major part of the version, return a new object but leave self\n    untouched.\n\n    :return: new object with the raised major part\n\n    &gt;&gt;&gt; ver = semver.parse(\"3.4.5\")\n    &gt;&gt;&gt; ver.bump_major()\n    Version(major=4, minor=0, patch=0, prerelease=None, build=None)\n    \"\"\"\n    cls = type(self)\n    return cls(self._major + 1)\n</code></pre>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.DatasetVersion.bump_minor","title":"bump_minor","text":"<pre><code>bump_minor() -&gt; Version\n</code></pre> <p>Raise the minor part of the version, return a new object but leave self untouched.</p> <p>:return: new object with the raised minor part</p> <p>ver = semver.parse(\"3.4.5\") ver.bump_minor() Version(major=3, minor=5, patch=0, prerelease=None, build=None)</p> Source code in <code>semver/version.py</code> <pre><code>def bump_minor(self) -&gt; \"Version\":\n    \"\"\"\n    Raise the minor part of the version, return a new object but leave self\n    untouched.\n\n    :return: new object with the raised minor part\n\n    &gt;&gt;&gt; ver = semver.parse(\"3.4.5\")\n    &gt;&gt;&gt; ver.bump_minor()\n    Version(major=3, minor=5, patch=0, prerelease=None, build=None)\n    \"\"\"\n    cls = type(self)\n    return cls(self._major, self._minor + 1)\n</code></pre>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.DatasetVersion.bump_patch","title":"bump_patch","text":"<pre><code>bump_patch() -&gt; Version\n</code></pre> <p>Raise the patch part of the version, return a new object but leave self untouched.</p> <p>:return: new object with the raised patch part</p> <p>ver = semver.parse(\"3.4.5\") ver.bump_patch() Version(major=3, minor=4, patch=6, prerelease=None, build=None)</p> Source code in <code>semver/version.py</code> <pre><code>def bump_patch(self) -&gt; \"Version\":\n    \"\"\"\n    Raise the patch part of the version, return a new object but leave self\n    untouched.\n\n    :return: new object with the raised patch part\n\n    &gt;&gt;&gt; ver = semver.parse(\"3.4.5\")\n    &gt;&gt;&gt; ver.bump_patch()\n    Version(major=3, minor=4, patch=6, prerelease=None, build=None)\n    \"\"\"\n    cls = type(self)\n    return cls(self._major, self._minor, self._patch + 1)\n</code></pre>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.DatasetVersion.bump_prerelease","title":"bump_prerelease","text":"<pre><code>bump_prerelease(\n    token: Optional[str] = \"rc\",\n) -&gt; Version\n</code></pre> <p>Raise the prerelease part of the version, return a new object but leave self untouched.</p> <p>:param token: defaults to <code>'rc'</code> :return: new :class:<code>Version</code> object with the raised prerelease part.     The original object is not modified.</p> <p>ver = semver.parse(\"3.4.5\") ver.bump_prerelease().prerelease 'rc.2' ver.bump_prerelease('').prerelease '1' ver.bump_prerelease(None).prerelease 'rc.1'</p> Source code in <code>semver/version.py</code> <pre><code>def bump_prerelease(self, token: Optional[str] = \"rc\") -&gt; \"Version\":\n    \"\"\"\n    Raise the prerelease part of the version, return a new object but leave\n    self untouched.\n\n    :param token: defaults to ``'rc'``\n    :return: new :class:`Version` object with the raised prerelease part.\n        The original object is not modified.\n\n    &gt;&gt;&gt; ver = semver.parse(\"3.4.5\")\n    &gt;&gt;&gt; ver.bump_prerelease().prerelease\n    'rc.2'\n    &gt;&gt;&gt; ver.bump_prerelease('').prerelease\n    '1'\n    &gt;&gt;&gt; ver.bump_prerelease(None).prerelease\n    'rc.1'\n    \"\"\"\n    cls = type(self)\n    if self._prerelease is not None:\n        prerelease = self._prerelease\n    elif token == \"\":\n        prerelease = \"0\"\n    elif token is None:\n        prerelease = \"rc.0\"\n    else:\n        prerelease = str(token) + \".0\"\n\n    prerelease = cls._increment_string(prerelease)\n    return cls(self._major, self._minor, self._patch, prerelease)\n</code></pre>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.DatasetVersion.compare","title":"compare","text":"<pre><code>compare(other: Comparable) -&gt; int\n</code></pre> <p>Compare self with other.</p> <p>:param other: the second version :return: The return value is negative if ver1 &lt; ver2,      zero if ver1 == ver2 and strictly positive if ver1 &gt; ver2</p> <p>Version.parse(\"1.0.0\").compare(\"2.0.0\") -1 Version.parse(\"2.0.0\").compare(\"1.0.0\") 1 Version.parse(\"2.0.0\").compare(\"2.0.0\") 0</p> Source code in <code>semver/version.py</code> <pre><code>def compare(self, other: Comparable) -&gt; int:\n    \"\"\"\n    Compare self with other.\n\n    :param other: the second version\n    :return: The return value is negative if ver1 &lt; ver2,\n         zero if ver1 == ver2 and strictly positive if ver1 &gt; ver2\n\n    &gt;&gt;&gt; Version.parse(\"1.0.0\").compare(\"2.0.0\")\n    -1\n    &gt;&gt;&gt; Version.parse(\"2.0.0\").compare(\"1.0.0\")\n    1\n    &gt;&gt;&gt; Version.parse(\"2.0.0\").compare(\"2.0.0\")\n    0\n    \"\"\"\n    cls = type(self)\n    if isinstance(other, String.__args__):  # type: ignore\n        other = cls.parse(other)\n    elif isinstance(other, dict):\n        other = cls(**other)\n    elif isinstance(other, (tuple, list)):\n        other = cls(*other)\n    elif not isinstance(other, cls):\n        raise TypeError(\n            f\"Expected str, bytes, dict, tuple, list, or {cls.__name__} instance, \"\n            f\"but got {type(other)}\"\n        )\n\n    v1 = self.to_tuple()[:3]\n    v2 = other.to_tuple()[:3]\n    x = _cmp(v1, v2)\n    if x:\n        return x\n\n    rc1, rc2 = self.prerelease, other.prerelease\n    rccmp = self._nat_cmp(rc1, rc2)\n\n    if not rccmp:\n        return 0\n    if not rc1:\n        return 1\n    elif not rc2:\n        return -1\n\n    return rccmp\n</code></pre>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.DatasetVersion.finalize_version","title":"finalize_version","text":"<pre><code>finalize_version() -&gt; Version\n</code></pre> <p>Remove any prerelease and build metadata from the version.</p> <p>:return: a new instance with the finalized version string</p> <p>str(semver.Version.parse('1.2.3-rc.5').finalize_version()) '1.2.3'</p> Source code in <code>semver/version.py</code> <pre><code>def finalize_version(self) -&gt; \"Version\":\n    \"\"\"\n    Remove any prerelease and build metadata from the version.\n\n    :return: a new instance with the finalized version string\n\n    &gt;&gt;&gt; str(semver.Version.parse('1.2.3-rc.5').finalize_version())\n    '1.2.3'\n    \"\"\"\n    cls = type(self)\n    return cls(self.major, self.minor, self.patch)\n</code></pre>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.DatasetVersion.is_compatible","title":"is_compatible","text":"<pre><code>is_compatible(other: Version) -&gt; bool\n</code></pre> <p>Check if current version is compatible with other version.</p> <p>The result is True, if either of the following is true:</p> <ul> <li>both versions are equal, or</li> <li>both majors are equal and higher than 0. Same for both minors.   Both pre-releases are equal, or</li> <li>both majors are equal and higher than 0. The minor of b's   minor version is higher then a's. Both pre-releases are equal.</li> </ul> <p>The algorithm does not check patches.</p> <p>.. versionadded:: 3.0.0</p> <p>:param other: the version to check for compatibility :return: True, if <code>other</code> is compatible with the old version,          otherwise False</p> <p>Version(1, 1, 0).is_compatible(Version(1, 0, 0)) False Version(1, 0, 0).is_compatible(Version(1, 1, 0)) True</p> Source code in <code>semver/version.py</code> <pre><code>def is_compatible(self, other: \"Version\") -&gt; bool:\n    \"\"\"\n    Check if current version is compatible with other version.\n\n    The result is True, if either of the following is true:\n\n    * both versions are equal, or\n    * both majors are equal and higher than 0. Same for both minors.\n      Both pre-releases are equal, or\n    * both majors are equal and higher than 0. The minor of b's\n      minor version is higher then a's. Both pre-releases are equal.\n\n    The algorithm does *not* check patches.\n\n    .. versionadded:: 3.0.0\n\n    :param other: the version to check for compatibility\n    :return: True, if ``other`` is compatible with the old version,\n             otherwise False\n\n    &gt;&gt;&gt; Version(1, 1, 0).is_compatible(Version(1, 0, 0))\n    False\n    &gt;&gt;&gt; Version(1, 0, 0).is_compatible(Version(1, 1, 0))\n    True\n    \"\"\"\n    if not isinstance(other, Version):\n        raise TypeError(f\"Expected a Version type but got {type(other)}\")\n\n    # All major-0 versions should be incompatible with anything but itself\n    if (0 == self.major == other.major) and (self[:4] != other[:4]):\n        return False\n\n    return (\n        (self.major == other.major)\n        and (other.minor &gt;= self.minor)\n        and (self.prerelease == other.prerelease)\n    )\n</code></pre>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.DatasetVersion.is_valid","title":"is_valid  <code>classmethod</code>","text":"<pre><code>is_valid(version: str) -&gt; bool\n</code></pre> <p>Check if the string is a valid semver version.</p> <p>.. versionadded:: 2.9.1</p> <p>.. versionchanged:: 3.0.0    Renamed from :meth:<code>~semver.version.Version.isvalid</code></p> <p>:param version: the version string to check :return: True if the version string is a valid semver version, False          otherwise.</p> Source code in <code>semver/version.py</code> <pre><code>@classmethod\ndef is_valid(cls, version: str) -&gt; bool:\n    \"\"\"\n    Check if the string is a valid semver version.\n\n    .. versionadded:: 2.9.1\n\n    .. versionchanged:: 3.0.0\n       Renamed from :meth:`~semver.version.Version.isvalid`\n\n    :param version: the version string to check\n    :return: True if the version string is a valid semver version, False\n             otherwise.\n    \"\"\"\n    try:\n        cls.parse(version)\n        return True\n    except ValueError:\n        return False\n</code></pre>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.DatasetVersion.match","title":"match","text":"<pre><code>match(match_expr: str) -&gt; bool\n</code></pre> <p>Compare self to match a match expression.</p> <p>:param match_expr: optional operator and version; valid operators are       <code>&lt;</code>   smaller than       <code>&gt;</code>   greater than       <code>&gt;=</code>  greator or equal than       <code>&lt;=</code>  smaller or equal than       <code>==</code>  equal       <code>!=</code>  not equal :return: True if the expression matches the version, otherwise False</p> <p>semver.Version.parse(\"2.0.0\").match(\"&gt;=1.0.0\") True semver.Version.parse(\"1.0.0\").match(\"&gt;1.0.0\") False semver.Version.parse(\"4.0.4\").match(\"4.0.4\") True</p> Source code in <code>semver/version.py</code> <pre><code>def match(self, match_expr: str) -&gt; bool:\n    \"\"\"\n    Compare self to match a match expression.\n\n    :param match_expr: optional operator and version; valid operators are\n          ``&lt;``   smaller than\n          ``&gt;``   greater than\n          ``&gt;=``  greator or equal than\n          ``&lt;=``  smaller or equal than\n          ``==``  equal\n          ``!=``  not equal\n    :return: True if the expression matches the version, otherwise False\n\n    &gt;&gt;&gt; semver.Version.parse(\"2.0.0\").match(\"&gt;=1.0.0\")\n    True\n    &gt;&gt;&gt; semver.Version.parse(\"1.0.0\").match(\"&gt;1.0.0\")\n    False\n    &gt;&gt;&gt; semver.Version.parse(\"4.0.4\").match(\"4.0.4\")\n    True\n    \"\"\"\n    prefix = match_expr[:2]\n    if prefix in (\"&gt;=\", \"&lt;=\", \"==\", \"!=\"):\n        match_version = match_expr[2:]\n    elif prefix and prefix[0] in (\"&gt;\", \"&lt;\"):\n        prefix = prefix[0]\n        match_version = match_expr[1:]\n    elif match_expr and match_expr[0] in \"0123456789\":\n        prefix = \"==\"\n        match_version = match_expr\n    else:\n        raise ValueError(\n            \"match_expr parameter should be in format &lt;op&gt;&lt;ver&gt;, \"\n            \"where &lt;op&gt; is one of \"\n            \"['&lt;', '&gt;', '==', '&lt;=', '&gt;=', '!=']. \"\n            \"You provided: %r\" % match_expr\n        )\n\n    possibilities_dict = {\n        \"&gt;\": (1,),\n        \"&lt;\": (-1,),\n        \"==\": (0,),\n        \"!=\": (-1, 1),\n        \"&gt;=\": (0, 1),\n        \"&lt;=\": (-1, 0),\n    }\n\n    possibilities = possibilities_dict[prefix]\n    cmp_res = self.compare(match_version)\n\n    return cmp_res in possibilities\n</code></pre>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.DatasetVersion.next_version","title":"next_version","text":"<pre><code>next_version(\n    part: str,\n    prerelease_token: str = \"rc\",\n) -&gt; Version\n</code></pre> <p>Determines next version, preserving natural order.</p> <p>.. versionadded:: 2.10.0</p> <p>This function is taking prereleases into account. The \"major\", \"minor\", and \"patch\" raises the respective parts like the <code>bump_*</code> functions. The real difference is using the \"prerelease\" part. It gives you the next patch version of the prerelease, for example:</p> <p>str(semver.parse(\"0.1.4\").next_version(\"prerelease\")) '0.1.5-rc.1'</p> <p>:param part: One of \"major\", \"minor\", \"patch\", or \"prerelease\" :param prerelease_token: prefix string of prerelease, defaults to 'rc' :return: new object with the appropriate part raised</p> Source code in <code>semver/version.py</code> <pre><code>def next_version(self, part: str, prerelease_token: str = \"rc\") -&gt; \"Version\":\n    \"\"\"\n    Determines next version, preserving natural order.\n\n    .. versionadded:: 2.10.0\n\n    This function is taking prereleases into account.\n    The \"major\", \"minor\", and \"patch\" raises the respective parts like\n    the ``bump_*`` functions. The real difference is using the\n    \"prerelease\" part. It gives you the next patch version of the\n    prerelease, for example:\n\n    &gt;&gt;&gt; str(semver.parse(\"0.1.4\").next_version(\"prerelease\"))\n    '0.1.5-rc.1'\n\n    :param part: One of \"major\", \"minor\", \"patch\", or \"prerelease\"\n    :param prerelease_token: prefix string of prerelease, defaults to 'rc'\n    :return: new object with the appropriate part raised\n    \"\"\"\n    cls = type(self)\n    # \"build\" is currently not used, that's why we use [:-1]\n    validparts = cls.NAMES[:-1]\n    if part not in validparts:\n        raise ValueError(\n            f\"Invalid part. Expected one of {validparts}, but got {part!r}\"\n        )\n    version = self\n    if (version.prerelease or version.build) and (\n        part == \"patch\"\n        or (part == \"minor\" and version.patch == 0)\n        or (part == \"major\" and version.minor == version.patch == 0)\n    ):\n        return version.replace(prerelease=None, build=None)\n\n    # Only check the main parts:\n    if part in cls.NAMES[:3]:\n        return getattr(version, \"bump_\" + part)()\n\n    if not version.prerelease:\n        version = version.bump_patch()\n    return version.bump_prerelease(prerelease_token)\n</code></pre>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.DatasetVersion.replace","title":"replace","text":"<pre><code>replace(\n    **parts: Union[int, Optional[str]],\n) -&gt; Version\n</code></pre> <p>Replace one or more parts of a version and return a new :class:<code>Version</code> object, but leave self untouched.</p> <p>.. versionadded:: 2.9.0    Added :func:<code>Version.replace</code></p> <p>:param parts: the parts to be updated. Valid keys are:   <code>major</code>, <code>minor</code>, <code>patch</code>, <code>prerelease</code>, or <code>build</code> :return: the new :class:<code>~semver.version.Version</code> object with   the changed parts :raises TypeError: if <code>parts</code> contain invalid keys</p> Source code in <code>semver/version.py</code> <pre><code>def replace(self, **parts: Union[int, Optional[str]]) -&gt; \"Version\":\n    \"\"\"\n    Replace one or more parts of a version and return a new :class:`Version`\n    object, but leave self untouched.\n\n    .. versionadded:: 2.9.0\n       Added :func:`Version.replace`\n\n    :param parts: the parts to be updated. Valid keys are:\n      ``major``, ``minor``, ``patch``, ``prerelease``, or ``build``\n    :return: the new :class:`~semver.version.Version` object with\n      the changed parts\n    :raises TypeError: if ``parts`` contain invalid keys\n    \"\"\"\n    version = self.to_dict()\n    version.update(parts)\n    try:\n        return type(self)(**version)  # type: ignore\n    except TypeError:\n        unknownkeys = set(parts) - set(self.to_dict())\n        error = \"replace() got %d unexpected keyword argument(s): %s\" % (\n            len(unknownkeys),\n            \", \".join(unknownkeys),\n        )\n        raise TypeError(error)\n</code></pre>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.DatasetVersion.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dictionary of version information</p> Source code in <code>src/deriva_ml/dataset/aux_classes.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"\n\n    Returns:\n        dictionary of version information\n\n    \"\"\"\n    return {\"major\": self.major, \"minor\": self.minor, \"patch\": self.patch}\n</code></pre>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.DatasetVersion.to_tuple","title":"to_tuple","text":"<pre><code>to_tuple() -&gt; tuple[int, int, int]\n</code></pre> <p>Returns:</p> Type Description <code>tuple[int, int, int]</code> <p>tuple of version information</p> Source code in <code>src/deriva_ml/dataset/aux_classes.py</code> <pre><code>def to_tuple(self) -&gt; tuple[int, int, int]:\n    \"\"\"\n\n    Returns:\n        tuple of version information\n\n    \"\"\"\n    return self.major, self.minor, self.patch\n</code></pre>"},{"location":"code-docs/dataset_aux_classes/#deriva_ml.dataset.aux_classes.VersionPart","title":"VersionPart","text":"<p>               Bases: <code>Enum</code></p> <p>Simple enumeration for semantic versioning.</p> <p>Attributes:</p> Name Type Description <code>major</code> <code>int</code> <p>Major version number</p> <code>minor</code> <code>int</code> <p>Minor version number</p> <code>patch</code> <code>int</code> <p>Patch version number</p> Source code in <code>src/deriva_ml/dataset/aux_classes.py</code> <pre><code>class VersionPart(Enum):\n    \"\"\"Simple enumeration for semantic versioning.\n\n    Attributes:\n        major (int): Major version number\n        minor (int): Minor version number\n        patch (int): Patch version number\n\n    \"\"\"\n\n    major = \"major\"\n    minor = \"minor\"\n    patch = \"patch\"\n</code></pre>"},{"location":"code-docs/dataset_bag/","title":"DatasetBag Class","text":"<p>The module implements the sqllite interface to a set of directories representing a dataset bag.</p>"},{"location":"code-docs/dataset_bag/#deriva_ml.dataset.dataset_bag.DatasetBag","title":"DatasetBag","text":"<p>DatasetBag is a class that manages a materialized bag.  It is created from a locally materialized BDBag for a dataset_table, which is created either by DerivaML.create_execution, or directly by calling DerivaML.download_dataset.</p> <p>A general a bag may contain multiple datasets, if the dataset is nested. The DatasetBag is used to represent only one of the datasets in the bag.</p> <p>All the metadata associated with the dataset is stored in a SQLLite database that can be queried using SQL.</p> <p>Attributes:</p> Name Type Description <code>dataset_rid</code> <code>RID</code> <p>RID for the specified dataset</p> <code>version</code> <p>The version of the dataset</p> <code>model</code> <code>DatabaseModel</code> <p>The Database model that has all the catalog metadata associated with this dataset. database:</p> <code>dbase</code> <code>Connection</code> <p>connection to the sqlite database holding table values</p> <code>domain_schema</code> <code>str</code> <p>Name of the domain schema</p> Source code in <code>src/deriva_ml/dataset/dataset_bag.py</code> <pre><code>class DatasetBag:\n    \"\"\"\n    DatasetBag is a class that manages a materialized bag.  It is created from a locally materialized\n    BDBag for a dataset_table, which is created either by DerivaML.create_execution, or directly by\n    calling DerivaML.download_dataset.\n\n    A general a bag may contain multiple datasets, if the dataset is nested. The DatasetBag is used to\n    represent only one of the datasets in the bag.\n\n    All the metadata associated with the dataset is stored in a SQLLite database that can be queried using SQL.\n\n    Attributes:\n        dataset_rid (RID): RID for the specified dataset\n        version: The version of the dataset\n        model (DatabaseModel): The Database model that has all the catalog metadata associated with this dataset.\n            database:\n        dbase (sqlite3.Connection): connection to the sqlite database holding table values\n        domain_schema (str): Name of the domain schema\n    \"\"\"\n\n    def __init__(self, database_model: DatabaseModel, dataset_rid: RID | None = None) -&gt; None:\n        \"\"\"\n        Initialize a DatasetBag instance.\n\n        Args:\n            database_model: Database version of the bag.\n            dataset_rid: Optional RID for the dataset.\n        \"\"\"\n        self.model = database_model\n        self.database = cast(sqlite3.Connection, self.model.dbase)\n\n        self.dataset_rid = dataset_rid or self.model.dataset_rid\n        if not self.dataset_rid:\n            raise DerivaMLException(\"No dataset RID provided\")\n\n        self.model.rid_lookup(self.dataset_rid)  # Check to make sure that this dataset is in the bag.\n\n        self.version = self.model.dataset_version(self.dataset_rid)\n        self._dataset_table = self.model.dataset_table\n\n    def __repr__(self) -&gt; str:\n        return f\"&lt;deriva_ml.DatasetBag object {self.dataset_rid} at {hex(id(self))}&gt;\"\n\n    def list_tables(self) -&gt; list[str]:\n        \"\"\"List the names of the tables in the catalog\n\n        Returns:\n            A list of table names.  These names are all qualified with the Deriva schema name.\n        \"\"\"\n        return self.model.list_tables()\n\n    def _dataset_table_view(self, table: str) -&gt; str:\n        \"\"\"Return a SQL command that will return all of the elements in the specified table that are associated with\n        dataset_rid\"\"\"\n\n        table_name = self.model.normalize_table_name(table)\n\n        # Get the names of the columns in the table.\n        with self.database as dbase:\n            select_args = \",\".join(\n                [f'\"{table_name}\".\"{c[1]}\"' for c in dbase.execute(f'PRAGMA table_info(\"{table_name}\")').fetchall()]\n            )\n\n        # Get the list of datasets in the bag including the dataset itself.\n        datasets = \",\".join(\n            [f'\"{self.dataset_rid}\"'] + [f'\"{ds.dataset_rid}\"' for ds in self.list_dataset_children(recurse=True)]\n        )\n\n        # Find the paths that terminate in the table we are looking for\n        # Assemble the ON clause by looking at each table pair, and looking up the FK columns that connect them.\n        paths = [\n            (\n                [f'\"{self.model.normalize_table_name(t.name)}\"' for t in p],\n                [self.model._table_relationship(t1, t2) for t1, t2 in zip(p, p[1:])],\n            )\n            for p in self.model._schema_to_paths()\n            if p[-1].name == table\n        ]\n\n        sql = []\n        dataset_table_name = f'\"{self.model.normalize_table_name(self._dataset_table.name)}\"'\n\n        def column_name(col: Column) -&gt; str:\n            return f'\"{self.model.normalize_table_name(col.table.name)}\".\"{col.name}\"'\n\n        for ts, on in paths:\n            tables = \" JOIN \".join(ts)\n            on_expression = \" and \".join([f\"{column_name(left)}={column_name(right)}\" for left, right in on])\n            sql.append(\n                f\"SELECT {select_args} FROM {tables} \"\n                f\"{'ON ' + on_expression if on_expression else ''} \"\n                f\"WHERE {dataset_table_name}.RID IN ({datasets})\"\n            )\n            if table_name == self.model.normalize_table_name(self._dataset_table.name):\n                sql.append(\n                    f\"SELECT {select_args} FROM {dataset_table_name} WHERE {dataset_table_name}.RID IN ({datasets})\"\n                )\n        sql = \" UNION \".join(sql) if len(sql) &gt; 1 else sql[0]\n        return sql\n\n    def get_table(self, table: str) -&gt; Generator[tuple, None, None]:\n        \"\"\"Retrieve the contents of the specified table. If schema is not provided as part of the table name,\n        the method will attempt to locate the schema for the table.\n\n        Args:\n            table: return: A generator that yields tuples of column values.\n\n        Returns:\n          A generator that yields tuples of column values.\n\n        \"\"\"\n        result = self.database.execute(self._dataset_table_view(table))\n        while row := result.fetchone():\n            yield row\n\n    def get_table_as_dataframe(self, table: str) -&gt; pd.DataFrame:\n        \"\"\"Retrieve the contents of the specified table as a dataframe.\n\n\n        If schema is not provided as part of the table name,\n        the method will attempt to locate the schema for the table.\n\n        Args:\n            table: Table to retrieve data from.\n\n        Returns:\n          A dataframe containing the contents of the specified table.\n        \"\"\"\n        return pd.read_sql(self._dataset_table_view(table), self.database)\n\n    def get_table_as_dict(self, table: str) -&gt; Generator[dict[str, Any], None, None]:\n        \"\"\"Retrieve the contents of the specified table as a dictionary.\n\n        Args:\n            table: Table to retrieve data from. f schema is not provided as part of the table name,\n                the method will attempt to locate the schema for the table.\n\n        Returns:\n          A generator producing dictionaries containing the contents of the specified table as name/value pairs.\n        \"\"\"\n\n        table_name = self.model.normalize_table_name(table)\n        schema, table = table_name.split(\":\")\n        with self.database as _dbase:\n            mapper = SQLMapper(self.model, table)\n            result = self.database.execute(self._dataset_table_view(table))\n            while row := result.fetchone():\n                yield mapper.transform_tuple(row)\n\n    @validate_call\n    def list_dataset_members(self, recurse: bool = False) -&gt; dict[str, list[dict[str, Any]]]:\n        \"\"\"Return a list of entities associated with a specific dataset.\n\n        Args:\n           recurse: Whether to include nested datasets.\n\n        Returns:\n            Dictionary of entities associated with the dataset.\n        \"\"\"\n\n        # Look at each of the element types that might be in the _dataset_table and get the list of rid for them from\n        # the appropriate association table.\n        members = defaultdict(list)\n        for assoc_table in self._dataset_table.find_associations():\n            member_fkey = assoc_table.other_fkeys.pop()\n            if member_fkey.pk_table.name == \"Dataset\" and member_fkey.foreign_key_columns[0].name != \"Nested_Dataset\":\n                # Sometimes find_assoc gets confused on Dataset_Dataset.\n                member_fkey = assoc_table.self_fkey\n\n            target_table = member_fkey.pk_table\n            member_table = assoc_table.table\n\n            if target_table.schema.name != self.model.domain_schema and not (\n                target_table == self._dataset_table or target_table.name == \"File\"\n            ):\n                # Look at domain tables and nested datasets.\n                continue\n            sql_target = self.model.normalize_table_name(target_table.name)\n            sql_member = self.model.normalize_table_name(member_table.name)\n\n            # Get the names of the columns that we are going to need for linking\n            member_link = tuple(c.name for c in next(iter(member_fkey.column_map.items())))\n            with self.database as db:\n                col_names = [c[1] for c in db.execute(f'PRAGMA table_info(\"{sql_target}\")').fetchall()]\n                select_cols = \",\".join([f'\"{sql_target}\".{c}' for c in col_names])\n                sql_cmd = (\n                    f'SELECT {select_cols} FROM \"{sql_member}\" '\n                    f'JOIN \"{sql_target}\" ON \"{sql_member}\".{member_link[0]} = \"{sql_target}\".{member_link[1]} '\n                    f'WHERE \"{self.dataset_rid}\" = \"{sql_member}\".Dataset;'\n                )\n                mapper = SQLMapper(self.model, sql_target)\n                target_entities = [mapper.transform_tuple(e) for e in db.execute(sql_cmd).fetchall()]\n            members[target_table.name].extend(target_entities)\n            if recurse and (target_table.name == self._dataset_table.name):\n                # Get the members for all the nested datasets and add to the member list.\n                nested_datasets = [d[\"RID\"] for d in target_entities]\n                for ds in nested_datasets:\n                    nested_dataset = self.model.get_dataset(ds)\n                    for k, v in nested_dataset.list_dataset_members(recurse=recurse).items():\n                        members[k].extend(v)\n        return dict(members)\n\n    def find_features(self, table: str | Table) -&gt; Iterable[Feature]:\n        \"\"\"Find features for a table.\n\n        Args:\n            table: The table to find features for.\n\n        Returns:\n            An iterable of Feature instances.\n        \"\"\"\n        return self.model.find_features(table)\n\n    def list_feature_values(self, table: Table | str, feature_name: str) -&gt; datapath._ResultSet:\n        \"\"\"Return feature values for a table.\n\n        Args:\n            table: The table to get feature values for.\n            feature_name: Name of the feature.\n\n        Returns:\n            Feature values.\n        \"\"\"\n        feature = self.model.lookup_feature(table, feature_name)\n        feature_table = self.model.normalize_table_name(feature.feature_table.name)\n\n        with self.database as db:\n            col_names = [c[1] for c in db.execute(f'PRAGMA table_info(\"{feature_table}\")').fetchall()]\n            sql_cmd = f'SELECT * FROM \"{feature_table}\"'\n            return cast(datapath._ResultSet, [dict(zip(col_names, r)) for r in db.execute(sql_cmd).fetchall()])\n\n    def list_dataset_children(self, recurse: bool = False) -&gt; list[DatasetBag]:\n        \"\"\"Get nested datasets.\n\n        Args:\n            recurse: Whether to include children of children.\n\n        Returns:\n            List of child dataset bags.\n        \"\"\"\n        ds_table = self.model.normalize_table_name(\"Dataset\")\n        nds_table = self.model.normalize_table_name(\"Dataset_Dataset\")\n        dv_table = self.model.normalize_table_name(\"Dataset_Version\")\n        with self.database as db:\n            sql_cmd = (\n                f'SELECT  \"{nds_table}\".Nested_Dataset, \"{dv_table}\".Version '\n                f'FROM \"{nds_table}\" JOIN \"{dv_table}\" JOIN \"{ds_table}\" on '\n                f'\"{ds_table}\".Version == \"{dv_table}\".RID AND '\n                f'\"{nds_table}\".Nested_Dataset == \"{ds_table}\".RID '\n                f'where \"{nds_table}\".Dataset == \"{self.dataset_rid}\"'\n            )\n            nested = [DatasetBag(self.model, r[0]) for r in db.execute(sql_cmd).fetchall()]\n\n        result = copy(nested)\n        if recurse:\n            for child in nested:\n                result.extend(child.list_dataset_children(recurse))\n        return result\n\n    @validate_call(config=ConfigDict(arbitrary_types_allowed=True))\n    def lookup_term(self, table: str | Table, term_name: str) -&gt; VocabularyTerm:\n        \"\"\"Finds a term in a vocabulary table.\n\n        Searches for a term in the specified vocabulary table, matching either the primary name\n        or any of its synonyms.\n\n        Args:\n            table: Vocabulary table to search in (name or Table object).\n            term_name: Name or synonym of the term to find.\n\n        Returns:\n            VocabularyTerm: The matching vocabulary term.\n\n        Raises:\n            DerivaMLVocabularyException: If the table is not a vocabulary table, or term is not found.\n\n        Examples:\n            Look up by primary name:\n                &gt;&gt;&gt; term = ml.lookup_term(\"tissue_types\", \"epithelial\")\n                &gt;&gt;&gt; print(term.description)\n\n            Look up by synonym:\n                &gt;&gt;&gt; term = ml.lookup_term(\"tissue_types\", \"epithelium\")\n        \"\"\"\n        # Get and validate vocabulary table reference\n        vocab_table = self.model.normalize_table_name(table)\n        if not self.model.is_vocabulary(table):\n            raise DerivaMLException(f\"The table {table} is not a controlled vocabulary\")\n\n        # Search for term by name or synonym\n        for term in self.get_table_as_dict(vocab_table):\n            if term_name == term[\"Name\"] or (term[\"Synonyms\"] and term_name in term[\"Synonyms\"]):\n                term[\"Synonyms\"] = list(term[\"Synonyms\"])\n                return VocabularyTerm.model_validate(term)\n\n        # Term not found\n        raise DerivaMLInvalidTerm(vocab_table, term_name)\n</code></pre>"},{"location":"code-docs/dataset_bag/#deriva_ml.dataset.dataset_bag.DatasetBag.__init__","title":"__init__","text":"<pre><code>__init__(\n    database_model: DatabaseModel,\n    dataset_rid: RID | None = None,\n) -&gt; None\n</code></pre> <p>Initialize a DatasetBag instance.</p> <p>Parameters:</p> Name Type Description Default <code>database_model</code> <code>DatabaseModel</code> <p>Database version of the bag.</p> required <code>dataset_rid</code> <code>RID | None</code> <p>Optional RID for the dataset.</p> <code>None</code> Source code in <code>src/deriva_ml/dataset/dataset_bag.py</code> <pre><code>def __init__(self, database_model: DatabaseModel, dataset_rid: RID | None = None) -&gt; None:\n    \"\"\"\n    Initialize a DatasetBag instance.\n\n    Args:\n        database_model: Database version of the bag.\n        dataset_rid: Optional RID for the dataset.\n    \"\"\"\n    self.model = database_model\n    self.database = cast(sqlite3.Connection, self.model.dbase)\n\n    self.dataset_rid = dataset_rid or self.model.dataset_rid\n    if not self.dataset_rid:\n        raise DerivaMLException(\"No dataset RID provided\")\n\n    self.model.rid_lookup(self.dataset_rid)  # Check to make sure that this dataset is in the bag.\n\n    self.version = self.model.dataset_version(self.dataset_rid)\n    self._dataset_table = self.model.dataset_table\n</code></pre>"},{"location":"code-docs/dataset_bag/#deriva_ml.dataset.dataset_bag.DatasetBag.find_features","title":"find_features","text":"<pre><code>find_features(\n    table: str | Table,\n) -&gt; Iterable[Feature]\n</code></pre> <p>Find features for a table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str | Table</code> <p>The table to find features for.</p> required <p>Returns:</p> Type Description <code>Iterable[Feature]</code> <p>An iterable of Feature instances.</p> Source code in <code>src/deriva_ml/dataset/dataset_bag.py</code> <pre><code>def find_features(self, table: str | Table) -&gt; Iterable[Feature]:\n    \"\"\"Find features for a table.\n\n    Args:\n        table: The table to find features for.\n\n    Returns:\n        An iterable of Feature instances.\n    \"\"\"\n    return self.model.find_features(table)\n</code></pre>"},{"location":"code-docs/dataset_bag/#deriva_ml.dataset.dataset_bag.DatasetBag.get_table","title":"get_table","text":"<pre><code>get_table(\n    table: str,\n) -&gt; Generator[tuple, None, None]\n</code></pre> <p>Retrieve the contents of the specified table. If schema is not provided as part of the table name, the method will attempt to locate the schema for the table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>return: A generator that yields tuples of column values.</p> required <p>Returns:</p> Type Description <code>None</code> <p>A generator that yields tuples of column values.</p> Source code in <code>src/deriva_ml/dataset/dataset_bag.py</code> <pre><code>def get_table(self, table: str) -&gt; Generator[tuple, None, None]:\n    \"\"\"Retrieve the contents of the specified table. If schema is not provided as part of the table name,\n    the method will attempt to locate the schema for the table.\n\n    Args:\n        table: return: A generator that yields tuples of column values.\n\n    Returns:\n      A generator that yields tuples of column values.\n\n    \"\"\"\n    result = self.database.execute(self._dataset_table_view(table))\n    while row := result.fetchone():\n        yield row\n</code></pre>"},{"location":"code-docs/dataset_bag/#deriva_ml.dataset.dataset_bag.DatasetBag.get_table_as_dataframe","title":"get_table_as_dataframe","text":"<pre><code>get_table_as_dataframe(\n    table: str,\n) -&gt; pd.DataFrame\n</code></pre> <p>Retrieve the contents of the specified table as a dataframe.</p> <p>If schema is not provided as part of the table name, the method will attempt to locate the schema for the table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>Table to retrieve data from.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe containing the contents of the specified table.</p> Source code in <code>src/deriva_ml/dataset/dataset_bag.py</code> <pre><code>def get_table_as_dataframe(self, table: str) -&gt; pd.DataFrame:\n    \"\"\"Retrieve the contents of the specified table as a dataframe.\n\n\n    If schema is not provided as part of the table name,\n    the method will attempt to locate the schema for the table.\n\n    Args:\n        table: Table to retrieve data from.\n\n    Returns:\n      A dataframe containing the contents of the specified table.\n    \"\"\"\n    return pd.read_sql(self._dataset_table_view(table), self.database)\n</code></pre>"},{"location":"code-docs/dataset_bag/#deriva_ml.dataset.dataset_bag.DatasetBag.get_table_as_dict","title":"get_table_as_dict","text":"<pre><code>get_table_as_dict(\n    table: str,\n) -&gt; Generator[\n    dict[str, Any], None, None\n]\n</code></pre> <p>Retrieve the contents of the specified table as a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>Table to retrieve data from. f schema is not provided as part of the table name, the method will attempt to locate the schema for the table.</p> required <p>Returns:</p> Type Description <code>None</code> <p>A generator producing dictionaries containing the contents of the specified table as name/value pairs.</p> Source code in <code>src/deriva_ml/dataset/dataset_bag.py</code> <pre><code>def get_table_as_dict(self, table: str) -&gt; Generator[dict[str, Any], None, None]:\n    \"\"\"Retrieve the contents of the specified table as a dictionary.\n\n    Args:\n        table: Table to retrieve data from. f schema is not provided as part of the table name,\n            the method will attempt to locate the schema for the table.\n\n    Returns:\n      A generator producing dictionaries containing the contents of the specified table as name/value pairs.\n    \"\"\"\n\n    table_name = self.model.normalize_table_name(table)\n    schema, table = table_name.split(\":\")\n    with self.database as _dbase:\n        mapper = SQLMapper(self.model, table)\n        result = self.database.execute(self._dataset_table_view(table))\n        while row := result.fetchone():\n            yield mapper.transform_tuple(row)\n</code></pre>"},{"location":"code-docs/dataset_bag/#deriva_ml.dataset.dataset_bag.DatasetBag.list_dataset_children","title":"list_dataset_children","text":"<pre><code>list_dataset_children(\n    recurse: bool = False,\n) -&gt; list[DatasetBag]\n</code></pre> <p>Get nested datasets.</p> <p>Parameters:</p> Name Type Description Default <code>recurse</code> <code>bool</code> <p>Whether to include children of children.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[DatasetBag]</code> <p>List of child dataset bags.</p> Source code in <code>src/deriva_ml/dataset/dataset_bag.py</code> <pre><code>def list_dataset_children(self, recurse: bool = False) -&gt; list[DatasetBag]:\n    \"\"\"Get nested datasets.\n\n    Args:\n        recurse: Whether to include children of children.\n\n    Returns:\n        List of child dataset bags.\n    \"\"\"\n    ds_table = self.model.normalize_table_name(\"Dataset\")\n    nds_table = self.model.normalize_table_name(\"Dataset_Dataset\")\n    dv_table = self.model.normalize_table_name(\"Dataset_Version\")\n    with self.database as db:\n        sql_cmd = (\n            f'SELECT  \"{nds_table}\".Nested_Dataset, \"{dv_table}\".Version '\n            f'FROM \"{nds_table}\" JOIN \"{dv_table}\" JOIN \"{ds_table}\" on '\n            f'\"{ds_table}\".Version == \"{dv_table}\".RID AND '\n            f'\"{nds_table}\".Nested_Dataset == \"{ds_table}\".RID '\n            f'where \"{nds_table}\".Dataset == \"{self.dataset_rid}\"'\n        )\n        nested = [DatasetBag(self.model, r[0]) for r in db.execute(sql_cmd).fetchall()]\n\n    result = copy(nested)\n    if recurse:\n        for child in nested:\n            result.extend(child.list_dataset_children(recurse))\n    return result\n</code></pre>"},{"location":"code-docs/dataset_bag/#deriva_ml.dataset.dataset_bag.DatasetBag.list_dataset_members","title":"list_dataset_members","text":"<pre><code>list_dataset_members(\n    recurse: bool = False,\n) -&gt; dict[str, list[dict[str, Any]]]\n</code></pre> <p>Return a list of entities associated with a specific dataset.</p> <p>Parameters:</p> Name Type Description Default <code>recurse</code> <code>bool</code> <p>Whether to include nested datasets.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, list[dict[str, Any]]]</code> <p>Dictionary of entities associated with the dataset.</p> Source code in <code>src/deriva_ml/dataset/dataset_bag.py</code> <pre><code>@validate_call\ndef list_dataset_members(self, recurse: bool = False) -&gt; dict[str, list[dict[str, Any]]]:\n    \"\"\"Return a list of entities associated with a specific dataset.\n\n    Args:\n       recurse: Whether to include nested datasets.\n\n    Returns:\n        Dictionary of entities associated with the dataset.\n    \"\"\"\n\n    # Look at each of the element types that might be in the _dataset_table and get the list of rid for them from\n    # the appropriate association table.\n    members = defaultdict(list)\n    for assoc_table in self._dataset_table.find_associations():\n        member_fkey = assoc_table.other_fkeys.pop()\n        if member_fkey.pk_table.name == \"Dataset\" and member_fkey.foreign_key_columns[0].name != \"Nested_Dataset\":\n            # Sometimes find_assoc gets confused on Dataset_Dataset.\n            member_fkey = assoc_table.self_fkey\n\n        target_table = member_fkey.pk_table\n        member_table = assoc_table.table\n\n        if target_table.schema.name != self.model.domain_schema and not (\n            target_table == self._dataset_table or target_table.name == \"File\"\n        ):\n            # Look at domain tables and nested datasets.\n            continue\n        sql_target = self.model.normalize_table_name(target_table.name)\n        sql_member = self.model.normalize_table_name(member_table.name)\n\n        # Get the names of the columns that we are going to need for linking\n        member_link = tuple(c.name for c in next(iter(member_fkey.column_map.items())))\n        with self.database as db:\n            col_names = [c[1] for c in db.execute(f'PRAGMA table_info(\"{sql_target}\")').fetchall()]\n            select_cols = \",\".join([f'\"{sql_target}\".{c}' for c in col_names])\n            sql_cmd = (\n                f'SELECT {select_cols} FROM \"{sql_member}\" '\n                f'JOIN \"{sql_target}\" ON \"{sql_member}\".{member_link[0]} = \"{sql_target}\".{member_link[1]} '\n                f'WHERE \"{self.dataset_rid}\" = \"{sql_member}\".Dataset;'\n            )\n            mapper = SQLMapper(self.model, sql_target)\n            target_entities = [mapper.transform_tuple(e) for e in db.execute(sql_cmd).fetchall()]\n        members[target_table.name].extend(target_entities)\n        if recurse and (target_table.name == self._dataset_table.name):\n            # Get the members for all the nested datasets and add to the member list.\n            nested_datasets = [d[\"RID\"] for d in target_entities]\n            for ds in nested_datasets:\n                nested_dataset = self.model.get_dataset(ds)\n                for k, v in nested_dataset.list_dataset_members(recurse=recurse).items():\n                    members[k].extend(v)\n    return dict(members)\n</code></pre>"},{"location":"code-docs/dataset_bag/#deriva_ml.dataset.dataset_bag.DatasetBag.list_feature_values","title":"list_feature_values","text":"<pre><code>list_feature_values(\n    table: Table | str,\n    feature_name: str,\n) -&gt; datapath._ResultSet\n</code></pre> <p>Return feature values for a table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table | str</code> <p>The table to get feature values for.</p> required <code>feature_name</code> <code>str</code> <p>Name of the feature.</p> required <p>Returns:</p> Type Description <code>_ResultSet</code> <p>Feature values.</p> Source code in <code>src/deriva_ml/dataset/dataset_bag.py</code> <pre><code>def list_feature_values(self, table: Table | str, feature_name: str) -&gt; datapath._ResultSet:\n    \"\"\"Return feature values for a table.\n\n    Args:\n        table: The table to get feature values for.\n        feature_name: Name of the feature.\n\n    Returns:\n        Feature values.\n    \"\"\"\n    feature = self.model.lookup_feature(table, feature_name)\n    feature_table = self.model.normalize_table_name(feature.feature_table.name)\n\n    with self.database as db:\n        col_names = [c[1] for c in db.execute(f'PRAGMA table_info(\"{feature_table}\")').fetchall()]\n        sql_cmd = f'SELECT * FROM \"{feature_table}\"'\n        return cast(datapath._ResultSet, [dict(zip(col_names, r)) for r in db.execute(sql_cmd).fetchall()])\n</code></pre>"},{"location":"code-docs/dataset_bag/#deriva_ml.dataset.dataset_bag.DatasetBag.list_tables","title":"list_tables","text":"<pre><code>list_tables() -&gt; list[str]\n</code></pre> <p>List the names of the tables in the catalog</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of table names.  These names are all qualified with the Deriva schema name.</p> Source code in <code>src/deriva_ml/dataset/dataset_bag.py</code> <pre><code>def list_tables(self) -&gt; list[str]:\n    \"\"\"List the names of the tables in the catalog\n\n    Returns:\n        A list of table names.  These names are all qualified with the Deriva schema name.\n    \"\"\"\n    return self.model.list_tables()\n</code></pre>"},{"location":"code-docs/dataset_bag/#deriva_ml.dataset.dataset_bag.DatasetBag.lookup_term","title":"lookup_term","text":"<pre><code>lookup_term(\n    table: str | Table, term_name: str\n) -&gt; VocabularyTerm\n</code></pre> <p>Finds a term in a vocabulary table.</p> <p>Searches for a term in the specified vocabulary table, matching either the primary name or any of its synonyms.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str | Table</code> <p>Vocabulary table to search in (name or Table object).</p> required <code>term_name</code> <code>str</code> <p>Name or synonym of the term to find.</p> required <p>Returns:</p> Name Type Description <code>VocabularyTerm</code> <code>VocabularyTerm</code> <p>The matching vocabulary term.</p> <p>Raises:</p> Type Description <code>DerivaMLVocabularyException</code> <p>If the table is not a vocabulary table, or term is not found.</p> <p>Examples:</p> <p>Look up by primary name:     &gt;&gt;&gt; term = ml.lookup_term(\"tissue_types\", \"epithelial\")     &gt;&gt;&gt; print(term.description)</p> <p>Look up by synonym:     &gt;&gt;&gt; term = ml.lookup_term(\"tissue_types\", \"epithelium\")</p> Source code in <code>src/deriva_ml/dataset/dataset_bag.py</code> <pre><code>@validate_call(config=ConfigDict(arbitrary_types_allowed=True))\ndef lookup_term(self, table: str | Table, term_name: str) -&gt; VocabularyTerm:\n    \"\"\"Finds a term in a vocabulary table.\n\n    Searches for a term in the specified vocabulary table, matching either the primary name\n    or any of its synonyms.\n\n    Args:\n        table: Vocabulary table to search in (name or Table object).\n        term_name: Name or synonym of the term to find.\n\n    Returns:\n        VocabularyTerm: The matching vocabulary term.\n\n    Raises:\n        DerivaMLVocabularyException: If the table is not a vocabulary table, or term is not found.\n\n    Examples:\n        Look up by primary name:\n            &gt;&gt;&gt; term = ml.lookup_term(\"tissue_types\", \"epithelial\")\n            &gt;&gt;&gt; print(term.description)\n\n        Look up by synonym:\n            &gt;&gt;&gt; term = ml.lookup_term(\"tissue_types\", \"epithelium\")\n    \"\"\"\n    # Get and validate vocabulary table reference\n    vocab_table = self.model.normalize_table_name(table)\n    if not self.model.is_vocabulary(table):\n        raise DerivaMLException(f\"The table {table} is not a controlled vocabulary\")\n\n    # Search for term by name or synonym\n    for term in self.get_table_as_dict(vocab_table):\n        if term_name == term[\"Name\"] or (term[\"Synonyms\"] and term_name in term[\"Synonyms\"]):\n            term[\"Synonyms\"] = list(term[\"Synonyms\"])\n            return VocabularyTerm.model_validate(term)\n\n    # Term not found\n    raise DerivaMLInvalidTerm(vocab_table, term_name)\n</code></pre>"},{"location":"code-docs/deriva_definitions/","title":"Helper Classes","text":"<p>Shared definitions that are used in different DerivaML modules. This module re-exports all symbols from the core submodules for backwards compatibility.</p>"},{"location":"code-docs/deriva_definitions/#deriva_ml.core.definitions.BaseStrEnum","title":"BaseStrEnum","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Base class for string-based enumerations.</p> <p>Extends both str and Enum to create string enums that are both string-like and enumerated. This provides type safety while maintaining string compatibility.</p> Example <p>class MyEnum(BaseStrEnum): ...     VALUE = \"value\" isinstance(MyEnum.VALUE, str)  # True isinstance(MyEnum.VALUE, Enum)  # True</p> Source code in <code>src/deriva_ml/core/enums.py</code> <pre><code>class BaseStrEnum(str, Enum):\n    \"\"\"Base class for string-based enumerations.\n\n    Extends both str and Enum to create string enums that are both string-like and enumerated.\n    This provides type safety while maintaining string compatibility.\n\n    Example:\n        &gt;&gt;&gt; class MyEnum(BaseStrEnum):\n        ...     VALUE = \"value\"\n        &gt;&gt;&gt; isinstance(MyEnum.VALUE, str)  # True\n        &gt;&gt;&gt; isinstance(MyEnum.VALUE, Enum)  # True\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"code-docs/deriva_definitions/#deriva_ml.core.definitions.BuiltinTypes","title":"BuiltinTypes","text":"<p>               Bases: <code>Enum</code></p> <p>ERMrest built-in data types.</p> <p>Maps ERMrest's built-in data types to their type names. These types are used for defining column types in tables and for type validation.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>str</code> <p>Text/string type.</p> <code>int2</code> <code>str</code> <p>16-bit integer.</p> <code>jsonb</code> <code>str</code> <p>Binary JSON.</p> <code>float8</code> <code>str</code> <p>64-bit float.</p> <code>timestamp</code> <code>str</code> <p>Timestamp without timezone.</p> <code>int8</code> <code>str</code> <p>64-bit integer.</p> <code>boolean</code> <code>str</code> <p>Boolean type.</p> <code>json</code> <code>str</code> <p>JSON type.</p> <code>float4</code> <code>str</code> <p>32-bit float.</p> <code>int4</code> <code>str</code> <p>32-bit integer.</p> <code>timestamptz</code> <code>str</code> <p>Timestamp with timezone.</p> <code>date</code> <code>str</code> <p>Date type.</p> <code>ermrest_rid</code> <code>str</code> <p>Resource identifier.</p> <code>ermrest_rcb</code> <code>str</code> <p>Record created by.</p> <code>ermrest_rmb</code> <code>str</code> <p>Record modified by.</p> <code>ermrest_rct</code> <code>str</code> <p>Record creation time.</p> <code>ermrest_rmt</code> <code>str</code> <p>Record modification time.</p> <code>markdown</code> <code>str</code> <p>Markdown text.</p> <code>longtext</code> <code>str</code> <p>Long text.</p> <code>ermrest_curie</code> <code>str</code> <p>Compact URI.</p> <code>ermrest_uri</code> <code>str</code> <p>URI type.</p> <code>color_rgb_hex</code> <code>str</code> <p>RGB color in hex.</p> <code>serial2</code> <code>str</code> <p>16-bit auto-incrementing.</p> <code>serial4</code> <code>str</code> <p>32-bit auto-incrementing.</p> <code>serial8</code> <code>str</code> <p>64-bit auto-incrementing.</p> Source code in <code>src/deriva_ml/core/enums.py</code> <pre><code>class BuiltinTypes(Enum):\n    \"\"\"ERMrest built-in data types.\n\n    Maps ERMrest's built-in data types to their type names. These types are used for defining\n    column types in tables and for type validation.\n\n    Attributes:\n        text (str): Text/string type.\n        int2 (str): 16-bit integer.\n        jsonb (str): Binary JSON.\n        float8 (str): 64-bit float.\n        timestamp (str): Timestamp without timezone.\n        int8 (str): 64-bit integer.\n        boolean (str): Boolean type.\n        json (str): JSON type.\n        float4 (str): 32-bit float.\n        int4 (str): 32-bit integer.\n        timestamptz (str): Timestamp with timezone.\n        date (str): Date type.\n        ermrest_rid (str): Resource identifier.\n        ermrest_rcb (str): Record created by.\n        ermrest_rmb (str): Record modified by.\n        ermrest_rct (str): Record creation time.\n        ermrest_rmt (str): Record modification time.\n        markdown (str): Markdown text.\n        longtext (str): Long text.\n        ermrest_curie (str): Compact URI.\n        ermrest_uri (str): URI type.\n        color_rgb_hex (str): RGB color in hex.\n        serial2 (str): 16-bit auto-incrementing.\n        serial4 (str): 32-bit auto-incrementing.\n        serial8 (str): 64-bit auto-incrementing.\n    \"\"\"\n\n    text = builtin_types.text.typename\n    int2 = builtin_types.int2.typename\n    jsonb = builtin_types.json.typename\n    float8 = builtin_types.float8.typename\n    timestamp = builtin_types.timestamp.typename\n    int8 = builtin_types.int8.typename\n    boolean = builtin_types.boolean.typename\n    json = builtin_types.json.typename\n    float4 = builtin_types.float4.typename\n    int4 = builtin_types.int4.typename\n    timestamptz = builtin_types.timestamptz.typename\n    date = builtin_types.date.typename\n    ermrest_rid = builtin_types.ermrest_rid.typename\n    ermrest_rcb = builtin_types.ermrest_rcb.typename\n    ermrest_rmb = builtin_types.ermrest_rmb.typename\n    ermrest_rct = builtin_types.ermrest_rct.typename\n    ermrest_rmt = builtin_types.ermrest_rmt.typename\n    markdown = builtin_types.markdown.typename\n    longtext = builtin_types.longtext.typename\n    ermrest_curie = builtin_types.ermrest_curie.typename\n    ermrest_uri = builtin_types.ermrest_uri.typename\n    color_rgb_hex = builtin_types.color_rgb_hex.typename\n    serial2 = builtin_types.serial2.typename\n    serial4 = builtin_types.serial4.typename\n    serial8 = builtin_types.serial8.typename\n</code></pre>"},{"location":"code-docs/deriva_definitions/#deriva_ml.core.definitions.ColumnDefinition","title":"ColumnDefinition","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines a column in an ERMrest table.</p> <p>Provides a Pydantic model for defining columns with their types, constraints, and metadata. Maps to deriva_py's Column.define functionality.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the column.</p> <code>type</code> <code>BuiltinTypes</code> <p>ERMrest data type for the column.</p> <code>nullok</code> <code>bool</code> <p>Whether NULL values are allowed. Defaults to True.</p> <code>default</code> <code>Any</code> <p>Default value for the column.</p> <code>comment</code> <code>str | None</code> <p>Description of the column's purpose.</p> <code>acls</code> <code>dict</code> <p>Access control lists.</p> <code>acl_bindings</code> <code>dict</code> <p>Dynamic access control bindings.</p> <code>annotations</code> <code>dict</code> <p>Additional metadata annotations.</p> Example <p>col = ColumnDefinition( ...     name=\"score\", ...     type=BuiltinTypes.float4, ...     nullok=False, ...     comment=\"Confidence score between 0 and 1\" ... )</p> Source code in <code>src/deriva_ml/core/ermrest.py</code> <pre><code>class ColumnDefinition(BaseModel):\n    \"\"\"Defines a column in an ERMrest table.\n\n    Provides a Pydantic model for defining columns with their types, constraints, and metadata.\n    Maps to deriva_py's Column.define functionality.\n\n    Attributes:\n        name (str): Name of the column.\n        type (BuiltinTypes): ERMrest data type for the column.\n        nullok (bool): Whether NULL values are allowed. Defaults to True.\n        default (Any): Default value for the column.\n        comment (str | None): Description of the column's purpose.\n        acls (dict): Access control lists.\n        acl_bindings (dict): Dynamic access control bindings.\n        annotations (dict): Additional metadata annotations.\n\n    Example:\n        &gt;&gt;&gt; col = ColumnDefinition(\n        ...     name=\"score\",\n        ...     type=BuiltinTypes.float4,\n        ...     nullok=False,\n        ...     comment=\"Confidence score between 0 and 1\"\n        ... )\n    \"\"\"\n    name: str\n    type: BuiltinTypes\n    nullok: bool = True\n    default: Any = None\n    comment: str | None = None\n    acls: dict = Field(default_factory=dict)\n    acl_bindings: dict = Field(default_factory=dict)\n    annotations: dict = Field(default_factory=dict)\n\n    @field_validator(\"type\", mode=\"before\")\n    @classmethod\n    def extract_type_name(cls, value: Any) -&gt; Any:\n        if isinstance(value, dict):\n            return BuiltinTypes(value[\"typename\"])\n        else:\n            return value\n\n    @model_serializer()\n    def serialize_column_definition(self):\n        return em.Column.define(\n            self.name,\n            builtin_types[self.type.value],\n            nullok=self.nullok,\n            default=self.default,\n            comment=self.comment,\n            acls=self.acls,\n            acl_bindings=self.acl_bindings,\n            annotations=self.annotations,\n        )\n</code></pre>"},{"location":"code-docs/deriva_definitions/#deriva_ml.core.definitions.ExecAssetType","title":"ExecAssetType","text":"<p>               Bases: <code>BaseStrEnum</code></p> <p>Execution asset type identifiers.</p> <p>Defines the types of assets that can be produced during an execution.</p> <p>Attributes:</p> Name Type Description <code>input_file</code> <code>str</code> <p>Input file used by the execution.</p> <code>output_file</code> <code>str</code> <p>Output file produced by the execution.</p> <code>notebook_output</code> <code>str</code> <p>Jupyter notebook output from the execution.</p> Source code in <code>src/deriva_ml/core/enums.py</code> <pre><code>class ExecAssetType(BaseStrEnum):\n    \"\"\"Execution asset type identifiers.\n\n    Defines the types of assets that can be produced during an execution.\n\n    Attributes:\n        input_file (str): Input file used by the execution.\n        output_file (str): Output file produced by the execution.\n        notebook_output (str): Jupyter notebook output from the execution.\n    \"\"\"\n\n    input_file = \"Input_File\"\n    output_file = \"Output_File\"\n    notebook_output = \"Notebook_Output\"\n    model_file = \"Model_File\"\n</code></pre>"},{"location":"code-docs/deriva_definitions/#deriva_ml.core.definitions.ExecMetadataType","title":"ExecMetadataType","text":"<p>               Bases: <code>BaseStrEnum</code></p> <p>Execution metadata type identifiers.</p> <p>Defines the types of metadata that can be associated with an execution.</p> <p>Attributes:</p> Name Type Description <code>execution_config</code> <code>str</code> <p>Execution configuration data.</p> <code>runtime_env</code> <code>str</code> <p>Runtime environment information.</p> Source code in <code>src/deriva_ml/core/enums.py</code> <pre><code>class ExecMetadataType(BaseStrEnum):\n    \"\"\"Execution metadata type identifiers.\n\n    Defines the types of metadata that can be associated with an execution.\n\n    Attributes:\n        execution_config (str): Execution configuration data.\n        runtime_env (str): Runtime environment information.\n    \"\"\"\n\n    execution_config = \"Execution_Config\"\n    runtime_env = \"Runtime_Env\"\n</code></pre>"},{"location":"code-docs/deriva_definitions/#deriva_ml.core.definitions.FileSpec","title":"FileSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>An entry into the File table</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The File url to the url.</p> <code>description</code> <code>str | None</code> <p>The description of the file.</p> <code>md5</code> <code>str</code> <p>The MD5 hash of the file.</p> <code>length</code> <code>int</code> <p>The length of the file in bytes.</p> <code>file_types</code> <code>conlist(str) | None</code> <p>A list of file types.  Each files_type should be a defined term in MLVocab.file_type vocabulary.</p> Source code in <code>src/deriva_ml/core/filespec.py</code> <pre><code>class FileSpec(BaseModel):\n    \"\"\"An entry into the File table\n\n    Attributes:\n        url: The File url to the url.\n        description: The description of the file.\n        md5: The MD5 hash of the file.\n        length: The length of the file in bytes.\n        file_types: A list of file types.  Each files_type should be a defined term in MLVocab.file_type vocabulary.\n    \"\"\"\n\n    url: str = Field(alias=\"URL\", validation_alias=\"url\")\n    md5: str = Field(alias=\"MD5\", validation_alias=\"md5\")\n    length: int = Field(alias=\"Length\", validation_alias=\"length\")\n    description: str | None = Field(default=\"\", alias=\"Description\", validation_alias=\"description\")\n    file_types: conlist(str) | None = []\n\n    @field_validator(\"url\")\n    @classmethod\n    def validate_file_url(cls, url: str) -&gt; str:\n        \"\"\"Examine the provided URL. If it's a local path, convert it into a tag URL.\n\n        Args:\n            url: The URL to validate and potentially convert\n\n        Returns:\n            The validated/converted URL\n\n        Raises:\n            ValidationError: If the URL is not a file URL\n        \"\"\"\n        url_parts = urlparse(url)\n        if url_parts.scheme == \"tag\":\n            # Already a tag URL, so just return it.\n            return url\n        elif (not url_parts.scheme) or url_parts.scheme == \"file\":\n            # There is no scheme part of the URL, or it is a file URL, so it is a local file path.\n            # Convert to a tag URL.\n            return f\"tag://{gethostname()},{date.today()}:file://{url_parts.path}\"\n        else:\n            raise ValueError(\"url is not a file URL\")\n\n    @classmethod\n    def create_filespecs(\n        cls, path: Path | str, description: str, file_types: list[str] | Callable[[Path], list[str]] | None = None\n    ) -&gt; Generator[FileSpec, None, None]:\n        \"\"\"Given a file or directory, generate the sequence of corresponding FileSpecs suitable to create a File table.\n\n        Args:\n            path: Path to the file or directory.\n            description: The description of the file(s)\n            file_types: A list of file types or a function that takes a file path and returns a list of file types.\n\n        Returns:\n            An iterable of FileSpecs for each file in the directory.\n        \"\"\"\n\n        path = Path(path)\n        file_types = file_types or []\n        file_types_fn = file_types if callable(file_types) else lambda _x: file_types\n\n        def create_spec(file_path: Path) -&gt; FileSpec:\n            hashes = hash_utils.compute_file_hashes(file_path, hashes=frozenset([\"md5\", \"sha256\"]))\n            md5 = hashes[\"md5\"][0]\n            type_list = file_types_fn(file_path)\n            return FileSpec(\n                length=path.stat().st_size,\n                md5=md5,\n                description=description,\n                url=file_path.as_posix(),\n                file_types=type_list if \"File\" in type_list else [\"File\"] + type_list,\n            )\n\n        files = [path] if path.is_file() else [f for f in Path(path).rglob(\"*\") if f.is_file()]\n        return (create_spec(file) for file in files)\n\n    @staticmethod\n    def read_filespec(path: Path | str) -&gt; Generator[FileSpec, None, None]:\n        \"\"\"Get FileSpecs from a JSON lines file.\n\n        Args:\n         path: Path to the .jsonl file (string or Path).\n\n        Yields:\n             A FileSpec object.\n        \"\"\"\n        path = Path(path)\n        with path.open(\"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                yield FileSpec(**json.loads(line))\n</code></pre>"},{"location":"code-docs/deriva_definitions/#deriva_ml.core.definitions.FileSpec.create_filespecs","title":"create_filespecs  <code>classmethod</code>","text":"<pre><code>create_filespecs(\n    path: Path | str,\n    description: str,\n    file_types: list[str]\n    | Callable[[Path], list[str]]\n    | None = None,\n) -&gt; Generator[FileSpec, None, None]\n</code></pre> <p>Given a file or directory, generate the sequence of corresponding FileSpecs suitable to create a File table.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>Path to the file or directory.</p> required <code>description</code> <code>str</code> <p>The description of the file(s)</p> required <code>file_types</code> <code>list[str] | Callable[[Path], list[str]] | None</code> <p>A list of file types or a function that takes a file path and returns a list of file types.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>An iterable of FileSpecs for each file in the directory.</p> Source code in <code>src/deriva_ml/core/filespec.py</code> <pre><code>@classmethod\ndef create_filespecs(\n    cls, path: Path | str, description: str, file_types: list[str] | Callable[[Path], list[str]] | None = None\n) -&gt; Generator[FileSpec, None, None]:\n    \"\"\"Given a file or directory, generate the sequence of corresponding FileSpecs suitable to create a File table.\n\n    Args:\n        path: Path to the file or directory.\n        description: The description of the file(s)\n        file_types: A list of file types or a function that takes a file path and returns a list of file types.\n\n    Returns:\n        An iterable of FileSpecs for each file in the directory.\n    \"\"\"\n\n    path = Path(path)\n    file_types = file_types or []\n    file_types_fn = file_types if callable(file_types) else lambda _x: file_types\n\n    def create_spec(file_path: Path) -&gt; FileSpec:\n        hashes = hash_utils.compute_file_hashes(file_path, hashes=frozenset([\"md5\", \"sha256\"]))\n        md5 = hashes[\"md5\"][0]\n        type_list = file_types_fn(file_path)\n        return FileSpec(\n            length=path.stat().st_size,\n            md5=md5,\n            description=description,\n            url=file_path.as_posix(),\n            file_types=type_list if \"File\" in type_list else [\"File\"] + type_list,\n        )\n\n    files = [path] if path.is_file() else [f for f in Path(path).rglob(\"*\") if f.is_file()]\n    return (create_spec(file) for file in files)\n</code></pre>"},{"location":"code-docs/deriva_definitions/#deriva_ml.core.definitions.FileSpec.read_filespec","title":"read_filespec  <code>staticmethod</code>","text":"<pre><code>read_filespec(\n    path: Path | str,\n) -&gt; Generator[FileSpec, None, None]\n</code></pre> <p>Get FileSpecs from a JSON lines file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>Path to the .jsonl file (string or Path).</p> required <p>Yields:</p> Type Description <code>FileSpec</code> <p>A FileSpec object.</p> Source code in <code>src/deriva_ml/core/filespec.py</code> <pre><code>@staticmethod\ndef read_filespec(path: Path | str) -&gt; Generator[FileSpec, None, None]:\n    \"\"\"Get FileSpecs from a JSON lines file.\n\n    Args:\n     path: Path to the .jsonl file (string or Path).\n\n    Yields:\n         A FileSpec object.\n    \"\"\"\n    path = Path(path)\n    with path.open(\"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            yield FileSpec(**json.loads(line))\n</code></pre>"},{"location":"code-docs/deriva_definitions/#deriva_ml.core.definitions.FileSpec.validate_file_url","title":"validate_file_url  <code>classmethod</code>","text":"<pre><code>validate_file_url(url: str) -&gt; str\n</code></pre> <p>Examine the provided URL. If it's a local path, convert it into a tag URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to validate and potentially convert</p> required <p>Returns:</p> Type Description <code>str</code> <p>The validated/converted URL</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If the URL is not a file URL</p> Source code in <code>src/deriva_ml/core/filespec.py</code> <pre><code>@field_validator(\"url\")\n@classmethod\ndef validate_file_url(cls, url: str) -&gt; str:\n    \"\"\"Examine the provided URL. If it's a local path, convert it into a tag URL.\n\n    Args:\n        url: The URL to validate and potentially convert\n\n    Returns:\n        The validated/converted URL\n\n    Raises:\n        ValidationError: If the URL is not a file URL\n    \"\"\"\n    url_parts = urlparse(url)\n    if url_parts.scheme == \"tag\":\n        # Already a tag URL, so just return it.\n        return url\n    elif (not url_parts.scheme) or url_parts.scheme == \"file\":\n        # There is no scheme part of the URL, or it is a file URL, so it is a local file path.\n        # Convert to a tag URL.\n        return f\"tag://{gethostname()},{date.today()}:file://{url_parts.path}\"\n    else:\n        raise ValueError(\"url is not a file URL\")\n</code></pre>"},{"location":"code-docs/deriva_definitions/#deriva_ml.core.definitions.FileUploadState","title":"FileUploadState","text":"<p>               Bases: <code>BaseModel</code></p> <p>Tracks the state and result of a file upload operation.</p> <p>Attributes:</p> Name Type Description <code>state</code> <code>UploadState</code> <p>Current state of the upload (success, failed, etc.).</p> <code>status</code> <code>str</code> <p>Detailed status message.</p> <code>result</code> <code>Any</code> <p>Upload result data, if any.</p> <code>rid</code> <code>RID | None</code> <p>Resource identifier of the uploaded file, if successful.</p> Source code in <code>src/deriva_ml/core/ermrest.py</code> <pre><code>class FileUploadState(BaseModel):\n    \"\"\"Tracks the state and result of a file upload operation.\n\n    Attributes:\n        state (UploadState): Current state of the upload (success, failed, etc.).\n        status (str): Detailed status message.\n        result (Any): Upload result data, if any.\n        rid (RID | None): Resource identifier of the uploaded file, if successful.\n    \"\"\"\n    state: UploadState\n    status: str\n    result: Any\n\n    @computed_field\n    @property\n    def rid(self) -&gt; RID | None:\n        return self.result and self.result[\"RID\"]\n</code></pre>"},{"location":"code-docs/deriva_definitions/#deriva_ml.core.definitions.ForeignKeyDefinition","title":"ForeignKeyDefinition","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines a foreign key relationship between tables.</p> <p>Provides a Pydantic model for defining foreign key constraints with referential actions and metadata. Maps to deriva_py's ForeignKey.define functionality.</p> <p>Attributes:</p> Name Type Description <code>colnames</code> <code>Iterable[str]</code> <p>Names of columns in the referencing table.</p> <code>pk_sname</code> <code>str</code> <p>Schema name of the referenced table.</p> <code>pk_tname</code> <code>str</code> <p>Name of the referenced table.</p> <code>pk_colnames</code> <code>Iterable[str]</code> <p>Names of columns in the referenced table.</p> <code>constraint_names</code> <code>Iterable[str]</code> <p>Names for the foreign key constraints.</p> <code>on_update</code> <code>str</code> <p>Action on update of referenced row. Defaults to \"NO ACTION\".</p> <code>on_delete</code> <code>str</code> <p>Action on delete of referenced row. Defaults to \"NO ACTION\".</p> <code>comment</code> <code>str | None</code> <p>Description of the relationship.</p> <code>acls</code> <code>dict</code> <p>Access control lists.</p> <code>acl_bindings</code> <code>dict</code> <p>Dynamic access control bindings.</p> <code>annotations</code> <code>dict</code> <p>Additional metadata annotations.</p> Example <p>fk = ForeignKeyDefinition( ...     colnames=[\"dataset_id\"], ...     pk_sname=\"core\", ...     pk_tname=\"dataset\", ...     pk_colnames=[\"id\"], ...     on_delete=\"CASCADE\" ... )</p> Source code in <code>src/deriva_ml/core/ermrest.py</code> <pre><code>class ForeignKeyDefinition(BaseModel):\n    \"\"\"Defines a foreign key relationship between tables.\n\n    Provides a Pydantic model for defining foreign key constraints with referential actions\n    and metadata. Maps to deriva_py's ForeignKey.define functionality.\n\n    Attributes:\n        colnames (Iterable[str]): Names of columns in the referencing table.\n        pk_sname (str): Schema name of the referenced table.\n        pk_tname (str): Name of the referenced table.\n        pk_colnames (Iterable[str]): Names of columns in the referenced table.\n        constraint_names (Iterable[str]): Names for the foreign key constraints.\n        on_update (str): Action on update of referenced row. Defaults to \"NO ACTION\".\n        on_delete (str): Action on delete of referenced row. Defaults to \"NO ACTION\".\n        comment (str | None): Description of the relationship.\n        acls (dict): Access control lists.\n        acl_bindings (dict): Dynamic access control bindings.\n        annotations (dict): Additional metadata annotations.\n\n    Example:\n        &gt;&gt;&gt; fk = ForeignKeyDefinition(\n        ...     colnames=[\"dataset_id\"],\n        ...     pk_sname=\"core\",\n        ...     pk_tname=\"dataset\",\n        ...     pk_colnames=[\"id\"],\n        ...     on_delete=\"CASCADE\"\n        ... )\n    \"\"\"\n    colnames: Iterable[str]\n    pk_sname: str\n    pk_tname: str\n    pk_colnames: Iterable[str]\n    constraint_names: Iterable[str] = Field(default_factory=list)\n    on_update: str = \"NO ACTION\"\n    on_delete: str = \"NO ACTION\"\n    comment: str | None = None\n    acls: dict[str, Any] = Field(default_factory=dict)\n    acl_bindings: dict[str, Any] = Field(default_factory=dict)\n    annotations: dict[str, Any] = Field(default_factory=dict)\n\n    @model_serializer()\n    def serialize_fk_definition(self):\n        return em.ForeignKey.define(\n            fk_colnames=self.colnames,\n            pk_sname=self.pk_sname,\n            pk_tname=self.pk_tname,\n            pk_colnames=self.pk_colnames,\n            on_update=self.on_update,\n            on_delete=self.on_delete,\n            comment=self.comment,\n            acls=self.acls,\n            acl_bindings=self.acl_bindings,\n            annotations=self.annotations,\n        )\n</code></pre>"},{"location":"code-docs/deriva_definitions/#deriva_ml.core.definitions.KeyDefinition","title":"KeyDefinition","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines a key constraint in an ERMrest table.</p> <p>Provides a Pydantic model for defining primary keys and unique constraints. Maps to deriva_py's Key.define functionality.</p> <p>Attributes:</p> Name Type Description <code>colnames</code> <code>Iterable[str]</code> <p>Names of columns that form the key.</p> <code>constraint_names</code> <code>Iterable[str]</code> <p>Names for the key constraints.</p> <code>comment</code> <code>str | None</code> <p>Description of the key's purpose.</p> <code>annotations</code> <code>dict</code> <p>Additional metadata annotations.</p> Example <p>key = KeyDefinition( ...     colnames=[\"id\", \"version\"], ...     constraint_names=[\"unique_id_version\"], ...     comment=\"Unique identifier with version\" ... )</p> Source code in <code>src/deriva_ml/core/ermrest.py</code> <pre><code>class KeyDefinition(BaseModel):\n    \"\"\"Defines a key constraint in an ERMrest table.\n\n    Provides a Pydantic model for defining primary keys and unique constraints.\n    Maps to deriva_py's Key.define functionality.\n\n    Attributes:\n        colnames (Iterable[str]): Names of columns that form the key.\n        constraint_names (Iterable[str]): Names for the key constraints.\n        comment (str | None): Description of the key's purpose.\n        annotations (dict): Additional metadata annotations.\n\n    Example:\n        &gt;&gt;&gt; key = KeyDefinition(\n        ...     colnames=[\"id\", \"version\"],\n        ...     constraint_names=[\"unique_id_version\"],\n        ...     comment=\"Unique identifier with version\"\n        ... )\n    \"\"\"\n    colnames: Iterable[str]\n    constraint_names: Iterable[str]\n    comment: str | None = None\n    annotations: dict = Field(default_factory=dict)\n\n    @model_serializer()\n    def serialize_key_definition(self):\n        return em.Key.define(\n            colnames=self.colnames,\n            constraint_names=self.constraint_names,\n            comment=self.comment,\n            annotations=self.annotations,\n        )\n</code></pre>"},{"location":"code-docs/deriva_definitions/#deriva_ml.core.definitions.MLAsset","title":"MLAsset","text":"<p>               Bases: <code>BaseStrEnum</code></p> <p>Asset type identifiers.</p> <p>Defines the types of assets that can be associated with executions.</p> <p>Attributes:</p> Name Type Description <code>execution_metadata</code> <code>str</code> <p>Metadata about an execution.</p> <code>execution_asset</code> <code>str</code> <p>Asset produced by an execution.</p> Source code in <code>src/deriva_ml/core/enums.py</code> <pre><code>class MLAsset(BaseStrEnum):\n    \"\"\"Asset type identifiers.\n\n    Defines the types of assets that can be associated with executions.\n\n    Attributes:\n        execution_metadata (str): Metadata about an execution.\n        execution_asset (str): Asset produced by an execution.\n    \"\"\"\n\n    execution_metadata = \"Execution_Metadata\"\n    execution_asset = \"Execution_Asset\"\n</code></pre>"},{"location":"code-docs/deriva_definitions/#deriva_ml.core.definitions.MLVocab","title":"MLVocab","text":"<p>               Bases: <code>BaseStrEnum</code></p> <p>Controlled vocabulary type identifiers.</p> <p>Defines the names of controlled vocabulary tables used in DerivaML for various types of entities and attributes.</p> <p>Attributes:</p> Name Type Description <code>dataset_type</code> <code>str</code> <p>Dataset classification vocabulary.</p> <code>workflow_type</code> <code>str</code> <p>Workflow classification vocabulary.</p> <code>asset_type</code> <code>str</code> <p>Asset classification vocabulary.</p> <code>asset_role</code> <code>str</code> <p>Asset role classification vocabulary.</p> Source code in <code>src/deriva_ml/core/enums.py</code> <pre><code>class MLVocab(BaseStrEnum):\n    \"\"\"Controlled vocabulary type identifiers.\n\n    Defines the names of controlled vocabulary tables used in DerivaML for various types\n    of entities and attributes.\n\n    Attributes:\n        dataset_type (str): Dataset classification vocabulary.\n        workflow_type (str): Workflow classification vocabulary.\n        asset_type (str): Asset classification vocabulary.\n        asset_role (str): Asset role classification vocabulary.\n    \"\"\"\n\n    dataset_type = \"Dataset_Type\"\n    workflow_type = \"Workflow_Type\"\n    asset_type = \"Asset_Type\"\n    asset_role = \"Asset_Role\"\n    feature_name = \"Feature_Name\"\n</code></pre>"},{"location":"code-docs/deriva_definitions/#deriva_ml.core.definitions.Status","title":"Status","text":"<p>               Bases: <code>BaseStrEnum</code></p> <p>Execution status values.</p> <p>Represents the various states an execution can be in throughout its lifecycle.</p> <p>Attributes:</p> Name Type Description <code>initializing</code> <code>str</code> <p>Initial setup is in progress.</p> <code>created</code> <code>str</code> <p>Execution record has been created.</p> <code>pending</code> <code>str</code> <p>Execution is queued.</p> <code>running</code> <code>str</code> <p>Execution is in progress.</p> <code>aborted</code> <code>str</code> <p>Execution was manually stopped.</p> <code>completed</code> <code>str</code> <p>Execution finished successfully.</p> <code>failed</code> <code>str</code> <p>Execution encountered an error.</p> Source code in <code>src/deriva_ml/core/enums.py</code> <pre><code>class Status(BaseStrEnum):\n    \"\"\"Execution status values.\n\n    Represents the various states an execution can be in throughout its lifecycle.\n\n    Attributes:\n        initializing (str): Initial setup is in progress.\n        created (str): Execution record has been created.\n        pending (str): Execution is queued.\n        running (str): Execution is in progress.\n        aborted (str): Execution was manually stopped.\n        completed (str): Execution finished successfully.\n        failed (str): Execution encountered an error.\n    \"\"\"\n\n    initializing = \"Initializing\"\n    created = \"Created\"\n    pending = \"Pending\"\n    running = \"Running\"\n    aborted = \"Aborted\"\n    completed = \"Completed\"\n    failed = \"Failed\"\n</code></pre>"},{"location":"code-docs/deriva_definitions/#deriva_ml.core.definitions.TableDefinition","title":"TableDefinition","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines a complete table structure in ERMrest.</p> <p>Provides a Pydantic model for defining tables with their columns, keys, and relationships. Maps to deriva_py's Table.define functionality.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the table.</p> <code>column_defs</code> <code>Iterable[ColumnDefinition]</code> <p>Column definitions.</p> <code>key_defs</code> <code>Iterable[KeyDefinition]</code> <p>Key constraint definitions.</p> <code>fkey_defs</code> <code>Iterable[ForeignKeyDefinition]</code> <p>Foreign key relationship definitions.</p> <code>comment</code> <code>str | None</code> <p>Description of the table's purpose.</p> <code>acls</code> <code>dict</code> <p>Access control lists.</p> <code>acl_bindings</code> <code>dict</code> <p>Dynamic access control bindings.</p> <code>annotations</code> <code>dict</code> <p>Additional metadata annotations.</p> Example <p>table = TableDefinition( ...     name=\"experiment\", ...     column_defs=[ ...         ColumnDefinition(name=\"id\", type=BuiltinTypes.text), ...         ColumnDefinition(name=\"date\", type=BuiltinTypes.date) ...     ], ...     comment=\"Experimental data records\" ... )</p> Source code in <code>src/deriva_ml/core/ermrest.py</code> <pre><code>class TableDefinition(BaseModel):\n    \"\"\"Defines a complete table structure in ERMrest.\n\n    Provides a Pydantic model for defining tables with their columns, keys, and relationships.\n    Maps to deriva_py's Table.define functionality.\n\n    Attributes:\n        name (str): Name of the table.\n        column_defs (Iterable[ColumnDefinition]): Column definitions.\n        key_defs (Iterable[KeyDefinition]): Key constraint definitions.\n        fkey_defs (Iterable[ForeignKeyDefinition]): Foreign key relationship definitions.\n        comment (str | None): Description of the table's purpose.\n        acls (dict): Access control lists.\n        acl_bindings (dict): Dynamic access control bindings.\n        annotations (dict): Additional metadata annotations.\n\n    Example:\n        &gt;&gt;&gt; table = TableDefinition(\n        ...     name=\"experiment\",\n        ...     column_defs=[\n        ...         ColumnDefinition(name=\"id\", type=BuiltinTypes.text),\n        ...         ColumnDefinition(name=\"date\", type=BuiltinTypes.date)\n        ...     ],\n        ...     comment=\"Experimental data records\"\n        ... )\n    \"\"\"\n    name: str\n    column_defs: Iterable[ColumnDefinition]\n    key_defs: Iterable[KeyDefinition] = Field(default_factory=list)\n    fkey_defs: Iterable[ForeignKeyDefinition] = Field(default_factory=list)\n    comment: str | None = None\n    acls: dict = Field(default_factory=dict)\n    acl_bindings: dict = Field(default_factory=dict)\n    annotations: dict = Field(default_factory=dict)\n\n    @model_serializer()\n    def serialize_table_definition(self):\n        return em.Table.define(\n            tname=self.name,\n            column_defs=[c.model_dump() for c in self.column_defs],\n            key_defs=[k.model_dump() for k in self.key_defs],\n            fkey_defs=[fk.model_dump() for fk in self.fkey_defs],\n            comment=self.comment,\n            acls=self.acls,\n            acl_bindings=self.acl_bindings,\n            annotations=self.annotations,\n        )\n</code></pre>"},{"location":"code-docs/deriva_definitions/#deriva_ml.core.definitions.UploadState","title":"UploadState","text":"<p>               Bases: <code>Enum</code></p> <p>File upload operation states.</p> <p>Represents the various states a file upload operation can be in, from initiation to completion.</p> <p>Attributes:</p> Name Type Description <code>success</code> <code>int</code> <p>Upload completed successfully.</p> <code>failed</code> <code>int</code> <p>Upload failed.</p> <code>pending</code> <code>int</code> <p>Upload is queued.</p> <code>running</code> <code>int</code> <p>Upload is in progress.</p> <code>paused</code> <code>int</code> <p>Upload is temporarily paused.</p> <code>aborted</code> <code>int</code> <p>Upload was aborted.</p> <code>cancelled</code> <code>int</code> <p>Upload was cancelled.</p> <code>timeout</code> <code>int</code> <p>Upload timed out.</p> Source code in <code>src/deriva_ml/core/enums.py</code> <pre><code>class UploadState(Enum):\n    \"\"\"File upload operation states.\n\n    Represents the various states a file upload operation can be in, from initiation to completion.\n\n    Attributes:\n        success (int): Upload completed successfully.\n        failed (int): Upload failed.\n        pending (int): Upload is queued.\n        running (int): Upload is in progress.\n        paused (int): Upload is temporarily paused.\n        aborted (int): Upload was aborted.\n        cancelled (int): Upload was cancelled.\n        timeout (int): Upload timed out.\n    \"\"\"\n\n    success = 0\n    failed = 1\n    pending = 2\n    running = 3\n    paused = 4\n    aborted = 5\n    cancelled = 6\n    timeout = 7\n</code></pre>"},{"location":"code-docs/deriva_definitions/#deriva_ml.core.definitions.VocabularyTerm","title":"VocabularyTerm","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a term in a controlled vocabulary.</p> <p>A vocabulary term is a standardized entry in a controlled vocabulary table. Each term has a primary name, optional synonyms, and identifiers for cross-referencing.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Primary name of the term.</p> <code>synonyms</code> <code>list[str] | None</code> <p>Alternative names for the term.</p> <code>id</code> <code>str</code> <p>CURIE (Compact URI) identifier.</p> <code>uri</code> <code>str</code> <p>Full URI for the term.</p> <code>description</code> <code>str</code> <p>Explanation of the term's meaning.</p> <code>rid</code> <code>str</code> <p>Resource identifier in the catalog.</p> Example <p>term = VocabularyTerm( ...     Name=\"epithelial\", ...     Synonyms=[\"epithelium\"], ...     ID=\"tissue:0001\", ...     URI=\"http://example.org/tissue/0001\", ...     Description=\"Epithelial tissue type\", ...     RID=\"1-abc123\" ... )</p> Source code in <code>src/deriva_ml/core/ermrest.py</code> <pre><code>class VocabularyTerm(BaseModel):\n    \"\"\"Represents a term in a controlled vocabulary.\n\n    A vocabulary term is a standardized entry in a controlled vocabulary table. Each term has\n    a primary name, optional synonyms, and identifiers for cross-referencing.\n\n    Attributes:\n        name (str): Primary name of the term.\n        synonyms (list[str] | None): Alternative names for the term.\n        id (str): CURIE (Compact URI) identifier.\n        uri (str): Full URI for the term.\n        description (str): Explanation of the term's meaning.\n        rid (str): Resource identifier in the catalog.\n\n    Example:\n        &gt;&gt;&gt; term = VocabularyTerm(\n        ...     Name=\"epithelial\",\n        ...     Synonyms=[\"epithelium\"],\n        ...     ID=\"tissue:0001\",\n        ...     URI=\"http://example.org/tissue/0001\",\n        ...     Description=\"Epithelial tissue type\",\n        ...     RID=\"1-abc123\"\n        ... )\n    \"\"\"\n    name: str = Field(alias=\"Name\")\n    synonyms: list[str] | None = Field(alias=\"Synonyms\")\n    id: str = Field(alias=\"ID\")\n    uri: str = Field(alias=\"URI\")\n    description: str = Field(alias=\"Description\")\n    rid: str = Field(alias=\"RID\")\n\n    class Config:\n        extra = \"ignore\"\n</code></pre>"},{"location":"code-docs/deriva_ml_base/","title":"<code>DerivaML</code> Class","text":"<p>The DerivaML class provides a range of methods to interact with a Deriva catalog. These methods assume tha tthe catalog contains a <code>deriva-ml</code> and a domain schema.</p> <p>Data Catalog: The catalog must include both the domain schema and a standard ML schema for effective data management.</p> <p></p> <ul> <li>Domain schema: The domain schema includes the data collected or generated by domain-specific experiments or systems.</li> <li>ML schema: Each entity in the ML schema is designed to capture details of the ML development process. It including the following tables<ul> <li>A Dataset represents a data collection, such as aggregation identified for training, validation, and testing purposes.</li> <li>A Workflow represents a specific sequence of computational steps or human interactions.</li> <li>An Execution is an instance of a workflow that a user instantiates at a specific time. </li> <li>An Execution Asset is an output file that results from the execution of a workflow.</li> <li>An Execution Metadata is an asset entity for saving metadata files referencing a given execution.</li> </ul> </li> </ul>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.BuiltinTypes","title":"BuiltinTypes","text":"<p>               Bases: <code>Enum</code></p> <p>ERMrest built-in data types.</p> <p>Maps ERMrest's built-in data types to their type names. These types are used for defining column types in tables and for type validation.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>str</code> <p>Text/string type.</p> <code>int2</code> <code>str</code> <p>16-bit integer.</p> <code>jsonb</code> <code>str</code> <p>Binary JSON.</p> <code>float8</code> <code>str</code> <p>64-bit float.</p> <code>timestamp</code> <code>str</code> <p>Timestamp without timezone.</p> <code>int8</code> <code>str</code> <p>64-bit integer.</p> <code>boolean</code> <code>str</code> <p>Boolean type.</p> <code>json</code> <code>str</code> <p>JSON type.</p> <code>float4</code> <code>str</code> <p>32-bit float.</p> <code>int4</code> <code>str</code> <p>32-bit integer.</p> <code>timestamptz</code> <code>str</code> <p>Timestamp with timezone.</p> <code>date</code> <code>str</code> <p>Date type.</p> <code>ermrest_rid</code> <code>str</code> <p>Resource identifier.</p> <code>ermrest_rcb</code> <code>str</code> <p>Record created by.</p> <code>ermrest_rmb</code> <code>str</code> <p>Record modified by.</p> <code>ermrest_rct</code> <code>str</code> <p>Record creation time.</p> <code>ermrest_rmt</code> <code>str</code> <p>Record modification time.</p> <code>markdown</code> <code>str</code> <p>Markdown text.</p> <code>longtext</code> <code>str</code> <p>Long text.</p> <code>ermrest_curie</code> <code>str</code> <p>Compact URI.</p> <code>ermrest_uri</code> <code>str</code> <p>URI type.</p> <code>color_rgb_hex</code> <code>str</code> <p>RGB color in hex.</p> <code>serial2</code> <code>str</code> <p>16-bit auto-incrementing.</p> <code>serial4</code> <code>str</code> <p>32-bit auto-incrementing.</p> <code>serial8</code> <code>str</code> <p>64-bit auto-incrementing.</p> Source code in <code>src/deriva_ml/core/enums.py</code> <pre><code>class BuiltinTypes(Enum):\n    \"\"\"ERMrest built-in data types.\n\n    Maps ERMrest's built-in data types to their type names. These types are used for defining\n    column types in tables and for type validation.\n\n    Attributes:\n        text (str): Text/string type.\n        int2 (str): 16-bit integer.\n        jsonb (str): Binary JSON.\n        float8 (str): 64-bit float.\n        timestamp (str): Timestamp without timezone.\n        int8 (str): 64-bit integer.\n        boolean (str): Boolean type.\n        json (str): JSON type.\n        float4 (str): 32-bit float.\n        int4 (str): 32-bit integer.\n        timestamptz (str): Timestamp with timezone.\n        date (str): Date type.\n        ermrest_rid (str): Resource identifier.\n        ermrest_rcb (str): Record created by.\n        ermrest_rmb (str): Record modified by.\n        ermrest_rct (str): Record creation time.\n        ermrest_rmt (str): Record modification time.\n        markdown (str): Markdown text.\n        longtext (str): Long text.\n        ermrest_curie (str): Compact URI.\n        ermrest_uri (str): URI type.\n        color_rgb_hex (str): RGB color in hex.\n        serial2 (str): 16-bit auto-incrementing.\n        serial4 (str): 32-bit auto-incrementing.\n        serial8 (str): 64-bit auto-incrementing.\n    \"\"\"\n\n    text = builtin_types.text.typename\n    int2 = builtin_types.int2.typename\n    jsonb = builtin_types.json.typename\n    float8 = builtin_types.float8.typename\n    timestamp = builtin_types.timestamp.typename\n    int8 = builtin_types.int8.typename\n    boolean = builtin_types.boolean.typename\n    json = builtin_types.json.typename\n    float4 = builtin_types.float4.typename\n    int4 = builtin_types.int4.typename\n    timestamptz = builtin_types.timestamptz.typename\n    date = builtin_types.date.typename\n    ermrest_rid = builtin_types.ermrest_rid.typename\n    ermrest_rcb = builtin_types.ermrest_rcb.typename\n    ermrest_rmb = builtin_types.ermrest_rmb.typename\n    ermrest_rct = builtin_types.ermrest_rct.typename\n    ermrest_rmt = builtin_types.ermrest_rmt.typename\n    markdown = builtin_types.markdown.typename\n    longtext = builtin_types.longtext.typename\n    ermrest_curie = builtin_types.ermrest_curie.typename\n    ermrest_uri = builtin_types.ermrest_uri.typename\n    color_rgb_hex = builtin_types.color_rgb_hex.typename\n    serial2 = builtin_types.serial2.typename\n    serial4 = builtin_types.serial4.typename\n    serial8 = builtin_types.serial8.typename\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.ColumnDefinition","title":"ColumnDefinition","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines a column in an ERMrest table.</p> <p>Provides a Pydantic model for defining columns with their types, constraints, and metadata. Maps to deriva_py's Column.define functionality.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the column.</p> <code>type</code> <code>BuiltinTypes</code> <p>ERMrest data type for the column.</p> <code>nullok</code> <code>bool</code> <p>Whether NULL values are allowed. Defaults to True.</p> <code>default</code> <code>Any</code> <p>Default value for the column.</p> <code>comment</code> <code>str | None</code> <p>Description of the column's purpose.</p> <code>acls</code> <code>dict</code> <p>Access control lists.</p> <code>acl_bindings</code> <code>dict</code> <p>Dynamic access control bindings.</p> <code>annotations</code> <code>dict</code> <p>Additional metadata annotations.</p> Example <p>col = ColumnDefinition( ...     name=\"score\", ...     type=BuiltinTypes.float4, ...     nullok=False, ...     comment=\"Confidence score between 0 and 1\" ... )</p> Source code in <code>src/deriva_ml/core/ermrest.py</code> <pre><code>class ColumnDefinition(BaseModel):\n    \"\"\"Defines a column in an ERMrest table.\n\n    Provides a Pydantic model for defining columns with their types, constraints, and metadata.\n    Maps to deriva_py's Column.define functionality.\n\n    Attributes:\n        name (str): Name of the column.\n        type (BuiltinTypes): ERMrest data type for the column.\n        nullok (bool): Whether NULL values are allowed. Defaults to True.\n        default (Any): Default value for the column.\n        comment (str | None): Description of the column's purpose.\n        acls (dict): Access control lists.\n        acl_bindings (dict): Dynamic access control bindings.\n        annotations (dict): Additional metadata annotations.\n\n    Example:\n        &gt;&gt;&gt; col = ColumnDefinition(\n        ...     name=\"score\",\n        ...     type=BuiltinTypes.float4,\n        ...     nullok=False,\n        ...     comment=\"Confidence score between 0 and 1\"\n        ... )\n    \"\"\"\n    name: str\n    type: BuiltinTypes\n    nullok: bool = True\n    default: Any = None\n    comment: str | None = None\n    acls: dict = Field(default_factory=dict)\n    acl_bindings: dict = Field(default_factory=dict)\n    annotations: dict = Field(default_factory=dict)\n\n    @field_validator(\"type\", mode=\"before\")\n    @classmethod\n    def extract_type_name(cls, value: Any) -&gt; Any:\n        if isinstance(value, dict):\n            return BuiltinTypes(value[\"typename\"])\n        else:\n            return value\n\n    @model_serializer()\n    def serialize_column_definition(self):\n        return em.Column.define(\n            self.name,\n            builtin_types[self.type.value],\n            nullok=self.nullok,\n            default=self.default,\n            comment=self.comment,\n            acls=self.acls,\n            acl_bindings=self.acl_bindings,\n            annotations=self.annotations,\n        )\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML","title":"DerivaML","text":"<p>               Bases: <code>Dataset</code></p> <p>Core class for machine learning operations on a Deriva catalog.</p> <p>This class provides core functionality for managing ML workflows, features, and datasets in a Deriva catalog. It handles data versioning, feature management, vocabulary control, and execution tracking.</p> <p>Attributes:</p> Name Type Description <code>host_name</code> <code>str</code> <p>Hostname of the Deriva server (e.g., 'deriva.example.org').</p> <code>catalog_id</code> <code>Union[str, int]</code> <p>Catalog identifier or name.</p> <code>domain_schema</code> <code>str</code> <p>Schema name for domain-specific tables and relationships.</p> <code>model</code> <code>DerivaModel</code> <p>ERMRest model for the catalog.</p> <code>working_dir</code> <code>Path</code> <p>Directory for storing computation data and results.</p> <code>cache_dir</code> <code>Path</code> <p>Directory for caching downloaded datasets.</p> <code>ml_schema</code> <code>str</code> <p>Schema name for ML-specific tables (default: 'deriva_ml').</p> <code>configuration</code> <code>ExecutionConfiguration</code> <p>Current execution configuration.</p> <code>project_name</code> <code>str</code> <p>Name of the current project.</p> <code>start_time</code> <code>datetime</code> <p>Timestamp when this instance was created.</p> <code>status</code> <code>str</code> <p>Current status of operations.</p> Example <p>ml = DerivaML('deriva.example.org', 'my_catalog') ml.create_feature('my_table', 'new_feature') ml.add_term('vocabulary_table', 'new_term', description='Description of term')</p> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>class DerivaML(Dataset):\n    \"\"\"Core class for machine learning operations on a Deriva catalog.\n\n    This class provides core functionality for managing ML workflows, features, and datasets in a Deriva catalog.\n    It handles data versioning, feature management, vocabulary control, and execution tracking.\n\n    Attributes:\n        host_name (str): Hostname of the Deriva server (e.g., 'deriva.example.org').\n        catalog_id (Union[str, int]): Catalog identifier or name.\n        domain_schema (str): Schema name for domain-specific tables and relationships.\n        model (DerivaModel): ERMRest model for the catalog.\n        working_dir (Path): Directory for storing computation data and results.\n        cache_dir (Path): Directory for caching downloaded datasets.\n        ml_schema (str): Schema name for ML-specific tables (default: 'deriva_ml').\n        configuration (ExecutionConfiguration): Current execution configuration.\n        project_name (str): Name of the current project.\n        start_time (datetime): Timestamp when this instance was created.\n        status (str): Current status of operations.\n\n    Example:\n        &gt;&gt;&gt; ml = DerivaML('deriva.example.org', 'my_catalog')\n        &gt;&gt;&gt; ml.create_feature('my_table', 'new_feature')\n        &gt;&gt;&gt; ml.add_term('vocabulary_table', 'new_term', description='Description of term')\n    \"\"\"\n\n    def __init__(\n        self,\n        hostname: str,\n        catalog_id: str | int,\n        domain_schema: str | None = None,\n        project_name: str | None = None,\n        cache_dir: str | Path | None = None,\n        working_dir: str | Path | None = None,\n        ml_schema: str = ML_SCHEMA,\n        logging_level=logging.WARNING,\n        credential=None,\n        use_minid: bool = True,\n        check_auth: bool = True,\n    ):\n        \"\"\"Initializes a DerivaML instance.\n\n        This method will connect to a catalog and initialize local configuration for the ML execution.\n        This class is intended to be used as a base class on which domain-specific interfaces are built.\n\n        Args:\n            hostname: Hostname of the Deriva server.\n            catalog_id: Catalog ID. Either an identifier or a catalog name.\n            domain_schema: Schema name for domain-specific tables and relationships. Defaults to the name of the\n                schema that is not one of the standard schemas.  If there is more than one user-defined schema, then\n                this argument must be provided a value.\n            ml_schema: Schema name for ML schema. Used if you have a non-standard configuration of deriva-ml.\n            project_name: Project name. Defaults to name of domain schema.\n            cache_dir: Directory path for caching data downloaded from the Deriva server as bdbag. If not provided,\n                will default to working_dir.\n            working_dir: Directory path for storing data used by or generated by any computations. If no value is\n                provided, will default to  ${HOME}/deriva_ml\n            use_minid: Use the MINID service when downloading dataset bags.\n            check_auth: Check if the user has access to the catalog.\n        \"\"\"\n        # Get or use provided credentials for server access\n        self.credential = credential or get_credential(hostname)\n\n        # Initialize server connection and catalog access\n        server = DerivaServer(\n            \"https\",\n            hostname,\n            credentials=self.credential,\n            session_config=self._get_session_config(),\n        )\n\n        try:\n            if check_auth and server.get_authn_session():\n                pass\n        except Exception:\n            raise DerivaMLException(\n                \"You are not authorized to access this catalog. \"\n                \"Please check your credentials and make sure you have logged in.\"\n            )\n\n        self.catalog = server.connect_ermrest(catalog_id)\n        self.model = DerivaModel(self.catalog.getCatalogModel(), domain_schema=domain_schema)\n\n        # Set up working and cache directories\n        default_workdir = self.__class__.__name__ + \"_working\"\n        self.working_dir = (\n            Path(working_dir) / getpass.getuser() if working_dir else Path.home() / \"deriva-ml\"\n        ) / default_workdir\n\n        self.working_dir.mkdir(parents=True, exist_ok=True)\n        self.cache_dir = Path(cache_dir) if cache_dir else self.working_dir / \"cache\"\n        self.cache_dir.mkdir(parents=True, exist_ok=True)\n\n        # Initialize dataset functionality from the parent class\n        super().__init__(self.model, self.cache_dir, self.working_dir, use_minid=use_minid)\n\n        # Set up logging\n        self._logger = logging.getLogger(\"deriva_ml\")\n        self._logger.setLevel(logging_level)\n\n        # Store instance configuration\n        self.host_name = hostname\n        self.catalog_id = catalog_id\n        self.ml_schema = ml_schema\n        self.configuration = None\n        self._execution: Execution | None = None\n        self.domain_schema = self.model.domain_schema\n        self.project_name = project_name or self.domain_schema\n        self.start_time = datetime.now()\n        self.status = Status.pending.value\n\n        # Configure logging format\n        logging.basicConfig(\n            level=logging_level,\n            format=\"%(asctime)s - %(name)s.%(levelname)s - %(message)s\",\n        )\n\n        # Set Deriva library logging level\n        deriva_logger = logging.getLogger(\"deriva\")\n        deriva_logger.setLevel(logging_level)\n\n    def __del__(self):\n        \"\"\"Cleanup method to handle incomplete executions.\"\"\"\n        try:\n            # Mark execution as aborted if not completed\n            if self._execution and self._execution.status != Status.completed:\n                self._execution.update_status(Status.aborted, \"Execution Aborted\")\n        except (AttributeError, requests.HTTPError):\n            pass\n\n    @staticmethod\n    def _get_session_config():\n        \"\"\"Returns customized HTTP session configuration.\n\n        Configures retry behavior and connection settings for HTTP requests to the Deriva server. Settings include:\n        - Idempotent retry behavior for all HTTP methods\n        - Increased retry attempts for read and connect operations\n        - Exponential backoff for retries\n\n        Returns:\n            dict: Session configuration dictionary with retry and connection settings.\n\n        Example:\n            &gt;&gt;&gt; config = DerivaML._get_session_config()\n            &gt;&gt;&gt; print(config['retry_read']) # 8\n        \"\"\"\n        # Start with a default configuration\n        session_config = DEFAULT_SESSION_CONFIG.copy()\n\n        # Customize retry behavior for robustness\n        session_config.update(\n            {\n                # Allow retries for all HTTP methods (PUT/POST are idempotent)\n                \"allow_retry_on_all_methods\": True,\n                # Increase retry attempts for better reliability\n                \"retry_read\": 8,\n                \"retry_connect\": 5,\n                # Use exponential backoff for retries\n                \"retry_backoff_factor\": 5,\n            }\n        )\n        return session_config\n\n    @property\n    def pathBuilder(self) -&gt; SchemaWrapper:\n        \"\"\"Returns catalog path builder for queries.\n\n        The path builder provides a fluent interface for constructing complex queries against the catalog.\n        This is a core component used by many other methods to interact with the catalog.\n\n        Returns:\n            datapath._CatalogWrapper: A new instance of the catalog path builder.\n\n        Example:\n            &gt;&gt;&gt; path = ml.pathBuilder.schemas['my_schema'].tables['my_table']\n            &gt;&gt;&gt; results = path.entities().fetch()\n        \"\"\"\n        return self.catalog.getPathBuilder()\n\n    @property\n    def domain_path(self) -&gt; datapath.DataPath:\n        \"\"\"Returns path builder for domain schema.\n\n        Provides a convenient way to access tables and construct queries within the domain-specific schema.\n\n        Returns:\n            datapath._CatalogWrapper: Path builder object scoped to the domain schema.\n\n        Example:\n            &gt;&gt;&gt; domain = ml.domain_path\n            &gt;&gt;&gt; results = domain.my_table.entities().fetch()\n        \"\"\"\n        return self.pathBuilder.schemas[self.domain_schema]\n\n    def table_path(self, table: str | Table) -&gt; Path:\n        \"\"\"Returns a local filesystem path for table CSV files.\n\n        Generates a standardized path where CSV files should be placed when preparing to upload data to a table.\n        The path follows the project's directory structure conventions.\n\n        Args:\n            table: Name of the table or Table object to get the path for.\n\n        Returns:\n            Path: Filesystem path where the CSV file should be placed.\n\n        Example:\n            &gt;&gt;&gt; path = ml.table_path(\"experiment_results\")\n            &gt;&gt;&gt; df.to_csv(path) # Save data for upload\n        \"\"\"\n        return table_path(\n            self.working_dir,\n            schema=self.domain_schema,\n            table=self.model.name_to_table(table).name,\n        )\n\n    def download_dir(self, cached: bool = False) -&gt; Path:\n        \"\"\"Returns the appropriate download directory.\n\n        Provides the appropriate directory path for storing downloaded files, either in the cache or working directory.\n\n        Args:\n            cached: If True, returns the cache directory path. If False, returns the working directory path.\n\n        Returns:\n            Path: Directory path where downloaded files should be stored.\n\n        Example:\n            &gt;&gt;&gt; cache_dir = ml.download_dir(cached=True)\n            &gt;&gt;&gt; work_dir = ml.download_dir(cached=False)\n        \"\"\"\n        # Return cache directory if cached=True, otherwise working directory\n        return self.cache_dir if cached else self.working_dir\n\n    @staticmethod\n    def globus_login(host: str) -&gt; None:\n        \"\"\"Authenticates with Globus for accessing Deriva services.\n\n        Performs authentication using Globus Auth to access Deriva services. If already logged in, notifies the user.\n        Uses non-interactive authentication flow without a browser or local server.\n\n        Args:\n            host: The hostname of the Deriva server to authenticate with (e.g., 'deriva.example.org').\n\n        Example:\n            &gt;&gt;&gt; DerivaML.globus_login('deriva.example.org')\n            'Login Successful'\n        \"\"\"\n        gnl = GlobusNativeLogin(host=host)\n        if gnl.is_logged_in([host]):\n            print(\"You are already logged in.\")\n        else:\n            gnl.login(\n                [host],\n                no_local_server=True,\n                no_browser=True,\n                refresh_tokens=True,\n                update_bdbag_keychain=True,\n            )\n            print(\"Login Successful\")\n\n    def chaise_url(self, table: RID | Table | str) -&gt; str:\n        \"\"\"Generates Chaise web interface URL.\n\n        Chaise is Deriva's web interface for data exploration. This method creates a URL that directly links to\n        the specified table or record.\n\n        Args:\n            table: Table to generate URL for (name, Table object, or RID).\n\n        Returns:\n            str: URL in format: https://{host}/chaise/recordset/#{catalog}/{schema}:{table}\n\n        Raises:\n            DerivaMLException: If table or RID cannot be found.\n\n        Examples:\n            Using table name:\n                &gt;&gt;&gt; ml.chaise_url(\"experiment_table\")\n                'https://deriva.org/chaise/recordset/#1/schema:experiment_table'\n\n            Using RID:\n                &gt;&gt;&gt; ml.chaise_url(\"1-abc123\")\n        \"\"\"\n        # Get the table object and build base URI\n        table_obj = self.model.name_to_table(table)\n        try:\n            uri = self.catalog.get_server_uri().replace(\"ermrest/catalog/\", \"chaise/recordset/#\")\n        except DerivaMLException:\n            # Handle RID case\n            uri = self.cite(cast(str, table))\n        return f\"{uri}/{urlquote(table_obj.schema.name)}:{urlquote(table_obj.name)}\"\n\n    def cite(self, entity: Dict[str, Any] | str) -&gt; str:\n        \"\"\"Generates permanent citation URL.\n\n        Creates a versioned URL that can be used to reference a specific entity in the catalog. The URL includes\n        the catalog snapshot time to ensure version stability.\n\n        Args:\n            entity: Either a RID string or a dictionary containing entity data with a 'RID' key.\n\n        Returns:\n            str: Permanent citation URL in format: https://{host}/id/{catalog}/{rid}@{snapshot_time}\n\n        Raises:\n            DerivaMLException: If an entity doesn't exist or lacks a RID.\n\n        Examples:\n            Using a RID string:\n                &gt;&gt;&gt; url = ml.cite(\"1-abc123\")\n                &gt;&gt;&gt; print(url)\n                'https://deriva.org/id/1/1-abc123@2024-01-01T12:00:00'\n\n            Using a dictionary:\n                &gt;&gt;&gt; url = ml.cite({\"RID\": \"1-abc123\"})\n        \"\"\"\n        # Return if already a citation URL\n        if isinstance(entity, str) and entity.startswith(f\"https://{self.host_name}/id/{self.catalog_id}/\"):\n            return entity\n\n        try:\n            # Resolve RID and create citation URL with snapshot time\n            self.resolve_rid(rid := entity if isinstance(entity, str) else entity[\"RID\"])\n            return f\"https://{self.host_name}/id/{self.catalog_id}/{rid}@{self.catalog.latest_snapshot().snaptime}\"\n        except KeyError as e:\n            raise DerivaMLException(f\"Entity {e} does not have RID column\")\n        except DerivaMLException as _e:\n            raise DerivaMLException(\"Entity RID does not exist\")\n\n    def user_list(self) -&gt; List[Dict[str, str]]:\n        \"\"\"Returns catalog user list.\n\n        Retrieves basic information about all users who have access to the catalog, including their\n        identifiers and full names.\n\n        Returns:\n            List[Dict[str, str]]: List of user information dictionaries, each containing:\n                - 'ID': User identifier\n                - 'Full_Name': User's full name\n\n        Examples:\n\n            &gt;&gt;&gt; users = ml.user_list()\n            &gt;&gt;&gt; for user in users:\n            ...     print(f\"{user['Full_Name']} ({user['ID']})\")\n        \"\"\"\n        # Get the user table path and fetch basic user info\n        user_path = self.pathBuilder.public.ERMrest_Client.path\n        return [{\"ID\": u[\"ID\"], \"Full_Name\": u[\"Full_Name\"]} for u in user_path.entities().fetch()]\n\n    def resolve_rid(self, rid: RID) -&gt; ResolveRidResult:\n        \"\"\"Resolves RID to catalog location.\n\n        Looks up a RID and returns information about where it exists in the catalog, including schema,\n        table, and column metadata.\n\n        Args:\n            rid: Resource Identifier to resolve.\n\n        Returns:\n            ResolveRidResult: Named tuple containing:\n                - schema: Schema name\n                - table: Table name\n                - columns: Column definitions\n                - datapath: Path builder for accessing the entity\n\n        Raises:\n            DerivaMLException: If RID doesn't exist in catalog.\n\n        Examples:\n            &gt;&gt;&gt; result = ml.resolve_rid(\"1-abc123\")\n            &gt;&gt;&gt; print(f\"Found in {result.schema}.{result.table}\")\n            &gt;&gt;&gt; data = result.datapath.entities().fetch()\n        \"\"\"\n        try:\n            # Attempt to resolve RID using catalog model\n            return self.catalog.resolve_rid(rid, self.model.model)\n        except KeyError as _e:\n            raise DerivaMLException(f\"Invalid RID {rid}\")\n\n    def retrieve_rid(self, rid: RID) -&gt; dict[str, Any]:\n        \"\"\"Retrieves complete record for RID.\n\n        Fetches all column values for the entity identified by the RID.\n\n        Args:\n            rid: Resource Identifier of the record to retrieve.\n\n        Returns:\n            dict[str, Any]: Dictionary containing all column values for the entity.\n\n        Raises:\n            DerivaMLException: If the RID doesn't exist in the catalog.\n\n        Example:\n            &gt;&gt;&gt; record = ml.retrieve_rid(\"1-abc123\")\n            &gt;&gt;&gt; print(f\"Name: {record['name']}, Created: {record['creation_date']}\")\n        \"\"\"\n        # Resolve RID and fetch the first (only) matching record\n        return self.resolve_rid(rid).datapath.entities().fetch()[0]\n\n    def add_page(self, title: str, content: str) -&gt; None:\n        \"\"\"Adds page to web interface.\n\n        Creates a new page in the catalog's web interface with the specified title and content. The page will be\n        accessible through the catalog's navigation system.\n\n        Args:\n            title: The title of the page to be displayed in navigation and headers.\n            content: The main content of the page can include HTML markup.\n\n        Raises:\n            DerivaMLException: If the page creation fails or the user lacks necessary permissions.\n\n        Example:\n            &gt;&gt;&gt; ml.add_page(\n            ...     title=\"Analysis Results\",\n            ...     content=\"&lt;h1&gt;Results&lt;/h1&gt;&lt;p&gt;Analysis completed successfully...&lt;/p&gt;\"\n            ... )\n        \"\"\"\n        # Insert page into www tables with title and content\n        self.pathBuilder.www.tables[self.domain_schema].insert([{\"Title\": title, \"Content\": content}])\n\n    def create_vocabulary(self, vocab_name: str, comment: str = \"\", schema: str | None = None) -&gt; Table:\n        \"\"\"Creates a controlled vocabulary table.\n\n        A controlled vocabulary table maintains a list of standardized terms and their definitions. Each term can have\n        synonyms and descriptions to ensure consistent terminology usage across the dataset.\n\n        Args:\n            vocab_name: Name for the new vocabulary table. Must be a valid SQL identifier.\n            comment: Description of the vocabulary's purpose and usage. Defaults to empty string.\n            schema: Schema name to create the table in. If None, uses domain_schema.\n\n        Returns:\n            Table: ERMRest table object representing the newly created vocabulary table.\n\n        Raises:\n            DerivaMLException: If vocab_name is invalid or already exists.\n\n        Examples:\n            Create a vocabulary for tissue types:\n\n                &gt;&gt;&gt; table = ml.create_vocabulary(\n                ...     vocab_name=\"tissue_types\",\n                ...     comment=\"Standard tissue classifications\",\n                ...     schema=\"bio_schema\"\n                ... )\n        \"\"\"\n        # Use domain schema if none specified\n        schema = schema or self.domain_schema\n\n        # Create and return vocabulary table with RID-based URI pattern\n        try:\n            vocab_table = self.model.schemas[schema].create_table(\n                Table.define_vocabulary(vocab_name, f\"{self.project_name}:{{RID}}\", comment=comment)\n            )\n        except ValueError:\n            raise DerivaMLException(f\"Table {vocab_name} already exist\")\n        return vocab_table\n\n    def create_table(self, table: TableDefinition) -&gt; Table:\n        \"\"\"Creates a new table in the catalog.\n\n        Creates a table using the provided TableDefinition object, which specifies the table structure including\n        columns, keys, and foreign key relationships.\n\n        Args:\n            table: A TableDefinition object containing the complete specification of the table to create.\n\n        Returns:\n            Table: The newly created ERMRest table object.\n\n        Raises:\n            DerivaMLException: If table creation fails or the definition is invalid.\n\n        Example:\n\n            &gt;&gt;&gt; table_def = TableDefinition(\n            ...     name=\"experiments\",\n            ...     column_definitions=[\n            ...         ColumnDefinition(name=\"name\", type=BuiltinTypes.text),\n            ...         ColumnDefinition(name=\"date\", type=BuiltinTypes.date)\n            ...     ]\n            ... )\n            &gt;&gt;&gt; new_table = ml.create_table(table_def)\n        \"\"\"\n        # Create table in domain schema using provided definition\n        return self.model.schemas[self.domain_schema].create_table(table.model_dump())\n\n    @validate_call(config=ConfigDict(arbitrary_types_allowed=True))\n    def create_asset(\n        self,\n        asset_name: str,\n        column_defs: Iterable[ColumnDefinition] | None = None,\n        fkey_defs: Iterable[ColumnDefinition] | None = None,\n        referenced_tables: Iterable[Table] | None = None,\n        comment: str = \"\",\n        schema: str | None = None,\n    ) -&gt; Table:\n        \"\"\"Creates an asset table.\n\n        Args:\n            asset_name: Name of the asset table.\n            column_defs: Iterable of ColumnDefinition objects to provide additional metadata for asset.\n            fkey_defs: Iterable of ForeignKeyDefinition objects to provide additional metadata for asset.\n            referenced_tables: Iterable of Table objects to which asset should provide foreign-key references to.\n            comment: Description of the asset table. (Default value = '')\n            schema: Schema in which to create the asset table.  Defaults to domain_schema.\n\n        Returns:\n            Table object for the asset table.\n        \"\"\"\n        # Initialize empty collections if None provided\n        column_defs = column_defs or []\n        fkey_defs = fkey_defs or []\n        referenced_tables = referenced_tables or []\n        schema = schema or self.domain_schema\n\n        # Add an asset type to vocabulary\n        self.add_term(MLVocab.asset_type, asset_name, description=f\"A {asset_name} asset\")\n\n        # Create the main asset table\n        asset_table = self.model.schemas[schema].create_table(\n            Table.define_asset(\n                schema,\n                asset_name,\n                column_defs=[c.model_dump() for c in column_defs],\n                fkey_defs=[fk.model_dump() for fk in fkey_defs],\n                comment=comment,\n            )\n        )\n\n        # Create an association table between asset and asset type\n        self.model.schemas[self.domain_schema].create_table(\n            Table.define_association(\n                [\n                    (asset_table.name, asset_table),\n                    (\"Asset_Type\", self.model.name_to_table(\"Asset_Type\")),\n                ]\n            )\n        )\n\n        # Create references to other tables if specified\n        for t in referenced_tables:\n            asset_table.create_reference(self.model.name_to_table(t))\n\n        # Create an association table for tracking execution\n        atable = self.model.schemas[self.domain_schema].create_table(\n            Table.define_association(\n                [\n                    (asset_name, asset_table),\n                    (\n                        \"Execution\",\n                        self.model.schemas[self.ml_schema].tables[\"Execution\"],\n                    ),\n                ]\n            )\n        )\n        atable.create_reference(self.model.name_to_table(\"Asset_Role\"))\n\n        # Add asset annotations\n        asset_annotation(asset_table)\n        return asset_table\n\n    def list_assets(self, asset_table: Table | str) -&gt; list[dict[str, Any]]:\n        \"\"\"Lists contents of an asset table.\n\n        Returns a list of assets with their types for the specified asset table.\n\n        Args:\n            asset_table: Table or name of the asset table to list assets for.\n\n        Returns:\n            list[dict[str, Any]]: List of asset records, each containing:\n                - RID: Resource identifier\n                - Type: Asset type\n                - Metadata: Asset metadata\n\n        Raises:\n            DerivaMLException: If the table is not an asset table or doesn't exist.\n\n        Example:\n            &gt;&gt;&gt; assets = ml.list_assets(\"tissue_types\")\n            &gt;&gt;&gt; for asset in assets:\n            ...     print(f\"{asset['RID']}: {asset['Type']}\")\n        \"\"\"\n        # Validate and get asset table reference\n        asset_table = self.model.name_to_table(asset_table)\n        if not self.model.is_asset(asset_table):\n            raise DerivaMLException(f\"Table {asset_table.name} is not an asset\")\n\n        # Get path builders for asset and type tables\n        pb = self._model.catalog.getPathBuilder()\n        asset_path = pb.schemas[asset_table.schema.name].tables[asset_table.name]\n        (\n            asset_type_table,\n            _,\n            _,\n        ) = self._model.find_association(asset_table, MLVocab.asset_type)\n        type_path = pb.schemas[asset_type_table.schema.name].tables[asset_type_table.name]\n\n        # Build a list of assets with their types\n        assets = []\n        for asset in asset_path.entities().fetch():\n            # Get associated asset types for each asset\n            asset_types = (\n                type_path.filter(type_path.columns[asset_table.name] == asset[\"RID\"])\n                .attributes(type_path.Asset_Type)\n                .fetch()\n            )\n            # Combine asset data with its types\n            assets.append(\n                asset | {MLVocab.asset_type.value: [asset_type[MLVocab.asset_type.value] for asset_type in asset_types]}\n            )\n        return assets\n\n    @validate_call(config=ConfigDict(arbitrary_types_allowed=True))\n    def create_feature(\n        self,\n        target_table: Table | str,\n        feature_name: str,\n        terms: list[Table | str] | None = None,\n        assets: list[Table | str] | None = None,\n        metadata: list[ColumnDefinition | Table | Key | str] | None = None,\n        optional: list[str] | None = None,\n        comment: str = \"\",\n    ) -&gt; type[FeatureRecord]:\n        \"\"\"Creates a new feature definition.\n\n        A feature represents a measurable property or characteristic that can be associated with records in the target\n        table. Features can include vocabulary terms, asset references, and additional metadata.\n\n        Args:\n            target_table: Table to associate the feature with (name or Table object).\n            feature_name: Unique name for the feature within the target table.\n            terms: Optional vocabulary tables/names whose terms can be used as feature values.\n            assets: Optional asset tables/names that can be referenced by this feature.\n            metadata: Optional columns, tables, or keys to include in a feature definition.\n            optional: Column names that are not required when creating feature instances.\n            comment: Description of the feature's purpose and usage.\n\n        Returns:\n            type[FeatureRecord]: Feature class for creating validated instances.\n\n        Raises:\n            DerivaMLException: If a feature definition is invalid or conflicts with existing features.\n\n        Examples:\n            Create a feature with confidence score:\n                &gt;&gt;&gt; feature_class = ml.create_feature(\n                ...     target_table=\"samples\",\n                ...     feature_name=\"expression_level\",\n                ...     terms=[\"expression_values\"],\n                ...     metadata=[ColumnDefinition(name=\"confidence\", type=BuiltinTypes.float4)],\n                ...     comment=\"Gene expression measurement\"\n                ... )\n        \"\"\"\n        # Initialize empty collections if None provided\n        terms = terms or []\n        assets = assets or []\n        metadata = metadata or []\n        optional = optional or []\n\n        def normalize_metadata(m: Key | Table | ColumnDefinition | str):\n            \"\"\"Helper function to normalize metadata references.\"\"\"\n            if isinstance(m, str):\n                return self.model.name_to_table(m)\n            elif isinstance(m, ColumnDefinition):\n                return m.model_dump()\n            else:\n                return m\n\n        # Validate asset and term tables\n        if not all(map(self.model.is_asset, assets)):\n            raise DerivaMLException(\"Invalid create_feature asset table.\")\n        if not all(map(self.model.is_vocabulary, terms)):\n            raise DerivaMLException(\"Invalid create_feature asset table.\")\n\n        # Get references to required tables\n        target_table = self.model.name_to_table(target_table)\n        execution = self.model.schemas[self.ml_schema].tables[\"Execution\"]\n        feature_name_table = self.model.schemas[self.ml_schema].tables[\"Feature_Name\"]\n\n        # Add feature name to vocabulary\n        feature_name_term = self.add_term(\"Feature_Name\", feature_name, description=comment)\n        atable_name = f\"Execution_{target_table.name}_{feature_name_term.name}\"\n        # Create an association table implementing the feature\n        atable = self.model.schemas[self.domain_schema].create_table(\n            target_table.define_association(\n                table_name=atable_name,\n                associates=[execution, target_table, feature_name_table],\n                metadata=[normalize_metadata(m) for m in chain(assets, terms, metadata)],\n                comment=comment,\n            )\n        )\n        # Configure optional columns and default feature name\n        for c in optional:\n            atable.columns[c].alter(nullok=True)\n        atable.columns[\"Feature_Name\"].alter(default=feature_name_term.name)\n\n        # Return feature record class for creating instances\n        return self.feature_record_class(target_table, feature_name)\n\n    def feature_record_class(self, table: str | Table, feature_name: str) -&gt; type[FeatureRecord]:\n        \"\"\"Returns a pydantic model class for feature records.\n\n        Creates a typed interface for creating new instances of the specified feature. The returned class includes\n        validation and type checking based on the feature's definition.\n\n        Args:\n            table: The table containing the feature, either as name or Table object.\n            feature_name: Name of the feature to create a record class for.\n\n        Returns:\n            type[FeatureRecord]: A pydantic model class for creating validated feature records.\n\n        Raises:\n            DerivaMLException: If the feature doesn't exist or the table is invalid.\n\n        Example:\n            &gt;&gt;&gt; ExpressionFeature = ml.feature_record_class(\"samples\", \"expression_level\")\n            &gt;&gt;&gt; feature = ExpressionFeature(value=\"high\", confidence=0.95)\n        \"\"\"\n        # Look up a feature and return its record class\n        return self.lookup_feature(table, feature_name).feature_record_class()\n\n    def delete_feature(self, table: Table | str, feature_name: str) -&gt; bool:\n        \"\"\"Removes a feature definition and its data.\n\n        Deletes the feature and its implementation table from the catalog. This operation cannot be undone and\n        will remove all feature values associated with this feature.\n\n        Args:\n            table: The table containing the feature, either as name or Table object.\n            feature_name: Name of the feature to delete.\n\n        Returns:\n            bool: True if the feature was successfully deleted, False if it didn't exist.\n\n        Raises:\n            DerivaMLException: If deletion fails due to constraints or permissions.\n\n        Example:\n            &gt;&gt;&gt; success = ml.delete_feature(\"samples\", \"obsolete_feature\")\n            &gt;&gt;&gt; print(\"Deleted\" if success else \"Not found\")\n        \"\"\"\n        # Get table reference and find feature\n        table = self.model.name_to_table(table)\n        try:\n            # Find and delete the feature's implementation table\n            feature = next(f for f in self.model.find_features(table) if f.feature_name == feature_name)\n            feature.feature_table.drop()\n            return True\n        except StopIteration:\n            return False\n\n    def lookup_feature(self, table: str | Table, feature_name: str) -&gt; Feature:\n        \"\"\"Retrieves a Feature object.\n\n        Looks up and returns a Feature object that provides an interface to work with an existing feature\n        definition in the catalog.\n\n        Args:\n            table: The table containing the feature, either as name or Table object.\n            feature_name: Name of the feature to look up.\n\n        Returns:\n            Feature: An object representing the feature and its implementation.\n\n        Raises:\n            DerivaMLException: If the feature doesn't exist in the specified table.\n\n        Example:\n            &gt;&gt;&gt; feature = ml.lookup_feature(\"samples\", \"expression_level\")\n            &gt;&gt;&gt; print(feature.feature_name)\n            'expression_level'\n        \"\"\"\n        return self.model.lookup_feature(table, feature_name)\n\n    @validate_call(config=ConfigDict(arbitrary_types_allowed=True))\n    def list_feature_values(self, table: Table | str, feature_name: str) -&gt; datapath._ResultSet:\n        \"\"\"Retrieves all values for a feature.\n\n        Returns all instances of the specified feature that have been created, including their associated\n        metadata and references.\n\n        Args:\n            table: The table containing the feature, either as name or Table object.\n            feature_name: Name of the feature to retrieve values for.\n\n        Returns:\n            datapath._ResultSet: A result set containing all feature values and their metadata.\n\n        Raises:\n            DerivaMLException: If the feature doesn't exist or cannot be accessed.\n\n        Example:\n            &gt;&gt;&gt; values = ml.list_feature_values(\"samples\", \"expression_level\")\n            &gt;&gt;&gt; for value in values:\n            ...     print(f\"Sample {value['RID']}: {value['value']}\")\n        \"\"\"\n        # Get table and feature references\n        table = self.model.name_to_table(table)\n        feature = self.lookup_feature(table, feature_name)\n\n        # Build and execute query for feature values\n        pb = self.catalog.getPathBuilder()\n        return pb.schemas[feature.feature_table.schema.name].tables[feature.feature_table.name].entities().fetch()\n\n    @validate_call(config=ConfigDict(arbitrary_types_allowed=True))\n    def add_term(\n        self,\n        table: str | Table,\n        term_name: str,\n        description: str,\n        synonyms: list[str] | None = None,\n        exists_ok: bool = True,\n    ) -&gt; VocabularyTerm:\n        \"\"\"Adds a term to a vocabulary table.\n\n        Creates a new standardized term with description and optional synonyms in a vocabulary table.\n        Can either create a new term or return an existing one if it already exists.\n\n        Args:\n            table: Vocabulary table to add term to (name or Table object).\n            term_name: Primary name of the term (must be unique within vocabulary).\n            description: Explanation of term's meaning and usage.\n            synonyms: Alternative names for the term.\n            exists_ok: If True, return the existing term if found. If False, raise error.\n\n        Returns:\n            VocabularyTerm: Object representing the created or existing term.\n\n        Raises:\n            DerivaMLException: If a term exists and exists_ok=False, or if the table is not a vocabulary table.\n\n        Examples:\n            Add a new tissue type:\n                &gt;&gt;&gt; term = ml.add_term(\n                ...     table=\"tissue_types\",\n                ...     term_name=\"epithelial\",\n                ...     description=\"Epithelial tissue type\",\n                ...     synonyms=[\"epithelium\"]\n                ... )\n\n            Attempt to add an existing term:\n                &gt;&gt;&gt; term = ml.add_term(\"tissue_types\", \"epithelial\", \"...\", exists_ok=True)\n        \"\"\"\n        # Initialize an empty synonyms list if None\n        synonyms = synonyms or []\n\n        # Get table reference and validate if it is a vocabulary table\n        table = self.model.name_to_table(table)\n        pb = self.catalog.getPathBuilder()\n        if not (self.model.is_vocabulary(table)):\n            raise DerivaMLTableTypeError(\"vocabulary\", table.name)\n\n        # Get schema and table names for path building\n        schema_name = table.schema.name\n        table_name = table.name\n\n        try:\n            # Attempt to insert a new term\n            term_id = VocabularyTerm.model_validate(\n                pb.schemas[schema_name]\n                .tables[table_name]\n                .insert(\n                    [\n                        {\n                            \"Name\": term_name,\n                            \"Description\": description,\n                            \"Synonyms\": synonyms,\n                        }\n                    ],\n                    defaults={\"ID\", \"URI\"},\n                )[0]\n            )\n        except DataPathException:\n            # Term exists - look it up or raise an error\n            term_id = self.lookup_term(table, term_name)\n            if not exists_ok:\n                raise DerivaMLInvalidTerm(table.name, term_name, msg=\"term already exists\")\n        return term_id\n\n    @validate_call(config=ConfigDict(arbitrary_types_allowed=True))\n    def lookup_term(self, table: str | Table, term_name: str) -&gt; VocabularyTerm:\n        \"\"\"Finds a term in a vocabulary table.\n\n        Searches for a term in the specified vocabulary table, matching either the primary name\n        or any of its synonyms.\n\n        Args:\n            table: Vocabulary table to search in (name or Table object).\n            term_name: Name or synonym of the term to find.\n\n        Returns:\n            VocabularyTerm: The matching vocabulary term.\n\n        Raises:\n            DerivaMLVocabularyException: If the table is not a vocabulary table, or term is not found.\n\n        Examples:\n            Look up by primary name:\n                &gt;&gt;&gt; term = ml.lookup_term(\"tissue_types\", \"epithelial\")\n                &gt;&gt;&gt; print(term.description)\n\n            Look up by synonym:\n                &gt;&gt;&gt; term = ml.lookup_term(\"tissue_types\", \"epithelium\")\n        \"\"\"\n        # Get and validate vocabulary table reference\n        vocab_table = self.model.name_to_table(table)\n        if not self.model.is_vocabulary(vocab_table):\n            raise DerivaMLException(f\"The table {table} is not a controlled vocabulary\")\n\n        # Get schema and table paths\n        schema_name, table_name = vocab_table.schema.name, vocab_table.name\n        schema_path = self.catalog.getPathBuilder().schemas[schema_name]\n\n        # Search for term by name or synonym\n        for term in schema_path.tables[table_name].entities().fetch():\n            if term_name == term[\"Name\"] or (term[\"Synonyms\"] and term_name in term[\"Synonyms\"]):\n                return VocabularyTerm.model_validate(term)\n\n        # Term not found\n        raise DerivaMLInvalidTerm(table_name, term_name)\n\n    def list_vocabulary_terms(self, table: str | Table) -&gt; list[VocabularyTerm]:\n        \"\"\"Lists all terms in a vocabulary table.\n\n        Retrieves all terms, their descriptions, and synonyms from a controlled vocabulary table.\n\n        Args:\n            table: Vocabulary table to list terms from (name or Table object).\n\n        Returns:\n            list[VocabularyTerm]: List of vocabulary terms with their metadata.\n\n        Raises:\n            DerivaMLException: If table doesn't exist or is not a vocabulary table.\n\n        Examples:\n            &gt;&gt;&gt; terms = ml.list_vocabulary_terms(\"tissue_types\")\n            &gt;&gt;&gt; for term in terms:\n            ...     print(f\"{term.name}: {term.description}\")\n            ...     if term.synonyms:\n            ...         print(f\"  Synonyms: {', '.join(term.synonyms)}\")\n        \"\"\"\n        # Get path builder and table reference\n        pb = self.catalog.getPathBuilder()\n        table = self.model.name_to_table(table.value if isinstance(table, MLVocab) else table)\n\n        # Validate table is a vocabulary table\n        if not (self.model.is_vocabulary(table)):\n            raise DerivaMLException(f\"The table {table} is not a controlled vocabulary\")\n\n        # Fetch and convert all terms to VocabularyTerm objects\n        return [VocabularyTerm(**v) for v in pb.schemas[table.schema.name].tables[table.name].entities().fetch()]\n\n    @validate_call(config=ConfigDict(arbitrary_types_allowed=True))\n    def download_dataset_bag(\n        self,\n        dataset: DatasetSpec,\n        execution_rid: RID | None = None,\n    ) -&gt; DatasetBag:\n        \"\"\"Downloads a dataset to the local filesystem and creates a MINID if needed.\n\n        Downloads a dataset specified by DatasetSpec to the local filesystem. If the dataset doesn't have\n        a MINID (Minimal Viable Identifier), one will be created. The dataset can optionally be associated\n        with an execution record.\n\n        Args:\n            dataset: Specification of the dataset to download, including version and materialization options.\n            execution_rid: Optional execution RID to associate the download with.\n\n        Returns:\n            DatasetBag: Object containing:\n                - path: Local filesystem path to downloaded dataset\n                - rid: Dataset's Resource Identifier\n                - minid: Dataset's Minimal Viable Identifier\n\n        Examples:\n            Download with default options:\n                &gt;&gt;&gt; spec = DatasetSpec(rid=\"1-abc123\")\n                &gt;&gt;&gt; bag = ml.download_dataset_bag(dataset=spec)\n                &gt;&gt;&gt; print(f\"Downloaded to {bag.path}\")\n\n            Download with execution tracking:\n                &gt;&gt;&gt; bag = ml.download_dataset_bag(\n                ...     dataset=DatasetSpec(rid=\"1-abc123\", materialize=True),\n                ...     execution_rid=\"1-xyz789\"\n                ... )\n        \"\"\"\n        if not self._is_dataset_rid(dataset.rid):\n            raise DerivaMLTableTypeError(\"Dataset\", dataset.rid)\n        return self._download_dataset_bag(\n            dataset=dataset,\n            execution_rid=execution_rid,\n            snapshot_catalog=DerivaML(self.host_name, self._version_snapshot(dataset)),\n        )\n\n    def _update_status(self, new_status: Status, status_detail: str, execution_rid: RID):\n        \"\"\"Update the status of an execution in the catalog.\n\n        Args:\n            new_status: New status.\n            status_detail: Details of the status.\n            execution_rid: Resource Identifier (RID) of the execution.\n            new_status: Status:\n            status_detail: str:\n             execution_rid: RID:\n\n        Returns:\n\n        \"\"\"\n        self.status = new_status.value\n        self.pathBuilder.schemas[self.ml_schema].Execution.update(\n            [\n                {\n                    \"RID\": execution_rid,\n                    \"Status\": self.status,\n                    \"Status_Detail\": status_detail,\n                }\n            ]\n        )\n\n    @validate_call(config=ConfigDict(arbitrary_types_allowed=True))\n    def add_files(\n        self,\n        files: Iterable[FileSpec],\n        dataset_types: str | list[str] | None = None,\n        description: str = \"\",\n        execution_rid: RID | None = None,\n    ) -&gt; RID:\n        \"\"\"Adds files to the catalog with their metadata.\n\n        Registers files in the catalog along with their metadata (MD5, length, URL) and associates them with\n        specified file types. Optionally links files to an execution record.\n\n        Args:\n            files: File specifications containing MD5 checksum, length, and URL.\n            dataset_types: One or more dataset type terms from File_Type vocabulary.\n            description: Description of the files.\n            execution_rid: Optional execution RID to associate files with.\n\n        Returns:\n            RID: Resource of dataset that represents the newly added files.\n\n        Raises:\n            DerivaMLException: If file_types are invalid or execution_rid is not an execution record.\n\n        Examples:\n            Add a single file type:\n                &gt;&gt;&gt; files = [FileSpec(url=\"path/to/file.txt\", md5=\"abc123\", length=1000)]\n                &gt;&gt;&gt; rids = ml.add_files(files, file_types=\"text\")\n\n            Add multiple file types:\n                &gt;&gt;&gt; rids = ml.add_files(\n                ...     files=[FileSpec(url=\"image.png\", md5=\"def456\", length=2000)],\n                ...     file_types=[\"image\", \"png\"],\n                ...     execution_rid=\"1-xyz789\"\n                ... )\n        \"\"\"\n        if execution_rid and self.resolve_rid(execution_rid).table.name != \"Execution\":\n            raise DerivaMLTableTypeError(\"Execution\", execution_rid)\n\n        filespec_list = list(files)\n\n        # Get a list of all defined file types and their synonyms.\n        defined_types = set(\n            chain.from_iterable([[t.name] + t.synonyms for t in self.list_vocabulary_terms(MLVocab.asset_type)])\n        )\n\n        # Get a list of al of the file types used in the filespec_list\n        spec_types = set(chain.from_iterable(filespec.file_types for filespec in filespec_list))\n\n        # Now make sure that all of the file types and dataset_types in the spec list are defined.\n        if spec_types - defined_types:\n            raise DerivaMLInvalidTerm(MLVocab.asset_type.name, f\"{spec_types - defined_types}\")\n\n        # Normalize dataset_types, make sure FIle type is included.\n        if isinstance(dataset_types, list):\n            dataset_types = [\"File\"] + dataset_types if \"File\" not in dataset_types else dataset_types\n        else:\n            dataset_types = [\"File\", dataset_types] if dataset_types else [\"File\"]\n        for ds_type in dataset_types:\n            self.lookup_term(MLVocab.dataset_type, ds_type)\n\n        # Add files to the file table, and collect up the resulting entries by directory name.\n        pb = self._model.catalog.getPathBuilder()\n        file_records = list(\n            pb.schemas[self.ml_schema].tables[\"File\"].insert([f.model_dump(by_alias=True) for f in filespec_list])\n        )\n\n        # Get the name of the association table between file_table and file_type and add file_type records\n        atable = self.model.find_association(MLTable.file, MLVocab.asset_type)[0].name\n        # Need to get a link between file record and file_types.\n        type_map = {\n            file_spec.md5: file_spec.file_types + ([] if \"File\" in file_spec.file_types else [])\n            for file_spec in filespec_list\n        }\n        file_type_records = [\n            {MLVocab.asset_type.value: file_type, \"File\": file_record[\"RID\"]}\n            for file_record in file_records\n            for file_type in type_map[file_record[\"MD5\"]]\n        ]\n        pb.schemas[self._ml_schema].tables[atable].insert(file_type_records)\n\n        if execution_rid:\n            # Get the name of the association table between file_table and execution.\n            pb.schemas[self._ml_schema].File_Execution.insert(\n                [\n                    {\"File\": file_record[\"RID\"], \"Execution\": execution_rid, \"Asset_Role\": \"Output\"}\n                    for file_record in file_records\n                ]\n            )\n\n        # Now create datasets to capture the original directory structure of the files.\n        dir_rid_map = defaultdict(list)\n        for e in file_records:\n            dir_rid_map[Path(urlsplit(e[\"URL\"]).path).parent].append(e[\"RID\"])\n\n        nested_datasets = []\n        path_length = 0\n        dataset = None\n        # Start with the longest path so we get subdirectories first.\n        for p, rids in sorted(dir_rid_map.items(), key=lambda kv: len(kv[0].parts), reverse=True):\n            dataset = self.create_dataset(\n                dataset_types=dataset_types, execution_rid=execution_rid, description=description\n            )\n            members = rids\n            if len(p.parts) &lt; path_length:\n                # Going up one level in directory, so Create nested dataset\n                members = nested_datasets + rids\n                nested_datasets = []\n            self.add_dataset_members(dataset_rid=dataset, members=members, execution_rid=execution_rid)\n            nested_datasets.append(dataset)\n            path_length = len(p.parts)\n\n        return dataset\n\n    def list_files(self, file_types: list[str] | None = None) -&gt; list[dict[str, Any]]:\n        \"\"\"Lists files in the catalog with their metadata.\n\n        Returns a list of files with their metadata including URL, MD5 hash, length, description,\n        and associated file types. Files can be optionally filtered by type.\n\n        Args:\n            file_types: Filter results to only include these file types.\n\n        Returns:\n            list[dict[str, Any]]: List of file records, each containing:\n                - RID: Resource identifier\n                - URL: File location\n                - MD5: File hash\n                - Length: File size\n                - Description: File description\n                - File_Types: List of associated file types\n\n        Examples:\n            List all files:\n                &gt;&gt;&gt; files = ml.list_files()\n                &gt;&gt;&gt; for f in files:\n                ...     print(f\"{f['RID']}: {f['URL']}\")\n\n            Filter by file type:\n                &gt;&gt;&gt; image_files = ml.list_files([\"image\", \"png\"])\n        \"\"\"\n\n        asset_type_atable, file_fk, asset_type_fk = self.model.find_association(\"File\", \"Asset_Type\")\n        ml_path = self.pathBuilder.schemas[self._ml_schema]\n        file = ml_path.File\n        asset_type = ml_path.tables[asset_type_atable.name]\n\n        path = file.path\n        path = path.link(asset_type.alias(\"AT\"), on=file.RID == asset_type.columns[file_fk], join_type=\"left\")\n        if file_types:\n            path = path.filter(asset_type.columns[asset_type_fk] == datapath.Any(*file_types))\n        path = path.attributes(\n            path.File.RID,\n            path.File.URL,\n            path.File.MD5,\n            path.File.Length,\n            path.File.Description,\n            path.AT.columns[asset_type_fk],\n        )\n\n        file_map = {}\n        for f in path.fetch():\n            entry = file_map.setdefault(f[\"RID\"], {**f, \"File_Types\": []})\n            if ft := f.get(\"Asset_Type\"):  # assign-and-test in one go\n                entry[\"File_Types\"].append(ft)\n\n        # Now get rid of the File_Type key and return the result\n        return [(f, f.pop(\"Asset_Type\"))[0] for f in file_map.values()]\n\n    def list_workflows(self) -&gt; list[Workflow]:\n        \"\"\"Lists all workflows in the catalog.\n\n        Retrieves all workflow definitions, including their names, URLs, types, versions,\n        and descriptions.\n\n        Returns:\n            list[Workflow]: List of workflow objects, each containing:\n                - name: Workflow name\n                - url: Source code URL\n                - workflow_type: Type of workflow\n                - version: Version identifier\n                - description: Workflow description\n                - rid: Resource identifier\n                - checksum: Source code checksum\n\n        Examples:\n            &gt;&gt;&gt; workflows = ml.list_workflows()\n            &gt;&gt;&gt; for w in workflows:\n                    print(f\"{w.name} (v{w.version}): {w.description}\")\n                    print(f\"  Source: {w.url}\")\n        \"\"\"\n        # Get a workflow table path and fetch all workflows\n        workflow_path = self.pathBuilder.schemas[self.ml_schema].Workflow\n        return [\n            Workflow(\n                name=w[\"Name\"],\n                url=w[\"URL\"],\n                workflow_type=w[\"Workflow_Type\"],\n                version=w[\"Version\"],\n                description=w[\"Description\"],\n                rid=w[\"RID\"],\n                checksum=w[\"Checksum\"],\n            )\n            for w in workflow_path.entities().fetch()\n        ]\n\n    def add_workflow(self, workflow: Workflow) -&gt; RID:\n        \"\"\"Adds a workflow to the catalog.\n\n        Registers a new workflow in the catalog or returns the RID of an existing workflow with the same\n        URL or checksum.\n\n        Each workflow represents a specific computational process or analysis pipeline.\n\n        Args:\n            workflow: Workflow object containing name, URL, type, version, and description.\n\n        Returns:\n            RID: Resource Identifier of the added or existing workflow.\n\n        Raises:\n            DerivaMLException: If workflow insertion fails or required fields are missing.\n\n        Examples:\n            &gt;&gt;&gt; workflow = Workflow(\n            ...     name=\"Gene Analysis\",\n            ...     url=\"https://github.com/org/repo/workflows/gene_analysis.py\",\n            ...     workflow_type=\"python_script\",\n            ...     version=\"1.0.0\",\n            ...     description=\"Analyzes gene expression patterns\"\n            ... )\n            &gt;&gt;&gt; workflow_rid = ml.add_workflow(workflow)\n        \"\"\"\n        # Check if a workflow already exists by URL\n        if workflow_rid := self.lookup_workflow(workflow.checksum or workflow.url):\n            return workflow_rid\n\n        # Get an ML schema path for the workflow table\n        ml_schema_path = self.pathBuilder.schemas[self.ml_schema]\n\n        try:\n            # Create a workflow record\n            workflow_record = {\n                \"URL\": workflow.url,\n                \"Name\": workflow.name,\n                \"Description\": workflow.description,\n                \"Checksum\": workflow.checksum,\n                \"Version\": workflow.version,\n                MLVocab.workflow_type: self.lookup_term(MLVocab.workflow_type, workflow.workflow_type).name,\n            }\n            # Insert a workflow and get its RID\n            workflow_rid = ml_schema_path.Workflow.insert([workflow_record])[0][\"RID\"]\n        except Exception as e:\n            error = format_exception(e)\n            raise DerivaMLException(f\"Failed to insert workflow. Error: {error}\")\n        return workflow_rid\n\n    def lookup_workflow(self, url_or_checksum: str) -&gt; RID | None:\n        \"\"\"Finds a workflow by URL.\n\n        Args:\n            url_or_checksum: URL or checksum of the workflow.\n        Returns:\n            RID: Resource Identifier of the workflow if found, None otherwise.\n\n        Example:\n            &gt;&gt;&gt; rid = ml.lookup_workflow(\"https://github.com/org/repo/workflow.py\")\n            &gt;&gt;&gt; if rid:\n            ...     print(f\"Found workflow: {rid}\")\n        \"\"\"\n        # Get a workflow table path\n        workflow_path = self.pathBuilder.schemas[self.ml_schema].Workflow\n        try:\n            # Search for workflow by URL\n            url_column = workflow_path.URL\n            checksum_column = workflow_path.Checksum\n            return list(\n                workflow_path.path.filter(\n                    (url_column == url_or_checksum) | (checksum_column == url_or_checksum)\n                ).entities()\n            )[0][\"RID\"]\n        except IndexError:\n            return None\n\n    def create_workflow(self, name: str, workflow_type: str, description: str = \"\") -&gt; Workflow:\n        \"\"\"Creates a new workflow definition.\n\n        Creates a Workflow object that represents a computational process or analysis pipeline. The workflow type\n        must be a term from the controlled vocabulary. This method is typically used to define new analysis\n        workflows before execution.\n\n        Args:\n            name: Name of the workflow.\n            workflow_type: Type of workflow (must exist in workflow_type vocabulary).\n            description: Description of what the workflow does.\n\n        Returns:\n            Workflow: New workflow object ready for registration.\n\n        Raises:\n            DerivaMLException: If workflow_type is not in the vocabulary.\n\n        Examples:\n            &gt;&gt;&gt; workflow = ml.create_workflow(\n            ...     name=\"RNA Analysis\",\n            ...     workflow_type=\"python_notebook\",\n            ...     description=\"RNA sequence analysis pipeline\"\n            ... )\n            &gt;&gt;&gt; rid = ml.add_workflow(workflow)\n        \"\"\"\n        # Validate workflow type exists in vocabulary\n        self.lookup_term(MLVocab.workflow_type, workflow_type)\n\n        # Create and return a new workflow object\n        return Workflow(name=name, workflow_type=workflow_type, description=description)\n\n    def create_execution(self, configuration: ExecutionConfiguration, dry_run: bool = False) -&gt; \"Execution\":\n        \"\"\"Creates an execution environment.\n\n        Given an execution configuration, initialize the local compute environment to prepare for executing an\n        ML or analytic routine.  This routine has a number of side effects.\n\n        1. The datasets specified in the configuration are downloaded and placed in the cache-dir. If a version is\n        not specified in the configuration, then a new minor version number is created for the dataset and downloaded.\n\n        2. If any execution assets are provided in the configuration, they are downloaded\n        and placed in the working directory.\n\n\n        Args:\n            configuration: ExecutionConfiguration:\n            dry_run: Do not create an execution record or upload results.\n\n        Returns:\n            An execution object.\n        \"\"\"\n        # Import here to avoid circular dependency\n        from deriva_ml.execution.execution import Execution\n\n        # Create and store an execution instance\n        self._execution = Execution(configuration, self, dry_run=dry_run)\n        return self._execution\n\n    def restore_execution(self, execution_rid: RID | None = None) -&gt; Execution:\n        \"\"\"Restores a previous execution.\n\n        Given an execution RID, retrieves the execution configuration and restores the local compute environment.\n        This routine has a number of side effects.\n\n        1. The datasets specified in the configuration are downloaded and placed in the cache-dir. If a version is\n        not specified in the configuration, then a new minor version number is created for the dataset and downloaded.\n\n        2. If any execution assets are provided in the configuration, they are downloaded and placed\n        in the working directory.\n\n        Args:\n            execution_rid: Resource Identifier (RID) of the execution to restore.\n\n        Returns:\n            Execution: An execution object representing the restored execution environment.\n\n        Raises:\n            DerivaMLException: If execution_rid is not valid or execution cannot be restored.\n\n        Example:\n            &gt;&gt;&gt; execution = ml.restore_execution(\"1-abc123\")\n        \"\"\"\n        # Import here to avoid circular dependency\n        from deriva_ml.execution.execution import Execution\n\n        # If no RID provided, try to find single execution in working directory\n        if not execution_rid:\n            e_rids = execution_rids(self.working_dir)\n            if len(e_rids) != 1:\n                raise DerivaMLException(f\"Multiple execution RIDs were found {e_rids}.\")\n            execution_rid = e_rids[0]\n\n        # Try to load configuration from a file\n        cfile = asset_file_path(\n            prefix=self.working_dir,\n            exec_rid=execution_rid,\n            file_name=\"configuration.json\",\n            asset_table=self.model.name_to_table(\"Execution_Metadata\"),\n            metadata={},\n        )\n\n        # Load configuration from a file or create from an execution record\n        if cfile.exists():\n            configuration = ExecutionConfiguration.load_configuration(cfile)\n        else:\n            execution = self.retrieve_rid(execution_rid)\n            configuration = ExecutionConfiguration(\n                workflow=execution[\"Workflow\"],\n                description=execution[\"Description\"],\n            )\n\n        # Create and return an execution instance\n        return Execution(configuration, self, reload=execution_rid)\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.domain_path","title":"domain_path  <code>property</code>","text":"<pre><code>domain_path: DataPath\n</code></pre> <p>Returns path builder for domain schema.</p> <p>Provides a convenient way to access tables and construct queries within the domain-specific schema.</p> <p>Returns:</p> Type Description <code>DataPath</code> <p>datapath._CatalogWrapper: Path builder object scoped to the domain schema.</p> Example <p>domain = ml.domain_path results = domain.my_table.entities().fetch()</p>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.pathBuilder","title":"pathBuilder  <code>property</code>","text":"<pre><code>pathBuilder: _SchemaWrapper\n</code></pre> <p>Returns catalog path builder for queries.</p> <p>The path builder provides a fluent interface for constructing complex queries against the catalog. This is a core component used by many other methods to interact with the catalog.</p> <p>Returns:</p> Type Description <code>_SchemaWrapper</code> <p>datapath._CatalogWrapper: A new instance of the catalog path builder.</p> Example <p>path = ml.pathBuilder.schemas['my_schema'].tables['my_table'] results = path.entities().fetch()</p>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.__del__","title":"__del__","text":"<pre><code>__del__()\n</code></pre> <p>Cleanup method to handle incomplete executions.</p> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>def __del__(self):\n    \"\"\"Cleanup method to handle incomplete executions.\"\"\"\n    try:\n        # Mark execution as aborted if not completed\n        if self._execution and self._execution.status != Status.completed:\n            self._execution.update_status(Status.aborted, \"Execution Aborted\")\n    except (AttributeError, requests.HTTPError):\n        pass\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.__init__","title":"__init__","text":"<pre><code>__init__(\n    hostname: str,\n    catalog_id: str | int,\n    domain_schema: str | None = None,\n    project_name: str | None = None,\n    cache_dir: str | Path | None = None,\n    working_dir: str\n    | Path\n    | None = None,\n    ml_schema: str = ML_SCHEMA,\n    logging_level=logging.WARNING,\n    credential=None,\n    use_minid: bool = True,\n    check_auth: bool = True,\n)\n</code></pre> <p>Initializes a DerivaML instance.</p> <p>This method will connect to a catalog and initialize local configuration for the ML execution. This class is intended to be used as a base class on which domain-specific interfaces are built.</p> <p>Parameters:</p> Name Type Description Default <code>hostname</code> <code>str</code> <p>Hostname of the Deriva server.</p> required <code>catalog_id</code> <code>str | int</code> <p>Catalog ID. Either an identifier or a catalog name.</p> required <code>domain_schema</code> <code>str | None</code> <p>Schema name for domain-specific tables and relationships. Defaults to the name of the schema that is not one of the standard schemas.  If there is more than one user-defined schema, then this argument must be provided a value.</p> <code>None</code> <code>ml_schema</code> <code>str</code> <p>Schema name for ML schema. Used if you have a non-standard configuration of deriva-ml.</p> <code>ML_SCHEMA</code> <code>project_name</code> <code>str | None</code> <p>Project name. Defaults to name of domain schema.</p> <code>None</code> <code>cache_dir</code> <code>str | Path | None</code> <p>Directory path for caching data downloaded from the Deriva server as bdbag. If not provided, will default to working_dir.</p> <code>None</code> <code>working_dir</code> <code>str | Path | None</code> <p>Directory path for storing data used by or generated by any computations. If no value is provided, will default to  ${HOME}/deriva_ml</p> <code>None</code> <code>use_minid</code> <code>bool</code> <p>Use the MINID service when downloading dataset bags.</p> <code>True</code> <code>check_auth</code> <code>bool</code> <p>Check if the user has access to the catalog.</p> <code>True</code> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>def __init__(\n    self,\n    hostname: str,\n    catalog_id: str | int,\n    domain_schema: str | None = None,\n    project_name: str | None = None,\n    cache_dir: str | Path | None = None,\n    working_dir: str | Path | None = None,\n    ml_schema: str = ML_SCHEMA,\n    logging_level=logging.WARNING,\n    credential=None,\n    use_minid: bool = True,\n    check_auth: bool = True,\n):\n    \"\"\"Initializes a DerivaML instance.\n\n    This method will connect to a catalog and initialize local configuration for the ML execution.\n    This class is intended to be used as a base class on which domain-specific interfaces are built.\n\n    Args:\n        hostname: Hostname of the Deriva server.\n        catalog_id: Catalog ID. Either an identifier or a catalog name.\n        domain_schema: Schema name for domain-specific tables and relationships. Defaults to the name of the\n            schema that is not one of the standard schemas.  If there is more than one user-defined schema, then\n            this argument must be provided a value.\n        ml_schema: Schema name for ML schema. Used if you have a non-standard configuration of deriva-ml.\n        project_name: Project name. Defaults to name of domain schema.\n        cache_dir: Directory path for caching data downloaded from the Deriva server as bdbag. If not provided,\n            will default to working_dir.\n        working_dir: Directory path for storing data used by or generated by any computations. If no value is\n            provided, will default to  ${HOME}/deriva_ml\n        use_minid: Use the MINID service when downloading dataset bags.\n        check_auth: Check if the user has access to the catalog.\n    \"\"\"\n    # Get or use provided credentials for server access\n    self.credential = credential or get_credential(hostname)\n\n    # Initialize server connection and catalog access\n    server = DerivaServer(\n        \"https\",\n        hostname,\n        credentials=self.credential,\n        session_config=self._get_session_config(),\n    )\n\n    try:\n        if check_auth and server.get_authn_session():\n            pass\n    except Exception:\n        raise DerivaMLException(\n            \"You are not authorized to access this catalog. \"\n            \"Please check your credentials and make sure you have logged in.\"\n        )\n\n    self.catalog = server.connect_ermrest(catalog_id)\n    self.model = DerivaModel(self.catalog.getCatalogModel(), domain_schema=domain_schema)\n\n    # Set up working and cache directories\n    default_workdir = self.__class__.__name__ + \"_working\"\n    self.working_dir = (\n        Path(working_dir) / getpass.getuser() if working_dir else Path.home() / \"deriva-ml\"\n    ) / default_workdir\n\n    self.working_dir.mkdir(parents=True, exist_ok=True)\n    self.cache_dir = Path(cache_dir) if cache_dir else self.working_dir / \"cache\"\n    self.cache_dir.mkdir(parents=True, exist_ok=True)\n\n    # Initialize dataset functionality from the parent class\n    super().__init__(self.model, self.cache_dir, self.working_dir, use_minid=use_minid)\n\n    # Set up logging\n    self._logger = logging.getLogger(\"deriva_ml\")\n    self._logger.setLevel(logging_level)\n\n    # Store instance configuration\n    self.host_name = hostname\n    self.catalog_id = catalog_id\n    self.ml_schema = ml_schema\n    self.configuration = None\n    self._execution: Execution | None = None\n    self.domain_schema = self.model.domain_schema\n    self.project_name = project_name or self.domain_schema\n    self.start_time = datetime.now()\n    self.status = Status.pending.value\n\n    # Configure logging format\n    logging.basicConfig(\n        level=logging_level,\n        format=\"%(asctime)s - %(name)s.%(levelname)s - %(message)s\",\n    )\n\n    # Set Deriva library logging level\n    deriva_logger = logging.getLogger(\"deriva\")\n    deriva_logger.setLevel(logging_level)\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.add_dataset_element_type","title":"add_dataset_element_type","text":"<pre><code>add_dataset_element_type(\n    element: str | Table,\n) -&gt; Table\n</code></pre> <p>A dataset_table is a heterogeneous collection of objects, each of which comes from a different table. This routine makes it possible to add objects from the specified table to a dataset_table.</p> <p>Parameters:</p> Name Type Description Default <code>element</code> <code>str | Table</code> <p>Name of the table or table object that is to be added to the dataset_table.</p> required <p>Returns:</p> Type Description <code>Table</code> <p>The table object that was added to the dataset_table.</p> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>@validate_call(config=ConfigDict(arbitrary_types_allowed=True))\ndef add_dataset_element_type(self, element: str | Table) -&gt; Table:\n    \"\"\"A dataset_table is a heterogeneous collection of objects, each of which comes from a different table. This\n    routine makes it possible to add objects from the specified table to a dataset_table.\n\n    Args:\n        element: Name of the table or table object that is to be added to the dataset_table.\n\n    Returns:\n        The table object that was added to the dataset_table.\n    \"\"\"\n    # Add table to map\n    element_table = self._model.name_to_table(element)\n    atable_def = Table.define_association([self._dataset_table, element_table])\n    try:\n        table = self._model.schemas[self._model.domain_schema].create_table(atable_def)\n    except ValueError as e:\n        if \"already exists\" in str(e):\n            table = self._model.name_to_table(atable_def[\"table_name\"])\n        else:\n            raise e\n\n    # self.model = self.catalog.getCatalogModel()\n    self._dataset_table.annotations.update(self._generate_dataset_download_annotations())\n    self._model.model.apply()\n    return table\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.add_dataset_members","title":"add_dataset_members","text":"<pre><code>add_dataset_members(\n    dataset_rid: RID,\n    members: list[RID]\n    | dict[str, list[RID]],\n    validate: bool = True,\n    description: str | None = \"\",\n    execution_rid: RID | None = None,\n) -&gt; None\n</code></pre> <p>Adds members to a dataset.</p> <p>Associates one or more records with a dataset. Can optionally validate member types and create a new dataset version to track the changes.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_rid</code> <code>RID</code> <p>Resource Identifier of the dataset.</p> required <code>members</code> <code>list[RID] | dict[str, list[RID]]</code> <p>List of RIDs to add as dataset members. Can be orginized into a dictionary that indicates the table that the member rids belong to.</p> required <code>validate</code> <code>bool</code> <p>Whether to validate member types. Defaults to True.</p> <code>True</code> <code>description</code> <code>str | None</code> <p>Optional description of the member additions.</p> <code>''</code> <code>execution_rid</code> <code>RID | None</code> <p>Optional execution RID to associate with changes.</p> <code>None</code> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If: - dataset_rid is invalid - members are invalid or of wrong type - adding members would create a cycle - validation fails</p> Example <p>ml.add_dataset_members( ...     dataset_rid=\"1-abc123\", ...     members=[\"1-def456\", \"1-ghi789\"], ...     description=\"Added sample data\" ... )</p> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>@validate_call\ndef add_dataset_members(\n    self,\n    dataset_rid: RID,\n    members: list[RID] | dict[str, list[RID]],\n    validate: bool = True,\n    description: str | None = \"\",\n    execution_rid: RID | None = None,\n) -&gt; None:\n    \"\"\"Adds members to a dataset.\n\n    Associates one or more records with a dataset. Can optionally validate member types\n    and create a new dataset version to track the changes.\n\n    Args:\n        dataset_rid: Resource Identifier of the dataset.\n        members: List of RIDs to add as dataset members. Can be orginized into a dictionary that indicates the\n            table that the member rids belong to.\n        validate: Whether to validate member types. Defaults to True.\n        description: Optional description of the member additions.\n        execution_rid: Optional execution RID to associate with changes.\n\n    Raises:\n        DerivaMLException: If:\n            - dataset_rid is invalid\n            - members are invalid or of wrong type\n            - adding members would create a cycle\n            - validation fails\n\n    Example:\n        &gt;&gt;&gt; ml.add_dataset_members(\n        ...     dataset_rid=\"1-abc123\",\n        ...     members=[\"1-def456\", \"1-ghi789\"],\n        ...     description=\"Added sample data\"\n        ... )\n    \"\"\"\n    description = description or \"Updated dataset via add_dataset_members\"\n\n    def check_dataset_cycle(member_rid, path=None):\n        \"\"\"\n\n        Args:\n          member_rid:\n          path: (Default value = None)\n\n        Returns:\n\n        \"\"\"\n        path = path or set(dataset_rid)\n        return member_rid in path\n\n    if validate:\n        existing_rids = set(m[\"RID\"] for ms in self.list_dataset_members(dataset_rid).values() for m in ms)\n        if overlap := set(existing_rids).intersection(members):\n            raise DerivaMLException(f\"Attempting to add existing member to dataset_table {dataset_rid}: {overlap}\")\n\n    # Now go through every rid to be added to the data set and sort them based on what association table entries\n    # need to be made.\n    dataset_elements = {}\n    association_map = {\n        a.other_fkeys.pop().pk_table.name: a.table.name for a in self._dataset_table.find_associations()\n    }\n\n    # Get a list of all the object types that can be linked to a dataset_table.\n    if type(members) is list:\n        members = set(members)\n        for m in members:\n            try:\n                rid_info = self._model.catalog.resolve_rid(m)\n            except KeyError:\n                raise DerivaMLException(f\"Invalid RID: {m}\")\n            if rid_info.table.name not in association_map:\n                raise DerivaMLException(f\"RID table: {rid_info.table.name} not part of dataset_table\")\n            if rid_info.table == self._dataset_table and check_dataset_cycle(rid_info.rid):\n                raise DerivaMLException(\"Creating cycle of datasets is not allowed\")\n            dataset_elements.setdefault(rid_info.table.name, []).append(rid_info.rid)\n    else:\n        dataset_elements = {t: set(ms) for t, ms in members.items()}\n    # Now make the entries into the association tables.\n    pb = self._model.catalog.getPathBuilder()\n    for table, elements in dataset_elements.items():\n        schema_path = pb.schemas[\n            self._ml_schema if (table == \"Dataset\" or table == \"File\") else self._model.domain_schema\n        ]\n        fk_column = \"Nested_Dataset\" if table == \"Dataset\" else table\n        if len(elements):\n            # Find out the name of the column in the association table.\n            schema_path.tables[association_map[table]].insert(\n                [{\"Dataset\": dataset_rid, fk_column: e} for e in elements]\n            )\n    self.increment_dataset_version(\n        dataset_rid,\n        VersionPart.minor,\n        description=description,\n        execution_rid=execution_rid,\n    )\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.add_files","title":"add_files","text":"<pre><code>add_files(\n    files: Iterable[FileSpec],\n    dataset_types: str\n    | list[str]\n    | None = None,\n    description: str = \"\",\n    execution_rid: RID | None = None,\n) -&gt; RID\n</code></pre> <p>Adds files to the catalog with their metadata.</p> <p>Registers files in the catalog along with their metadata (MD5, length, URL) and associates them with specified file types. Optionally links files to an execution record.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>Iterable[FileSpec]</code> <p>File specifications containing MD5 checksum, length, and URL.</p> required <code>dataset_types</code> <code>str | list[str] | None</code> <p>One or more dataset type terms from File_Type vocabulary.</p> <code>None</code> <code>description</code> <code>str</code> <p>Description of the files.</p> <code>''</code> <code>execution_rid</code> <code>RID | None</code> <p>Optional execution RID to associate files with.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>RID</code> <code>RID</code> <p>Resource of dataset that represents the newly added files.</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If file_types are invalid or execution_rid is not an execution record.</p> <p>Examples:</p> <p>Add a single file type:     &gt;&gt;&gt; files = [FileSpec(url=\"path/to/file.txt\", md5=\"abc123\", length=1000)]     &gt;&gt;&gt; rids = ml.add_files(files, file_types=\"text\")</p> <p>Add multiple file types:     &gt;&gt;&gt; rids = ml.add_files(     ...     files=[FileSpec(url=\"image.png\", md5=\"def456\", length=2000)],     ...     file_types=[\"image\", \"png\"],     ...     execution_rid=\"1-xyz789\"     ... )</p> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>@validate_call(config=ConfigDict(arbitrary_types_allowed=True))\ndef add_files(\n    self,\n    files: Iterable[FileSpec],\n    dataset_types: str | list[str] | None = None,\n    description: str = \"\",\n    execution_rid: RID | None = None,\n) -&gt; RID:\n    \"\"\"Adds files to the catalog with their metadata.\n\n    Registers files in the catalog along with their metadata (MD5, length, URL) and associates them with\n    specified file types. Optionally links files to an execution record.\n\n    Args:\n        files: File specifications containing MD5 checksum, length, and URL.\n        dataset_types: One or more dataset type terms from File_Type vocabulary.\n        description: Description of the files.\n        execution_rid: Optional execution RID to associate files with.\n\n    Returns:\n        RID: Resource of dataset that represents the newly added files.\n\n    Raises:\n        DerivaMLException: If file_types are invalid or execution_rid is not an execution record.\n\n    Examples:\n        Add a single file type:\n            &gt;&gt;&gt; files = [FileSpec(url=\"path/to/file.txt\", md5=\"abc123\", length=1000)]\n            &gt;&gt;&gt; rids = ml.add_files(files, file_types=\"text\")\n\n        Add multiple file types:\n            &gt;&gt;&gt; rids = ml.add_files(\n            ...     files=[FileSpec(url=\"image.png\", md5=\"def456\", length=2000)],\n            ...     file_types=[\"image\", \"png\"],\n            ...     execution_rid=\"1-xyz789\"\n            ... )\n    \"\"\"\n    if execution_rid and self.resolve_rid(execution_rid).table.name != \"Execution\":\n        raise DerivaMLTableTypeError(\"Execution\", execution_rid)\n\n    filespec_list = list(files)\n\n    # Get a list of all defined file types and their synonyms.\n    defined_types = set(\n        chain.from_iterable([[t.name] + t.synonyms for t in self.list_vocabulary_terms(MLVocab.asset_type)])\n    )\n\n    # Get a list of al of the file types used in the filespec_list\n    spec_types = set(chain.from_iterable(filespec.file_types for filespec in filespec_list))\n\n    # Now make sure that all of the file types and dataset_types in the spec list are defined.\n    if spec_types - defined_types:\n        raise DerivaMLInvalidTerm(MLVocab.asset_type.name, f\"{spec_types - defined_types}\")\n\n    # Normalize dataset_types, make sure FIle type is included.\n    if isinstance(dataset_types, list):\n        dataset_types = [\"File\"] + dataset_types if \"File\" not in dataset_types else dataset_types\n    else:\n        dataset_types = [\"File\", dataset_types] if dataset_types else [\"File\"]\n    for ds_type in dataset_types:\n        self.lookup_term(MLVocab.dataset_type, ds_type)\n\n    # Add files to the file table, and collect up the resulting entries by directory name.\n    pb = self._model.catalog.getPathBuilder()\n    file_records = list(\n        pb.schemas[self.ml_schema].tables[\"File\"].insert([f.model_dump(by_alias=True) for f in filespec_list])\n    )\n\n    # Get the name of the association table between file_table and file_type and add file_type records\n    atable = self.model.find_association(MLTable.file, MLVocab.asset_type)[0].name\n    # Need to get a link between file record and file_types.\n    type_map = {\n        file_spec.md5: file_spec.file_types + ([] if \"File\" in file_spec.file_types else [])\n        for file_spec in filespec_list\n    }\n    file_type_records = [\n        {MLVocab.asset_type.value: file_type, \"File\": file_record[\"RID\"]}\n        for file_record in file_records\n        for file_type in type_map[file_record[\"MD5\"]]\n    ]\n    pb.schemas[self._ml_schema].tables[atable].insert(file_type_records)\n\n    if execution_rid:\n        # Get the name of the association table between file_table and execution.\n        pb.schemas[self._ml_schema].File_Execution.insert(\n            [\n                {\"File\": file_record[\"RID\"], \"Execution\": execution_rid, \"Asset_Role\": \"Output\"}\n                for file_record in file_records\n            ]\n        )\n\n    # Now create datasets to capture the original directory structure of the files.\n    dir_rid_map = defaultdict(list)\n    for e in file_records:\n        dir_rid_map[Path(urlsplit(e[\"URL\"]).path).parent].append(e[\"RID\"])\n\n    nested_datasets = []\n    path_length = 0\n    dataset = None\n    # Start with the longest path so we get subdirectories first.\n    for p, rids in sorted(dir_rid_map.items(), key=lambda kv: len(kv[0].parts), reverse=True):\n        dataset = self.create_dataset(\n            dataset_types=dataset_types, execution_rid=execution_rid, description=description\n        )\n        members = rids\n        if len(p.parts) &lt; path_length:\n            # Going up one level in directory, so Create nested dataset\n            members = nested_datasets + rids\n            nested_datasets = []\n        self.add_dataset_members(dataset_rid=dataset, members=members, execution_rid=execution_rid)\n        nested_datasets.append(dataset)\n        path_length = len(p.parts)\n\n    return dataset\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.add_page","title":"add_page","text":"<pre><code>add_page(\n    title: str, content: str\n) -&gt; None\n</code></pre> <p>Adds page to web interface.</p> <p>Creates a new page in the catalog's web interface with the specified title and content. The page will be accessible through the catalog's navigation system.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>The title of the page to be displayed in navigation and headers.</p> required <code>content</code> <code>str</code> <p>The main content of the page can include HTML markup.</p> required <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If the page creation fails or the user lacks necessary permissions.</p> Example <p>ml.add_page( ...     title=\"Analysis Results\", ...     content=\"Results<p>Analysis completed successfully...</p>\" ... )</p> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>def add_page(self, title: str, content: str) -&gt; None:\n    \"\"\"Adds page to web interface.\n\n    Creates a new page in the catalog's web interface with the specified title and content. The page will be\n    accessible through the catalog's navigation system.\n\n    Args:\n        title: The title of the page to be displayed in navigation and headers.\n        content: The main content of the page can include HTML markup.\n\n    Raises:\n        DerivaMLException: If the page creation fails or the user lacks necessary permissions.\n\n    Example:\n        &gt;&gt;&gt; ml.add_page(\n        ...     title=\"Analysis Results\",\n        ...     content=\"&lt;h1&gt;Results&lt;/h1&gt;&lt;p&gt;Analysis completed successfully...&lt;/p&gt;\"\n        ... )\n    \"\"\"\n    # Insert page into www tables with title and content\n    self.pathBuilder.www.tables[self.domain_schema].insert([{\"Title\": title, \"Content\": content}])\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.add_term","title":"add_term","text":"<pre><code>add_term(\n    table: str | Table,\n    term_name: str,\n    description: str,\n    synonyms: list[str] | None = None,\n    exists_ok: bool = True,\n) -&gt; VocabularyTerm\n</code></pre> <p>Adds a term to a vocabulary table.</p> <p>Creates a new standardized term with description and optional synonyms in a vocabulary table. Can either create a new term or return an existing one if it already exists.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str | Table</code> <p>Vocabulary table to add term to (name or Table object).</p> required <code>term_name</code> <code>str</code> <p>Primary name of the term (must be unique within vocabulary).</p> required <code>description</code> <code>str</code> <p>Explanation of term's meaning and usage.</p> required <code>synonyms</code> <code>list[str] | None</code> <p>Alternative names for the term.</p> <code>None</code> <code>exists_ok</code> <code>bool</code> <p>If True, return the existing term if found. If False, raise error.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>VocabularyTerm</code> <code>VocabularyTerm</code> <p>Object representing the created or existing term.</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If a term exists and exists_ok=False, or if the table is not a vocabulary table.</p> <p>Examples:</p> <p>Add a new tissue type:     &gt;&gt;&gt; term = ml.add_term(     ...     table=\"tissue_types\",     ...     term_name=\"epithelial\",     ...     description=\"Epithelial tissue type\",     ...     synonyms=[\"epithelium\"]     ... )</p> <p>Attempt to add an existing term:     &gt;&gt;&gt; term = ml.add_term(\"tissue_types\", \"epithelial\", \"...\", exists_ok=True)</p> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>@validate_call(config=ConfigDict(arbitrary_types_allowed=True))\ndef add_term(\n    self,\n    table: str | Table,\n    term_name: str,\n    description: str,\n    synonyms: list[str] | None = None,\n    exists_ok: bool = True,\n) -&gt; VocabularyTerm:\n    \"\"\"Adds a term to a vocabulary table.\n\n    Creates a new standardized term with description and optional synonyms in a vocabulary table.\n    Can either create a new term or return an existing one if it already exists.\n\n    Args:\n        table: Vocabulary table to add term to (name or Table object).\n        term_name: Primary name of the term (must be unique within vocabulary).\n        description: Explanation of term's meaning and usage.\n        synonyms: Alternative names for the term.\n        exists_ok: If True, return the existing term if found. If False, raise error.\n\n    Returns:\n        VocabularyTerm: Object representing the created or existing term.\n\n    Raises:\n        DerivaMLException: If a term exists and exists_ok=False, or if the table is not a vocabulary table.\n\n    Examples:\n        Add a new tissue type:\n            &gt;&gt;&gt; term = ml.add_term(\n            ...     table=\"tissue_types\",\n            ...     term_name=\"epithelial\",\n            ...     description=\"Epithelial tissue type\",\n            ...     synonyms=[\"epithelium\"]\n            ... )\n\n        Attempt to add an existing term:\n            &gt;&gt;&gt; term = ml.add_term(\"tissue_types\", \"epithelial\", \"...\", exists_ok=True)\n    \"\"\"\n    # Initialize an empty synonyms list if None\n    synonyms = synonyms or []\n\n    # Get table reference and validate if it is a vocabulary table\n    table = self.model.name_to_table(table)\n    pb = self.catalog.getPathBuilder()\n    if not (self.model.is_vocabulary(table)):\n        raise DerivaMLTableTypeError(\"vocabulary\", table.name)\n\n    # Get schema and table names for path building\n    schema_name = table.schema.name\n    table_name = table.name\n\n    try:\n        # Attempt to insert a new term\n        term_id = VocabularyTerm.model_validate(\n            pb.schemas[schema_name]\n            .tables[table_name]\n            .insert(\n                [\n                    {\n                        \"Name\": term_name,\n                        \"Description\": description,\n                        \"Synonyms\": synonyms,\n                    }\n                ],\n                defaults={\"ID\", \"URI\"},\n            )[0]\n        )\n    except DataPathException:\n        # Term exists - look it up or raise an error\n        term_id = self.lookup_term(table, term_name)\n        if not exists_ok:\n            raise DerivaMLInvalidTerm(table.name, term_name, msg=\"term already exists\")\n    return term_id\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.add_workflow","title":"add_workflow","text":"<pre><code>add_workflow(workflow: Workflow) -&gt; RID\n</code></pre> <p>Adds a workflow to the catalog.</p> <p>Registers a new workflow in the catalog or returns the RID of an existing workflow with the same URL or checksum.</p> <p>Each workflow represents a specific computational process or analysis pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>workflow</code> <code>Workflow</code> <p>Workflow object containing name, URL, type, version, and description.</p> required <p>Returns:</p> Name Type Description <code>RID</code> <code>RID</code> <p>Resource Identifier of the added or existing workflow.</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If workflow insertion fails or required fields are missing.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; workflow = Workflow(\n...     name=\"Gene Analysis\",\n...     url=\"https://github.com/org/repo/workflows/gene_analysis.py\",\n...     workflow_type=\"python_script\",\n...     version=\"1.0.0\",\n...     description=\"Analyzes gene expression patterns\"\n... )\n&gt;&gt;&gt; workflow_rid = ml.add_workflow(workflow)\n</code></pre> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>def add_workflow(self, workflow: Workflow) -&gt; RID:\n    \"\"\"Adds a workflow to the catalog.\n\n    Registers a new workflow in the catalog or returns the RID of an existing workflow with the same\n    URL or checksum.\n\n    Each workflow represents a specific computational process or analysis pipeline.\n\n    Args:\n        workflow: Workflow object containing name, URL, type, version, and description.\n\n    Returns:\n        RID: Resource Identifier of the added or existing workflow.\n\n    Raises:\n        DerivaMLException: If workflow insertion fails or required fields are missing.\n\n    Examples:\n        &gt;&gt;&gt; workflow = Workflow(\n        ...     name=\"Gene Analysis\",\n        ...     url=\"https://github.com/org/repo/workflows/gene_analysis.py\",\n        ...     workflow_type=\"python_script\",\n        ...     version=\"1.0.0\",\n        ...     description=\"Analyzes gene expression patterns\"\n        ... )\n        &gt;&gt;&gt; workflow_rid = ml.add_workflow(workflow)\n    \"\"\"\n    # Check if a workflow already exists by URL\n    if workflow_rid := self.lookup_workflow(workflow.checksum or workflow.url):\n        return workflow_rid\n\n    # Get an ML schema path for the workflow table\n    ml_schema_path = self.pathBuilder.schemas[self.ml_schema]\n\n    try:\n        # Create a workflow record\n        workflow_record = {\n            \"URL\": workflow.url,\n            \"Name\": workflow.name,\n            \"Description\": workflow.description,\n            \"Checksum\": workflow.checksum,\n            \"Version\": workflow.version,\n            MLVocab.workflow_type: self.lookup_term(MLVocab.workflow_type, workflow.workflow_type).name,\n        }\n        # Insert a workflow and get its RID\n        workflow_rid = ml_schema_path.Workflow.insert([workflow_record])[0][\"RID\"]\n    except Exception as e:\n        error = format_exception(e)\n        raise DerivaMLException(f\"Failed to insert workflow. Error: {error}\")\n    return workflow_rid\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.chaise_url","title":"chaise_url","text":"<pre><code>chaise_url(\n    table: RID | Table | str,\n) -&gt; str\n</code></pre> <p>Generates Chaise web interface URL.</p> <p>Chaise is Deriva's web interface for data exploration. This method creates a URL that directly links to the specified table or record.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>RID | Table | str</code> <p>Table to generate URL for (name, Table object, or RID).</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>URL in format: https://{host}/chaise/recordset/#{catalog}/{schema}:{table}</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If table or RID cannot be found.</p> <p>Examples:</p> <p>Using table name:     &gt;&gt;&gt; ml.chaise_url(\"experiment_table\")     'https://deriva.org/chaise/recordset/#1/schema:experiment_table'</p> <p>Using RID:     &gt;&gt;&gt; ml.chaise_url(\"1-abc123\")</p> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>def chaise_url(self, table: RID | Table | str) -&gt; str:\n    \"\"\"Generates Chaise web interface URL.\n\n    Chaise is Deriva's web interface for data exploration. This method creates a URL that directly links to\n    the specified table or record.\n\n    Args:\n        table: Table to generate URL for (name, Table object, or RID).\n\n    Returns:\n        str: URL in format: https://{host}/chaise/recordset/#{catalog}/{schema}:{table}\n\n    Raises:\n        DerivaMLException: If table or RID cannot be found.\n\n    Examples:\n        Using table name:\n            &gt;&gt;&gt; ml.chaise_url(\"experiment_table\")\n            'https://deriva.org/chaise/recordset/#1/schema:experiment_table'\n\n        Using RID:\n            &gt;&gt;&gt; ml.chaise_url(\"1-abc123\")\n    \"\"\"\n    # Get the table object and build base URI\n    table_obj = self.model.name_to_table(table)\n    try:\n        uri = self.catalog.get_server_uri().replace(\"ermrest/catalog/\", \"chaise/recordset/#\")\n    except DerivaMLException:\n        # Handle RID case\n        uri = self.cite(cast(str, table))\n    return f\"{uri}/{urlquote(table_obj.schema.name)}:{urlquote(table_obj.name)}\"\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.cite","title":"cite","text":"<pre><code>cite(\n    entity: Dict[str, Any] | str,\n) -&gt; str\n</code></pre> <p>Generates permanent citation URL.</p> <p>Creates a versioned URL that can be used to reference a specific entity in the catalog. The URL includes the catalog snapshot time to ensure version stability.</p> <p>Parameters:</p> Name Type Description Default <code>entity</code> <code>Dict[str, Any] | str</code> <p>Either a RID string or a dictionary containing entity data with a 'RID' key.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Permanent citation URL in format: https://{host}/id/{catalog}/{rid}@{snapshot_time}</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If an entity doesn't exist or lacks a RID.</p> <p>Examples:</p> <p>Using a RID string:     &gt;&gt;&gt; url = ml.cite(\"1-abc123\")     &gt;&gt;&gt; print(url)     'https://deriva.org/id/1/1-abc123@2024-01-01T12:00:00'</p> <p>Using a dictionary:     &gt;&gt;&gt; url = ml.cite({\"RID\": \"1-abc123\"})</p> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>def cite(self, entity: Dict[str, Any] | str) -&gt; str:\n    \"\"\"Generates permanent citation URL.\n\n    Creates a versioned URL that can be used to reference a specific entity in the catalog. The URL includes\n    the catalog snapshot time to ensure version stability.\n\n    Args:\n        entity: Either a RID string or a dictionary containing entity data with a 'RID' key.\n\n    Returns:\n        str: Permanent citation URL in format: https://{host}/id/{catalog}/{rid}@{snapshot_time}\n\n    Raises:\n        DerivaMLException: If an entity doesn't exist or lacks a RID.\n\n    Examples:\n        Using a RID string:\n            &gt;&gt;&gt; url = ml.cite(\"1-abc123\")\n            &gt;&gt;&gt; print(url)\n            'https://deriva.org/id/1/1-abc123@2024-01-01T12:00:00'\n\n        Using a dictionary:\n            &gt;&gt;&gt; url = ml.cite({\"RID\": \"1-abc123\"})\n    \"\"\"\n    # Return if already a citation URL\n    if isinstance(entity, str) and entity.startswith(f\"https://{self.host_name}/id/{self.catalog_id}/\"):\n        return entity\n\n    try:\n        # Resolve RID and create citation URL with snapshot time\n        self.resolve_rid(rid := entity if isinstance(entity, str) else entity[\"RID\"])\n        return f\"https://{self.host_name}/id/{self.catalog_id}/{rid}@{self.catalog.latest_snapshot().snaptime}\"\n    except KeyError as e:\n        raise DerivaMLException(f\"Entity {e} does not have RID column\")\n    except DerivaMLException as _e:\n        raise DerivaMLException(\"Entity RID does not exist\")\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.create_asset","title":"create_asset","text":"<pre><code>create_asset(\n    asset_name: str,\n    column_defs: Iterable[\n        ColumnDefinition\n    ]\n    | None = None,\n    fkey_defs: Iterable[\n        ColumnDefinition\n    ]\n    | None = None,\n    referenced_tables: Iterable[Table]\n    | None = None,\n    comment: str = \"\",\n    schema: str | None = None,\n) -&gt; Table\n</code></pre> <p>Creates an asset table.</p> <p>Parameters:</p> Name Type Description Default <code>asset_name</code> <code>str</code> <p>Name of the asset table.</p> required <code>column_defs</code> <code>Iterable[ColumnDefinition] | None</code> <p>Iterable of ColumnDefinition objects to provide additional metadata for asset.</p> <code>None</code> <code>fkey_defs</code> <code>Iterable[ColumnDefinition] | None</code> <p>Iterable of ForeignKeyDefinition objects to provide additional metadata for asset.</p> <code>None</code> <code>referenced_tables</code> <code>Iterable[Table] | None</code> <p>Iterable of Table objects to which asset should provide foreign-key references to.</p> <code>None</code> <code>comment</code> <code>str</code> <p>Description of the asset table. (Default value = '')</p> <code>''</code> <code>schema</code> <code>str | None</code> <p>Schema in which to create the asset table.  Defaults to domain_schema.</p> <code>None</code> <p>Returns:</p> Type Description <code>Table</code> <p>Table object for the asset table.</p> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>@validate_call(config=ConfigDict(arbitrary_types_allowed=True))\ndef create_asset(\n    self,\n    asset_name: str,\n    column_defs: Iterable[ColumnDefinition] | None = None,\n    fkey_defs: Iterable[ColumnDefinition] | None = None,\n    referenced_tables: Iterable[Table] | None = None,\n    comment: str = \"\",\n    schema: str | None = None,\n) -&gt; Table:\n    \"\"\"Creates an asset table.\n\n    Args:\n        asset_name: Name of the asset table.\n        column_defs: Iterable of ColumnDefinition objects to provide additional metadata for asset.\n        fkey_defs: Iterable of ForeignKeyDefinition objects to provide additional metadata for asset.\n        referenced_tables: Iterable of Table objects to which asset should provide foreign-key references to.\n        comment: Description of the asset table. (Default value = '')\n        schema: Schema in which to create the asset table.  Defaults to domain_schema.\n\n    Returns:\n        Table object for the asset table.\n    \"\"\"\n    # Initialize empty collections if None provided\n    column_defs = column_defs or []\n    fkey_defs = fkey_defs or []\n    referenced_tables = referenced_tables or []\n    schema = schema or self.domain_schema\n\n    # Add an asset type to vocabulary\n    self.add_term(MLVocab.asset_type, asset_name, description=f\"A {asset_name} asset\")\n\n    # Create the main asset table\n    asset_table = self.model.schemas[schema].create_table(\n        Table.define_asset(\n            schema,\n            asset_name,\n            column_defs=[c.model_dump() for c in column_defs],\n            fkey_defs=[fk.model_dump() for fk in fkey_defs],\n            comment=comment,\n        )\n    )\n\n    # Create an association table between asset and asset type\n    self.model.schemas[self.domain_schema].create_table(\n        Table.define_association(\n            [\n                (asset_table.name, asset_table),\n                (\"Asset_Type\", self.model.name_to_table(\"Asset_Type\")),\n            ]\n        )\n    )\n\n    # Create references to other tables if specified\n    for t in referenced_tables:\n        asset_table.create_reference(self.model.name_to_table(t))\n\n    # Create an association table for tracking execution\n    atable = self.model.schemas[self.domain_schema].create_table(\n        Table.define_association(\n            [\n                (asset_name, asset_table),\n                (\n                    \"Execution\",\n                    self.model.schemas[self.ml_schema].tables[\"Execution\"],\n                ),\n            ]\n        )\n    )\n    atable.create_reference(self.model.name_to_table(\"Asset_Role\"))\n\n    # Add asset annotations\n    asset_annotation(asset_table)\n    return asset_table\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.create_dataset","title":"create_dataset","text":"<pre><code>create_dataset(\n    dataset_types: str\n    | list[str]\n    | None = None,\n    description: str = \"\",\n    execution_rid: RID | None = None,\n    version: DatasetVersion\n    | None = None,\n) -&gt; RID\n</code></pre> <p>Creates a new dataset in the catalog.</p> <p>Creates a dataset with specified types and description. The dataset can be associated with an execution and initialized with a specific version.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_types</code> <code>str | list[str] | None</code> <p>One or more dataset type terms from Dataset_Type vocabulary.</p> <code>None</code> <code>description</code> <code>str</code> <p>Description of the dataset's purpose and contents.</p> <code>''</code> <code>execution_rid</code> <code>RID | None</code> <p>Optional execution RID to associate with dataset creation.</p> <code>None</code> <code>version</code> <code>DatasetVersion | None</code> <p>Optional initial version number. Defaults to 0.1.0.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>RID</code> <code>RID</code> <p>Resource Identifier of the newly created dataset.</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If dataset_types are invalid or creation fails.</p> Example <p>rid = ml.create_dataset( ...     dataset_types=[\"experiment\", \"raw_data\"], ...     description=\"RNA sequencing experiment data\", ...     version=DatasetVersion(1, 0, 0) ... )</p> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>@validate_call(config=ConfigDict(arbitrary_types_allowed=True))\ndef create_dataset(\n    self,\n    dataset_types: str | list[str] | None = None,\n    description: str = \"\",\n    execution_rid: RID | None = None,\n    version: DatasetVersion | None = None,\n) -&gt; RID:\n    \"\"\"Creates a new dataset in the catalog.\n\n    Creates a dataset with specified types and description. The dataset can be associated\n    with an execution and initialized with a specific version.\n\n    Args:\n        dataset_types: One or more dataset type terms from Dataset_Type vocabulary.\n        description: Description of the dataset's purpose and contents.\n        execution_rid: Optional execution RID to associate with dataset creation.\n        version: Optional initial version number. Defaults to 0.1.0.\n\n    Returns:\n        RID: Resource Identifier of the newly created dataset.\n\n    Raises:\n        DerivaMLException: If dataset_types are invalid or creation fails.\n\n    Example:\n        &gt;&gt;&gt; rid = ml.create_dataset(\n        ...     dataset_types=[\"experiment\", \"raw_data\"],\n        ...     description=\"RNA sequencing experiment data\",\n        ...     version=DatasetVersion(1, 0, 0)\n        ... )\n    \"\"\"\n\n    version = version or DatasetVersion(0, 1, 0)\n    dataset_types = dataset_types or []\n\n    type_path = self._model.catalog.getPathBuilder().schemas[self._ml_schema].tables[MLVocab.dataset_type.value]\n    defined_types = list(type_path.entities().fetch())\n\n    def check_dataset_type(dtype: str) -&gt; bool:\n        for term in defined_types:\n            if dtype == term[\"Name\"] or (term[\"Synonyms\"] and ds_type in term[\"Synonyms\"]):\n                return True\n        return False\n\n    # Create the entry for the new dataset_table and get its RID.\n    ds_types = [dataset_types] if isinstance(dataset_types, str) else dataset_types\n    pb = self._model.catalog.getPathBuilder()\n    for ds_type in ds_types:\n        if not check_dataset_type(ds_type):\n            raise DerivaMLException(\"Dataset type must be a vocabulary term.\")\n    dataset_table_path = pb.schemas[self._dataset_table.schema.name].tables[self._dataset_table.name]\n    dataset_rid = dataset_table_path.insert(\n        [\n            {\n                \"Description\": description,\n                \"Deleted\": False,\n            }\n        ]\n    )[0][\"RID\"]\n\n    # Get the name of the association table between dataset_table and dataset_type.\n    associations = list(self._model.schemas[self._ml_schema].tables[MLVocab.dataset_type].find_associations())\n    atable = associations[0].name if associations else None\n    pb.schemas[self._ml_schema].tables[atable].insert(\n        [{MLVocab.dataset_type: ds_type, \"Dataset\": dataset_rid} for ds_type in ds_types]\n    )\n    if execution_rid is not None:\n        pb.schemas[self._ml_schema].Dataset_Execution.insert([{\"Dataset\": dataset_rid, \"Execution\": execution_rid}])\n    self._insert_dataset_versions(\n        [DatasetSpec(rid=dataset_rid, version=version)],\n        execution_rid=execution_rid,\n        description=\"Initial dataset creation.\",\n    )\n    return dataset_rid\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.create_execution","title":"create_execution","text":"<pre><code>create_execution(\n    configuration: ExecutionConfiguration,\n    dry_run: bool = False,\n) -&gt; \"Execution\"\n</code></pre> <p>Creates an execution environment.</p> <p>Given an execution configuration, initialize the local compute environment to prepare for executing an ML or analytic routine.  This routine has a number of side effects.</p> <ol> <li> <p>The datasets specified in the configuration are downloaded and placed in the cache-dir. If a version is not specified in the configuration, then a new minor version number is created for the dataset and downloaded.</p> </li> <li> <p>If any execution assets are provided in the configuration, they are downloaded and placed in the working directory.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>configuration</code> <code>ExecutionConfiguration</code> <p>ExecutionConfiguration:</p> required <code>dry_run</code> <code>bool</code> <p>Do not create an execution record or upload results.</p> <code>False</code> <p>Returns:</p> Type Description <code>'Execution'</code> <p>An execution object.</p> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>def create_execution(self, configuration: ExecutionConfiguration, dry_run: bool = False) -&gt; \"Execution\":\n    \"\"\"Creates an execution environment.\n\n    Given an execution configuration, initialize the local compute environment to prepare for executing an\n    ML or analytic routine.  This routine has a number of side effects.\n\n    1. The datasets specified in the configuration are downloaded and placed in the cache-dir. If a version is\n    not specified in the configuration, then a new minor version number is created for the dataset and downloaded.\n\n    2. If any execution assets are provided in the configuration, they are downloaded\n    and placed in the working directory.\n\n\n    Args:\n        configuration: ExecutionConfiguration:\n        dry_run: Do not create an execution record or upload results.\n\n    Returns:\n        An execution object.\n    \"\"\"\n    # Import here to avoid circular dependency\n    from deriva_ml.execution.execution import Execution\n\n    # Create and store an execution instance\n    self._execution = Execution(configuration, self, dry_run=dry_run)\n    return self._execution\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.create_feature","title":"create_feature","text":"<pre><code>create_feature(\n    target_table: Table | str,\n    feature_name: str,\n    terms: list[Table | str]\n    | None = None,\n    assets: list[Table | str]\n    | None = None,\n    metadata: list[\n        ColumnDefinition\n        | Table\n        | Key\n        | str\n    ]\n    | None = None,\n    optional: list[str] | None = None,\n    comment: str = \"\",\n) -&gt; type[FeatureRecord]\n</code></pre> <p>Creates a new feature definition.</p> <p>A feature represents a measurable property or characteristic that can be associated with records in the target table. Features can include vocabulary terms, asset references, and additional metadata.</p> <p>Parameters:</p> Name Type Description Default <code>target_table</code> <code>Table | str</code> <p>Table to associate the feature with (name or Table object).</p> required <code>feature_name</code> <code>str</code> <p>Unique name for the feature within the target table.</p> required <code>terms</code> <code>list[Table | str] | None</code> <p>Optional vocabulary tables/names whose terms can be used as feature values.</p> <code>None</code> <code>assets</code> <code>list[Table | str] | None</code> <p>Optional asset tables/names that can be referenced by this feature.</p> <code>None</code> <code>metadata</code> <code>list[ColumnDefinition | Table | Key | str] | None</code> <p>Optional columns, tables, or keys to include in a feature definition.</p> <code>None</code> <code>optional</code> <code>list[str] | None</code> <p>Column names that are not required when creating feature instances.</p> <code>None</code> <code>comment</code> <code>str</code> <p>Description of the feature's purpose and usage.</p> <code>''</code> <p>Returns:</p> Type Description <code>type[FeatureRecord]</code> <p>type[FeatureRecord]: Feature class for creating validated instances.</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If a feature definition is invalid or conflicts with existing features.</p> <p>Examples:</p> <p>Create a feature with confidence score:     &gt;&gt;&gt; feature_class = ml.create_feature(     ...     target_table=\"samples\",     ...     feature_name=\"expression_level\",     ...     terms=[\"expression_values\"],     ...     metadata=[ColumnDefinition(name=\"confidence\", type=BuiltinTypes.float4)],     ...     comment=\"Gene expression measurement\"     ... )</p> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>@validate_call(config=ConfigDict(arbitrary_types_allowed=True))\ndef create_feature(\n    self,\n    target_table: Table | str,\n    feature_name: str,\n    terms: list[Table | str] | None = None,\n    assets: list[Table | str] | None = None,\n    metadata: list[ColumnDefinition | Table | Key | str] | None = None,\n    optional: list[str] | None = None,\n    comment: str = \"\",\n) -&gt; type[FeatureRecord]:\n    \"\"\"Creates a new feature definition.\n\n    A feature represents a measurable property or characteristic that can be associated with records in the target\n    table. Features can include vocabulary terms, asset references, and additional metadata.\n\n    Args:\n        target_table: Table to associate the feature with (name or Table object).\n        feature_name: Unique name for the feature within the target table.\n        terms: Optional vocabulary tables/names whose terms can be used as feature values.\n        assets: Optional asset tables/names that can be referenced by this feature.\n        metadata: Optional columns, tables, or keys to include in a feature definition.\n        optional: Column names that are not required when creating feature instances.\n        comment: Description of the feature's purpose and usage.\n\n    Returns:\n        type[FeatureRecord]: Feature class for creating validated instances.\n\n    Raises:\n        DerivaMLException: If a feature definition is invalid or conflicts with existing features.\n\n    Examples:\n        Create a feature with confidence score:\n            &gt;&gt;&gt; feature_class = ml.create_feature(\n            ...     target_table=\"samples\",\n            ...     feature_name=\"expression_level\",\n            ...     terms=[\"expression_values\"],\n            ...     metadata=[ColumnDefinition(name=\"confidence\", type=BuiltinTypes.float4)],\n            ...     comment=\"Gene expression measurement\"\n            ... )\n    \"\"\"\n    # Initialize empty collections if None provided\n    terms = terms or []\n    assets = assets or []\n    metadata = metadata or []\n    optional = optional or []\n\n    def normalize_metadata(m: Key | Table | ColumnDefinition | str):\n        \"\"\"Helper function to normalize metadata references.\"\"\"\n        if isinstance(m, str):\n            return self.model.name_to_table(m)\n        elif isinstance(m, ColumnDefinition):\n            return m.model_dump()\n        else:\n            return m\n\n    # Validate asset and term tables\n    if not all(map(self.model.is_asset, assets)):\n        raise DerivaMLException(\"Invalid create_feature asset table.\")\n    if not all(map(self.model.is_vocabulary, terms)):\n        raise DerivaMLException(\"Invalid create_feature asset table.\")\n\n    # Get references to required tables\n    target_table = self.model.name_to_table(target_table)\n    execution = self.model.schemas[self.ml_schema].tables[\"Execution\"]\n    feature_name_table = self.model.schemas[self.ml_schema].tables[\"Feature_Name\"]\n\n    # Add feature name to vocabulary\n    feature_name_term = self.add_term(\"Feature_Name\", feature_name, description=comment)\n    atable_name = f\"Execution_{target_table.name}_{feature_name_term.name}\"\n    # Create an association table implementing the feature\n    atable = self.model.schemas[self.domain_schema].create_table(\n        target_table.define_association(\n            table_name=atable_name,\n            associates=[execution, target_table, feature_name_table],\n            metadata=[normalize_metadata(m) for m in chain(assets, terms, metadata)],\n            comment=comment,\n        )\n    )\n    # Configure optional columns and default feature name\n    for c in optional:\n        atable.columns[c].alter(nullok=True)\n    atable.columns[\"Feature_Name\"].alter(default=feature_name_term.name)\n\n    # Return feature record class for creating instances\n    return self.feature_record_class(target_table, feature_name)\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.create_table","title":"create_table","text":"<pre><code>create_table(\n    table: TableDefinition,\n) -&gt; Table\n</code></pre> <p>Creates a new table in the catalog.</p> <p>Creates a table using the provided TableDefinition object, which specifies the table structure including columns, keys, and foreign key relationships.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>TableDefinition</code> <p>A TableDefinition object containing the complete specification of the table to create.</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>The newly created ERMRest table object.</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If table creation fails or the definition is invalid.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; table_def = TableDefinition(\n...     name=\"experiments\",\n...     column_definitions=[\n...         ColumnDefinition(name=\"name\", type=BuiltinTypes.text),\n...         ColumnDefinition(name=\"date\", type=BuiltinTypes.date)\n...     ]\n... )\n&gt;&gt;&gt; new_table = ml.create_table(table_def)\n</code></pre> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>def create_table(self, table: TableDefinition) -&gt; Table:\n    \"\"\"Creates a new table in the catalog.\n\n    Creates a table using the provided TableDefinition object, which specifies the table structure including\n    columns, keys, and foreign key relationships.\n\n    Args:\n        table: A TableDefinition object containing the complete specification of the table to create.\n\n    Returns:\n        Table: The newly created ERMRest table object.\n\n    Raises:\n        DerivaMLException: If table creation fails or the definition is invalid.\n\n    Example:\n\n        &gt;&gt;&gt; table_def = TableDefinition(\n        ...     name=\"experiments\",\n        ...     column_definitions=[\n        ...         ColumnDefinition(name=\"name\", type=BuiltinTypes.text),\n        ...         ColumnDefinition(name=\"date\", type=BuiltinTypes.date)\n        ...     ]\n        ... )\n        &gt;&gt;&gt; new_table = ml.create_table(table_def)\n    \"\"\"\n    # Create table in domain schema using provided definition\n    return self.model.schemas[self.domain_schema].create_table(table.model_dump())\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.create_vocabulary","title":"create_vocabulary","text":"<pre><code>create_vocabulary(\n    vocab_name: str,\n    comment: str = \"\",\n    schema: str | None = None,\n) -&gt; Table\n</code></pre> <p>Creates a controlled vocabulary table.</p> <p>A controlled vocabulary table maintains a list of standardized terms and their definitions. Each term can have synonyms and descriptions to ensure consistent terminology usage across the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_name</code> <code>str</code> <p>Name for the new vocabulary table. Must be a valid SQL identifier.</p> required <code>comment</code> <code>str</code> <p>Description of the vocabulary's purpose and usage. Defaults to empty string.</p> <code>''</code> <code>schema</code> <code>str | None</code> <p>Schema name to create the table in. If None, uses domain_schema.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>ERMRest table object representing the newly created vocabulary table.</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If vocab_name is invalid or already exists.</p> <p>Examples:</p> <p>Create a vocabulary for tissue types:</p> <pre><code>&gt;&gt;&gt; table = ml.create_vocabulary(\n...     vocab_name=\"tissue_types\",\n...     comment=\"Standard tissue classifications\",\n...     schema=\"bio_schema\"\n... )\n</code></pre> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>def create_vocabulary(self, vocab_name: str, comment: str = \"\", schema: str | None = None) -&gt; Table:\n    \"\"\"Creates a controlled vocabulary table.\n\n    A controlled vocabulary table maintains a list of standardized terms and their definitions. Each term can have\n    synonyms and descriptions to ensure consistent terminology usage across the dataset.\n\n    Args:\n        vocab_name: Name for the new vocabulary table. Must be a valid SQL identifier.\n        comment: Description of the vocabulary's purpose and usage. Defaults to empty string.\n        schema: Schema name to create the table in. If None, uses domain_schema.\n\n    Returns:\n        Table: ERMRest table object representing the newly created vocabulary table.\n\n    Raises:\n        DerivaMLException: If vocab_name is invalid or already exists.\n\n    Examples:\n        Create a vocabulary for tissue types:\n\n            &gt;&gt;&gt; table = ml.create_vocabulary(\n            ...     vocab_name=\"tissue_types\",\n            ...     comment=\"Standard tissue classifications\",\n            ...     schema=\"bio_schema\"\n            ... )\n    \"\"\"\n    # Use domain schema if none specified\n    schema = schema or self.domain_schema\n\n    # Create and return vocabulary table with RID-based URI pattern\n    try:\n        vocab_table = self.model.schemas[schema].create_table(\n            Table.define_vocabulary(vocab_name, f\"{self.project_name}:{{RID}}\", comment=comment)\n        )\n    except ValueError:\n        raise DerivaMLException(f\"Table {vocab_name} already exist\")\n    return vocab_table\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.create_workflow","title":"create_workflow","text":"<pre><code>create_workflow(\n    name: str,\n    workflow_type: str,\n    description: str = \"\",\n) -&gt; Workflow\n</code></pre> <p>Creates a new workflow definition.</p> <p>Creates a Workflow object that represents a computational process or analysis pipeline. The workflow type must be a term from the controlled vocabulary. This method is typically used to define new analysis workflows before execution.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the workflow.</p> required <code>workflow_type</code> <code>str</code> <p>Type of workflow (must exist in workflow_type vocabulary).</p> required <code>description</code> <code>str</code> <p>Description of what the workflow does.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>Workflow</code> <code>Workflow</code> <p>New workflow object ready for registration.</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If workflow_type is not in the vocabulary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; workflow = ml.create_workflow(\n...     name=\"RNA Analysis\",\n...     workflow_type=\"python_notebook\",\n...     description=\"RNA sequence analysis pipeline\"\n... )\n&gt;&gt;&gt; rid = ml.add_workflow(workflow)\n</code></pre> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>def create_workflow(self, name: str, workflow_type: str, description: str = \"\") -&gt; Workflow:\n    \"\"\"Creates a new workflow definition.\n\n    Creates a Workflow object that represents a computational process or analysis pipeline. The workflow type\n    must be a term from the controlled vocabulary. This method is typically used to define new analysis\n    workflows before execution.\n\n    Args:\n        name: Name of the workflow.\n        workflow_type: Type of workflow (must exist in workflow_type vocabulary).\n        description: Description of what the workflow does.\n\n    Returns:\n        Workflow: New workflow object ready for registration.\n\n    Raises:\n        DerivaMLException: If workflow_type is not in the vocabulary.\n\n    Examples:\n        &gt;&gt;&gt; workflow = ml.create_workflow(\n        ...     name=\"RNA Analysis\",\n        ...     workflow_type=\"python_notebook\",\n        ...     description=\"RNA sequence analysis pipeline\"\n        ... )\n        &gt;&gt;&gt; rid = ml.add_workflow(workflow)\n    \"\"\"\n    # Validate workflow type exists in vocabulary\n    self.lookup_term(MLVocab.workflow_type, workflow_type)\n\n    # Create and return a new workflow object\n    return Workflow(name=name, workflow_type=workflow_type, description=description)\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.dataset_history","title":"dataset_history","text":"<pre><code>dataset_history(\n    dataset_rid: RID,\n) -&gt; list[DatasetHistory]\n</code></pre> <p>Retrieves the version history of a dataset.</p> <p>Returns a chronological list of dataset versions, including their version numbers, creation times, and associated metadata.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_rid</code> <code>RID</code> <p>Resource Identifier of the dataset.</p> required <p>Returns:</p> Type Description <code>list[DatasetHistory]</code> <p>list[DatasetHistory]: List of history entries, each containing: - dataset_version: Version number (major.minor.patch) - minid: Minimal Viable Identifier - snapshot: Catalog snapshot time - dataset_rid: Dataset Resource Identifier - version_rid: Version Resource Identifier - description: Version description - execution_rid: Associated execution RID</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If dataset_rid is not a valid dataset RID.</p> Example <p>history = ml.dataset_history(\"1-abc123\") for entry in history: ...     print(f\"Version {entry.dataset_version}: {entry.description}\")</p> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>def dataset_history(self, dataset_rid: RID) -&gt; list[DatasetHistory]:\n    \"\"\"Retrieves the version history of a dataset.\n\n    Returns a chronological list of dataset versions, including their version numbers,\n    creation times, and associated metadata.\n\n    Args:\n        dataset_rid: Resource Identifier of the dataset.\n\n    Returns:\n        list[DatasetHistory]: List of history entries, each containing:\n            - dataset_version: Version number (major.minor.patch)\n            - minid: Minimal Viable Identifier\n            - snapshot: Catalog snapshot time\n            - dataset_rid: Dataset Resource Identifier\n            - version_rid: Version Resource Identifier\n            - description: Version description\n            - execution_rid: Associated execution RID\n\n    Raises:\n        DerivaMLException: If dataset_rid is not a valid dataset RID.\n\n    Example:\n        &gt;&gt;&gt; history = ml.dataset_history(\"1-abc123\")\n        &gt;&gt;&gt; for entry in history:\n        ...     print(f\"Version {entry.dataset_version}: {entry.description}\")\n    \"\"\"\n\n    if not self._is_dataset_rid(dataset_rid):\n        raise DerivaMLException(f\"RID is not for a data set: {dataset_rid}\")\n    version_path = self._model.catalog.getPathBuilder().schemas[self._ml_schema].tables[\"Dataset_Version\"]\n    return [\n        DatasetHistory(\n            dataset_version=DatasetVersion.parse(v[\"Version\"]),\n            minid=v[\"Minid\"],\n            snapshot=v[\"Snapshot\"],\n            dataset_rid=dataset_rid,\n            version_rid=v[\"RID\"],\n            description=v[\"Description\"],\n            execution_rid=v[\"Execution\"],\n        )\n        for v in version_path.filter(version_path.Dataset == dataset_rid).entities().fetch()\n    ]\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.dataset_version","title":"dataset_version","text":"<pre><code>dataset_version(\n    dataset_rid: RID,\n) -&gt; DatasetVersion\n</code></pre> <p>Retrieve the current version of the specified dataset_table.</p> <p>Given a rid, return the most recent version of the dataset. It is important to remember that this version captures the state of the catalog at the time the version was created, not the current state of the catalog. This means that its possible that the values associated with an object in the catalog may be different from the values of that object in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_rid</code> <code>RID</code> <p>The RID of the dataset to retrieve the version for.</p> required <p>Returns:</p> Type Description <code>DatasetVersion</code> <p>A tuple with the semantic version of the dataset_table.</p> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>@validate_call\ndef dataset_version(self, dataset_rid: RID) -&gt; DatasetVersion:\n    \"\"\"Retrieve the current version of the specified dataset_table.\n\n    Given a rid, return the most recent version of the dataset. It is important to remember that this version\n    captures the state of the catalog at the time the version was created, not the current state of the catalog.\n    This means that its possible that the values associated with an object in the catalog may be different\n    from the values of that object in the dataset.\n\n    Args:\n        dataset_rid: The RID of the dataset to retrieve the version for.\n\n    Returns:\n        A tuple with the semantic version of the dataset_table.\n    \"\"\"\n    history = self.dataset_history(dataset_rid)\n    if not history:\n        return DatasetVersion(0, 1, 0)\n    else:\n        # Ensure we return a DatasetVersion, not a string\n        versions = [h.dataset_version for h in history]\n        return max(versions) if versions else DatasetVersion(0, 1, 0)\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.delete_dataset","title":"delete_dataset","text":"<pre><code>delete_dataset(\n    dataset_rid: RID,\n    recurse: bool = False,\n) -&gt; None\n</code></pre> <p>Delete a dataset_table from the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_rid</code> <code>RID</code> <p>RID of the dataset_table to delete.</p> required <code>recurse</code> <code>bool</code> <p>If True, delete the dataset_table along with any nested datasets. (Default value = False)</p> <code>False</code> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>@validate_call\ndef delete_dataset(self, dataset_rid: RID, recurse: bool = False) -&gt; None:\n    \"\"\"Delete a dataset_table from the catalog.\n\n    Args:\n        dataset_rid: RID of the dataset_table to delete.\n        recurse: If True, delete the dataset_table along with any nested datasets. (Default value = False)\n    \"\"\"\n    # Get association table entries for this dataset_table\n    # Delete association table entries\n    if not self._is_dataset_rid(dataset_rid):\n        raise DerivaMLException(\"Dataset_rid is not a dataset.\")\n\n    if parents := self.list_dataset_parents(dataset_rid):\n        raise DerivaMLException(f'Dataset_rid \"{dataset_rid}\" is in a nested dataset: {parents}.')\n\n    pb = self._model.catalog.getPathBuilder()\n    dataset_path = pb.schemas[self._dataset_table.schema.name].tables[self._dataset_table.name]\n\n    rid_list = [dataset_rid] + (self.list_dataset_children(dataset_rid=dataset_rid) if recurse else [])\n    dataset_path.update([{\"RID\": r, \"Deleted\": True} for r in rid_list])\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.delete_dataset_members","title":"delete_dataset_members","text":"<pre><code>delete_dataset_members(\n    dataset_rid: RID,\n    members: list[RID],\n    description: str = \"\",\n    execution_rid: RID | None = None,\n) -&gt; None\n</code></pre> <p>Remove elements to an existing dataset_table.</p> <p>Delete elements from an existing dataset. In addition to deleting members, the minor version number of the dataset is incremented and the description, if provide is applied to that new version.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_rid</code> <code>RID</code> <p>RID of dataset_table to extend or None if a new dataset_table is to be created.</p> required <code>members</code> <code>list[RID]</code> <p>List of member RIDs to add to the dataset_table.</p> required <code>description</code> <code>str</code> <p>Markdown description of the updated dataset.</p> <code>''</code> <code>execution_rid</code> <code>RID | None</code> <p>Optional RID of execution associated with this operation.</p> <code>None</code> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>@validate_call\ndef delete_dataset_members(\n    self,\n    dataset_rid: RID,\n    members: list[RID],\n    description: str = \"\",\n    execution_rid: RID | None = None,\n) -&gt; None:\n    \"\"\"Remove elements to an existing dataset_table.\n\n    Delete elements from an existing dataset. In addition to deleting members, the minor version number of the\n    dataset is incremented and the description, if provide is applied to that new version.\n\n    Args:\n        dataset_rid: RID of dataset_table to extend or None if a new dataset_table is to be created.\n        members: List of member RIDs to add to the dataset_table.\n        description: Markdown description of the updated dataset.\n        execution_rid: Optional RID of execution associated with this operation.\n    \"\"\"\n\n    members = set(members)\n    description = description or \"Deletes dataset members\"\n\n    # Now go through every rid to be added to the data set and sort them based on what association table entries\n    # need to be made.\n    dataset_elements = {}\n    association_map = {\n        a.other_fkeys.pop().pk_table.name: a.table.name for a in self._dataset_table.find_associations()\n    }\n    # Get a list of all the object types that can be linked to a dataset_table.\n    for m in members:\n        try:\n            rid_info = self._model.catalog.resolve_rid(m)\n        except KeyError:\n            raise DerivaMLException(f\"Invalid RID: {m}\")\n        if rid_info.table.name not in association_map:\n            raise DerivaMLException(f\"RID table: {rid_info.table.name} not part of dataset_table\")\n        dataset_elements.setdefault(rid_info.table.name, []).append(rid_info.rid)\n    # Now make the entries into the association tables.\n    pb = self._model.catalog.getPathBuilder()\n    for table, elements in dataset_elements.items():\n        schema_path = pb.schemas[self._ml_schema if table == \"Dataset\" else self._model.domain_schema]\n        fk_column = \"Nested_Dataset\" if table == \"Dataset\" else table\n\n        if len(elements):\n            atable_path = schema_path.tables[association_map[table]]\n            # Find out the name of the column in the association table.\n            for e in elements:\n                entity = atable_path.filter(\n                    (atable_path.Dataset == dataset_rid) &amp; (atable_path.columns[fk_column] == e),\n                )\n                entity.delete()\n    self.increment_dataset_version(\n        dataset_rid,\n        VersionPart.minor,\n        description=description,\n        execution_rid=execution_rid,\n    )\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.delete_feature","title":"delete_feature","text":"<pre><code>delete_feature(\n    table: Table | str,\n    feature_name: str,\n) -&gt; bool\n</code></pre> <p>Removes a feature definition and its data.</p> <p>Deletes the feature and its implementation table from the catalog. This operation cannot be undone and will remove all feature values associated with this feature.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table | str</code> <p>The table containing the feature, either as name or Table object.</p> required <code>feature_name</code> <code>str</code> <p>Name of the feature to delete.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the feature was successfully deleted, False if it didn't exist.</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If deletion fails due to constraints or permissions.</p> Example <p>success = ml.delete_feature(\"samples\", \"obsolete_feature\") print(\"Deleted\" if success else \"Not found\")</p> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>def delete_feature(self, table: Table | str, feature_name: str) -&gt; bool:\n    \"\"\"Removes a feature definition and its data.\n\n    Deletes the feature and its implementation table from the catalog. This operation cannot be undone and\n    will remove all feature values associated with this feature.\n\n    Args:\n        table: The table containing the feature, either as name or Table object.\n        feature_name: Name of the feature to delete.\n\n    Returns:\n        bool: True if the feature was successfully deleted, False if it didn't exist.\n\n    Raises:\n        DerivaMLException: If deletion fails due to constraints or permissions.\n\n    Example:\n        &gt;&gt;&gt; success = ml.delete_feature(\"samples\", \"obsolete_feature\")\n        &gt;&gt;&gt; print(\"Deleted\" if success else \"Not found\")\n    \"\"\"\n    # Get table reference and find feature\n    table = self.model.name_to_table(table)\n    try:\n        # Find and delete the feature's implementation table\n        feature = next(f for f in self.model.find_features(table) if f.feature_name == feature_name)\n        feature.feature_table.drop()\n        return True\n    except StopIteration:\n        return False\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.download_dataset_bag","title":"download_dataset_bag","text":"<pre><code>download_dataset_bag(\n    dataset: DatasetSpec,\n    execution_rid: RID | None = None,\n) -&gt; DatasetBag\n</code></pre> <p>Downloads a dataset to the local filesystem and creates a MINID if needed.</p> <p>Downloads a dataset specified by DatasetSpec to the local filesystem. If the dataset doesn't have a MINID (Minimal Viable Identifier), one will be created. The dataset can optionally be associated with an execution record.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DatasetSpec</code> <p>Specification of the dataset to download, including version and materialization options.</p> required <code>execution_rid</code> <code>RID | None</code> <p>Optional execution RID to associate the download with.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DatasetBag</code> <code>DatasetBag</code> <p>Object containing: - path: Local filesystem path to downloaded dataset - rid: Dataset's Resource Identifier - minid: Dataset's Minimal Viable Identifier</p> <p>Examples:</p> <p>Download with default options:     &gt;&gt;&gt; spec = DatasetSpec(rid=\"1-abc123\")     &gt;&gt;&gt; bag = ml.download_dataset_bag(dataset=spec)     &gt;&gt;&gt; print(f\"Downloaded to {bag.path}\")</p> <p>Download with execution tracking:     &gt;&gt;&gt; bag = ml.download_dataset_bag(     ...     dataset=DatasetSpec(rid=\"1-abc123\", materialize=True),     ...     execution_rid=\"1-xyz789\"     ... )</p> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>@validate_call(config=ConfigDict(arbitrary_types_allowed=True))\ndef download_dataset_bag(\n    self,\n    dataset: DatasetSpec,\n    execution_rid: RID | None = None,\n) -&gt; DatasetBag:\n    \"\"\"Downloads a dataset to the local filesystem and creates a MINID if needed.\n\n    Downloads a dataset specified by DatasetSpec to the local filesystem. If the dataset doesn't have\n    a MINID (Minimal Viable Identifier), one will be created. The dataset can optionally be associated\n    with an execution record.\n\n    Args:\n        dataset: Specification of the dataset to download, including version and materialization options.\n        execution_rid: Optional execution RID to associate the download with.\n\n    Returns:\n        DatasetBag: Object containing:\n            - path: Local filesystem path to downloaded dataset\n            - rid: Dataset's Resource Identifier\n            - minid: Dataset's Minimal Viable Identifier\n\n    Examples:\n        Download with default options:\n            &gt;&gt;&gt; spec = DatasetSpec(rid=\"1-abc123\")\n            &gt;&gt;&gt; bag = ml.download_dataset_bag(dataset=spec)\n            &gt;&gt;&gt; print(f\"Downloaded to {bag.path}\")\n\n        Download with execution tracking:\n            &gt;&gt;&gt; bag = ml.download_dataset_bag(\n            ...     dataset=DatasetSpec(rid=\"1-abc123\", materialize=True),\n            ...     execution_rid=\"1-xyz789\"\n            ... )\n    \"\"\"\n    if not self._is_dataset_rid(dataset.rid):\n        raise DerivaMLTableTypeError(\"Dataset\", dataset.rid)\n    return self._download_dataset_bag(\n        dataset=dataset,\n        execution_rid=execution_rid,\n        snapshot_catalog=DerivaML(self.host_name, self._version_snapshot(dataset)),\n    )\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.download_dir","title":"download_dir","text":"<pre><code>download_dir(\n    cached: bool = False,\n) -&gt; Path\n</code></pre> <p>Returns the appropriate download directory.</p> <p>Provides the appropriate directory path for storing downloaded files, either in the cache or working directory.</p> <p>Parameters:</p> Name Type Description Default <code>cached</code> <code>bool</code> <p>If True, returns the cache directory path. If False, returns the working directory path.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Directory path where downloaded files should be stored.</p> Example <p>cache_dir = ml.download_dir(cached=True) work_dir = ml.download_dir(cached=False)</p> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>def download_dir(self, cached: bool = False) -&gt; Path:\n    \"\"\"Returns the appropriate download directory.\n\n    Provides the appropriate directory path for storing downloaded files, either in the cache or working directory.\n\n    Args:\n        cached: If True, returns the cache directory path. If False, returns the working directory path.\n\n    Returns:\n        Path: Directory path where downloaded files should be stored.\n\n    Example:\n        &gt;&gt;&gt; cache_dir = ml.download_dir(cached=True)\n        &gt;&gt;&gt; work_dir = ml.download_dir(cached=False)\n    \"\"\"\n    # Return cache directory if cached=True, otherwise working directory\n    return self.cache_dir if cached else self.working_dir\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.feature_record_class","title":"feature_record_class","text":"<pre><code>feature_record_class(\n    table: str | Table,\n    feature_name: str,\n) -&gt; type[FeatureRecord]\n</code></pre> <p>Returns a pydantic model class for feature records.</p> <p>Creates a typed interface for creating new instances of the specified feature. The returned class includes validation and type checking based on the feature's definition.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str | Table</code> <p>The table containing the feature, either as name or Table object.</p> required <code>feature_name</code> <code>str</code> <p>Name of the feature to create a record class for.</p> required <p>Returns:</p> Type Description <code>type[FeatureRecord]</code> <p>type[FeatureRecord]: A pydantic model class for creating validated feature records.</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If the feature doesn't exist or the table is invalid.</p> Example <p>ExpressionFeature = ml.feature_record_class(\"samples\", \"expression_level\") feature = ExpressionFeature(value=\"high\", confidence=0.95)</p> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>def feature_record_class(self, table: str | Table, feature_name: str) -&gt; type[FeatureRecord]:\n    \"\"\"Returns a pydantic model class for feature records.\n\n    Creates a typed interface for creating new instances of the specified feature. The returned class includes\n    validation and type checking based on the feature's definition.\n\n    Args:\n        table: The table containing the feature, either as name or Table object.\n        feature_name: Name of the feature to create a record class for.\n\n    Returns:\n        type[FeatureRecord]: A pydantic model class for creating validated feature records.\n\n    Raises:\n        DerivaMLException: If the feature doesn't exist or the table is invalid.\n\n    Example:\n        &gt;&gt;&gt; ExpressionFeature = ml.feature_record_class(\"samples\", \"expression_level\")\n        &gt;&gt;&gt; feature = ExpressionFeature(value=\"high\", confidence=0.95)\n    \"\"\"\n    # Look up a feature and return its record class\n    return self.lookup_feature(table, feature_name).feature_record_class()\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.find_datasets","title":"find_datasets","text":"<pre><code>find_datasets(\n    deleted: bool = False,\n) -&gt; Iterable[dict[str, Any]]\n</code></pre> <p>Returns a list of currently available datasets.</p> <p>Parameters:</p> Name Type Description Default <code>deleted</code> <code>bool</code> <p>If True, included the datasets that have been deleted.</p> <code>False</code> <p>Returns:</p> Type Description <code>Iterable[dict[str, Any]]</code> <p>list of currently available datasets.</p> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>def find_datasets(self, deleted: bool = False) -&gt; Iterable[dict[str, Any]]:\n    \"\"\"Returns a list of currently available datasets.\n\n    Arguments:\n        deleted: If True, included the datasets that have been deleted.\n\n    Returns:\n         list of currently available datasets.\n    \"\"\"\n    # Get datapath to all the tables we will need: Dataset, DatasetType and the association table.\n    pb = self._model.catalog.getPathBuilder()\n    dataset_path = pb.schemas[self._dataset_table.schema.name].tables[self._dataset_table.name]\n    associations = list(self._model.schemas[self._ml_schema].tables[MLVocab.dataset_type].find_associations())\n    atable = associations[0].name if associations else None\n    ml_path = pb.schemas[self._ml_schema]\n    atable_path = ml_path.tables[atable]\n\n    if deleted:\n        filtered_path = dataset_path\n    else:\n        filtered_path = dataset_path.filter(\n            (dataset_path.Deleted == False) | (dataset_path.Deleted == None)  # noqa: E711, E712\n        )\n\n    # Get a list of all the dataset_type values associated with this dataset_table.\n    datasets = []\n    for dataset in filtered_path.entities().fetch():\n        ds_types = (\n            atable_path.filter(atable_path.Dataset == dataset[\"RID\"]).attributes(atable_path.Dataset_Type).fetch()\n        )\n        datasets.append(dataset | {MLVocab.dataset_type: [ds[MLVocab.dataset_type] for ds in ds_types]})\n    return datasets\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.globus_login","title":"globus_login  <code>staticmethod</code>","text":"<pre><code>globus_login(host: str) -&gt; None\n</code></pre> <p>Authenticates with Globus for accessing Deriva services.</p> <p>Performs authentication using Globus Auth to access Deriva services. If already logged in, notifies the user. Uses non-interactive authentication flow without a browser or local server.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The hostname of the Deriva server to authenticate with (e.g., 'deriva.example.org').</p> required Example <p>DerivaML.globus_login('deriva.example.org') 'Login Successful'</p> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>@staticmethod\ndef globus_login(host: str) -&gt; None:\n    \"\"\"Authenticates with Globus for accessing Deriva services.\n\n    Performs authentication using Globus Auth to access Deriva services. If already logged in, notifies the user.\n    Uses non-interactive authentication flow without a browser or local server.\n\n    Args:\n        host: The hostname of the Deriva server to authenticate with (e.g., 'deriva.example.org').\n\n    Example:\n        &gt;&gt;&gt; DerivaML.globus_login('deriva.example.org')\n        'Login Successful'\n    \"\"\"\n    gnl = GlobusNativeLogin(host=host)\n    if gnl.is_logged_in([host]):\n        print(\"You are already logged in.\")\n    else:\n        gnl.login(\n            [host],\n            no_local_server=True,\n            no_browser=True,\n            refresh_tokens=True,\n            update_bdbag_keychain=True,\n        )\n        print(\"Login Successful\")\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.increment_dataset_version","title":"increment_dataset_version","text":"<pre><code>increment_dataset_version(\n    dataset_rid: RID,\n    component: VersionPart,\n    description: str | None = \"\",\n    execution_rid: RID | None = None,\n) -&gt; DatasetVersion\n</code></pre> <p>Increments a dataset's version number.</p> <p>Creates a new version of the dataset by incrementing the specified version component (major, minor, or patch). The new version is recorded with an optional description and execution reference.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_rid</code> <code>RID</code> <p>Resource Identifier of the dataset to version.</p> required <code>component</code> <code>VersionPart</code> <p>Which version component to increment ('major', 'minor', or 'patch').</p> required <code>description</code> <code>str | None</code> <p>Optional description of the changes in this version.</p> <code>''</code> <code>execution_rid</code> <code>RID | None</code> <p>Optional execution RID to associate with this version.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DatasetVersion</code> <code>DatasetVersion</code> <p>The new version number.</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If dataset_rid is invalid or version increment fails.</p> Example <p>new_version = ml.increment_dataset_version( ...     dataset_rid=\"1-abc123\", ...     component=\"minor\", ...     description=\"Added new samples\" ... ) print(f\"New version: {new_version}\")  # e.g., \"1.2.0\"</p> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>@validate_call(config=ConfigDict(arbitrary_types_allowed=True))\ndef increment_dataset_version(\n    self,\n    dataset_rid: RID,\n    component: VersionPart,\n    description: str | None = \"\",\n    execution_rid: RID | None = None,\n) -&gt; DatasetVersion:\n    \"\"\"Increments a dataset's version number.\n\n    Creates a new version of the dataset by incrementing the specified version component\n    (major, minor, or patch). The new version is recorded with an optional description\n    and execution reference.\n\n    Args:\n        dataset_rid: Resource Identifier of the dataset to version.\n        component: Which version component to increment ('major', 'minor', or 'patch').\n        description: Optional description of the changes in this version.\n        execution_rid: Optional execution RID to associate with this version.\n\n    Returns:\n        DatasetVersion: The new version number.\n\n    Raises:\n        DerivaMLException: If dataset_rid is invalid or version increment fails.\n\n    Example:\n        &gt;&gt;&gt; new_version = ml.increment_dataset_version(\n        ...     dataset_rid=\"1-abc123\",\n        ...     component=\"minor\",\n        ...     description=\"Added new samples\"\n        ... )\n        &gt;&gt;&gt; print(f\"New version: {new_version}\")  # e.g., \"1.2.0\"\n    \"\"\"\n\n    # Find all the datasets that are reachable from this dataset and determine their new version numbers.\n    related_datasets = list(self._build_dataset_graph(dataset_rid=dataset_rid))\n    version_update_list = [\n        DatasetSpec(\n            rid=ds_rid,\n            version=self.dataset_version(ds_rid).increment_version(component),\n        )\n        for ds_rid in related_datasets\n    ]\n    self._insert_dataset_versions(version_update_list, description=description, execution_rid=execution_rid)\n    return next((d.version for d in version_update_list if d.rid == dataset_rid))\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.list_assets","title":"list_assets","text":"<pre><code>list_assets(\n    asset_table: Table | str,\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Lists contents of an asset table.</p> <p>Returns a list of assets with their types for the specified asset table.</p> <p>Parameters:</p> Name Type Description Default <code>asset_table</code> <code>Table | str</code> <p>Table or name of the asset table to list assets for.</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>list[dict[str, Any]]: List of asset records, each containing: - RID: Resource identifier - Type: Asset type - Metadata: Asset metadata</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If the table is not an asset table or doesn't exist.</p> Example <p>assets = ml.list_assets(\"tissue_types\") for asset in assets: ...     print(f\"{asset['RID']}: {asset['Type']}\")</p> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>def list_assets(self, asset_table: Table | str) -&gt; list[dict[str, Any]]:\n    \"\"\"Lists contents of an asset table.\n\n    Returns a list of assets with their types for the specified asset table.\n\n    Args:\n        asset_table: Table or name of the asset table to list assets for.\n\n    Returns:\n        list[dict[str, Any]]: List of asset records, each containing:\n            - RID: Resource identifier\n            - Type: Asset type\n            - Metadata: Asset metadata\n\n    Raises:\n        DerivaMLException: If the table is not an asset table or doesn't exist.\n\n    Example:\n        &gt;&gt;&gt; assets = ml.list_assets(\"tissue_types\")\n        &gt;&gt;&gt; for asset in assets:\n        ...     print(f\"{asset['RID']}: {asset['Type']}\")\n    \"\"\"\n    # Validate and get asset table reference\n    asset_table = self.model.name_to_table(asset_table)\n    if not self.model.is_asset(asset_table):\n        raise DerivaMLException(f\"Table {asset_table.name} is not an asset\")\n\n    # Get path builders for asset and type tables\n    pb = self._model.catalog.getPathBuilder()\n    asset_path = pb.schemas[asset_table.schema.name].tables[asset_table.name]\n    (\n        asset_type_table,\n        _,\n        _,\n    ) = self._model.find_association(asset_table, MLVocab.asset_type)\n    type_path = pb.schemas[asset_type_table.schema.name].tables[asset_type_table.name]\n\n    # Build a list of assets with their types\n    assets = []\n    for asset in asset_path.entities().fetch():\n        # Get associated asset types for each asset\n        asset_types = (\n            type_path.filter(type_path.columns[asset_table.name] == asset[\"RID\"])\n            .attributes(type_path.Asset_Type)\n            .fetch()\n        )\n        # Combine asset data with its types\n        assets.append(\n            asset | {MLVocab.asset_type.value: [asset_type[MLVocab.asset_type.value] for asset_type in asset_types]}\n        )\n    return assets\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.list_dataset_children","title":"list_dataset_children","text":"<pre><code>list_dataset_children(\n    dataset_rid: RID,\n    recurse: bool = False,\n) -&gt; list[RID]\n</code></pre> <p>Given a dataset_table RID, return a list of RIDs for any nested datasets.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_rid</code> <code>RID</code> <p>A dataset_table RID.</p> required <code>recurse</code> <code>bool</code> <p>If True, return a list of nested datasets RIDs.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[RID]</code> <p>list of nested dataset RIDs.</p> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>@validate_call\ndef list_dataset_children(self, dataset_rid: RID, recurse: bool = False) -&gt; list[RID]:\n    \"\"\"Given a dataset_table RID, return a list of RIDs for any nested datasets.\n\n    Args:\n        dataset_rid: A dataset_table RID.\n        recurse: If True, return a list of nested datasets RIDs.\n\n    Returns:\n      list of nested dataset RIDs.\n\n    \"\"\"\n    dataset_dataset_path = self._model.catalog.getPathBuilder().schemas[self._ml_schema].tables[\"Dataset_Dataset\"]\n    nested_datasets = list(dataset_dataset_path.entities().fetch())\n\n    def find_children(rid: RID):\n        children = [child[\"Nested_Dataset\"] for child in nested_datasets if child[\"Dataset\"] == rid]\n        if recurse:\n            for child in children.copy():\n                children.extend(find_children(child))\n        return children\n\n    return find_children(dataset_rid)\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.list_dataset_element_types","title":"list_dataset_element_types","text":"<pre><code>list_dataset_element_types() -&gt; (\n    Iterable[Table]\n)\n</code></pre> <p>List the types of entities that can be added to a dataset_table.</p> <p>Returns:</p> Type Description <code>Iterable[Table]</code> <p>return: An iterable of Table objects that can be included as an element of a dataset_table.</p> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>def list_dataset_element_types(self) -&gt; Iterable[Table]:\n    \"\"\"List the types of entities that can be added to a dataset_table.\n\n    Returns:\n      :return: An iterable of Table objects that can be included as an element of a dataset_table.\n    \"\"\"\n\n    def domain_table(table: Table) -&gt; bool:\n        return table.schema.name == self._model.domain_schema or table.name == self._dataset_table.name\n\n    return [t for a in self._dataset_table.find_associations() if domain_table(t := a.other_fkeys.pop().pk_table)]\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.list_dataset_members","title":"list_dataset_members","text":"<pre><code>list_dataset_members(\n    dataset_rid: RID,\n    recurse: bool = False,\n    limit: int | None = None,\n) -&gt; dict[str, list[dict[str, Any]]]\n</code></pre> <p>Lists members of a dataset.</p> <p>Returns a dictionary mapping member types to lists of member records. Can optionally recurse through nested datasets and limit the number of results.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_rid</code> <code>RID</code> <p>Resource Identifier of the dataset.</p> required <code>recurse</code> <code>bool</code> <p>Whether to include members of nested datasets. Defaults to False.</p> <code>False</code> <code>limit</code> <code>int | None</code> <p>Maximum number of members to return per type. None for no limit.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, list[dict[str, Any]]]</code> <p>dict[str, list[dict[str, Any]]]: Dictionary mapping member types to lists of members. Each member is a dictionary containing the record's attributes.</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If dataset_rid is invalid.</p> Example <p>members = ml.list_dataset_members(\"1-abc123\", recurse=True) for type_name, records in members.items(): ...     print(f\"{type_name}: {len(records)} records\")</p> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>def list_dataset_members(\n    self, dataset_rid: RID, recurse: bool = False, limit: int | None = None\n) -&gt; dict[str, list[dict[str, Any]]]:\n    \"\"\"Lists members of a dataset.\n\n    Returns a dictionary mapping member types to lists of member records. Can optionally\n    recurse through nested datasets and limit the number of results.\n\n    Args:\n        dataset_rid: Resource Identifier of the dataset.\n        recurse: Whether to include members of nested datasets. Defaults to False.\n        limit: Maximum number of members to return per type. None for no limit.\n\n    Returns:\n        dict[str, list[dict[str, Any]]]: Dictionary mapping member types to lists of members.\n            Each member is a dictionary containing the record's attributes.\n\n    Raises:\n        DerivaMLException: If dataset_rid is invalid.\n\n    Example:\n        &gt;&gt;&gt; members = ml.list_dataset_members(\"1-abc123\", recurse=True)\n        &gt;&gt;&gt; for type_name, records in members.items():\n        ...     print(f\"{type_name}: {len(records)} records\")\n    \"\"\"\n\n    if not self._is_dataset_rid(dataset_rid):\n        raise DerivaMLException(f\"RID is not for a dataset_table: {dataset_rid}\")\n\n    # Look at each of the element types that might be in the dataset_table and get the list of rid for them from\n    # the appropriate association table.\n    members = defaultdict(list)\n    pb = self._model.catalog.getPathBuilder()\n    for assoc_table in self._dataset_table.find_associations():\n        other_fkey = assoc_table.other_fkeys.pop()\n        target_table = other_fkey.pk_table\n        member_table = assoc_table.table\n\n        # Look at domain tables and nested datasets.\n        if target_table.schema.name != self._model.domain_schema and not (\n            target_table == self._dataset_table or target_table.name == \"File\"\n        ):\n            continue\n        member_column = (\n            \"Nested_Dataset\" if target_table == self._dataset_table else other_fkey.foreign_key_columns[0].name\n        )\n\n        target_path = pb.schemas[target_table.schema.name].tables[target_table.name]\n        member_path = pb.schemas[member_table.schema.name].tables[member_table.name]\n\n        path = member_path.filter(member_path.Dataset == dataset_rid).link(\n            target_path,\n            on=(member_path.columns[member_column] == target_path.columns[\"RID\"]),\n        )\n        target_entities = list(path.entities().fetch(limit=limit) if limit else path.entities().fetch())\n        members[target_table.name].extend(target_entities)\n        if recurse and target_table == self._dataset_table:\n            # Get the members for all the nested datasets and add to the member list.\n            nested_datasets = [d[\"RID\"] for d in target_entities]\n            for ds in nested_datasets:\n                for k, v in self.list_dataset_members(ds, recurse=recurse).items():\n                    members[k].extend(v)\n    return dict(members)\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.list_dataset_parents","title":"list_dataset_parents","text":"<pre><code>list_dataset_parents(\n    dataset_rid: RID,\n) -&gt; list[str]\n</code></pre> <p>Given a dataset_table RID, return a list of RIDs of the parent datasets if this is included in a nested dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_rid</code> <code>RID</code> <p>return: RID of the parent dataset_table.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>RID of the parent dataset_table.</p> Source code in <code>src/deriva_ml/dataset/dataset.py</code> <pre><code>@validate_call\ndef list_dataset_parents(self, dataset_rid: RID) -&gt; list[str]:\n    \"\"\"Given a dataset_table RID, return a list of RIDs of the parent datasets if this is included in a\n    nested dataset.\n\n    Args:\n        dataset_rid: return: RID of the parent dataset_table.\n\n    Returns:\n        RID of the parent dataset_table.\n    \"\"\"\n    if not self._is_dataset_rid(dataset_rid):\n        raise DerivaMLException(f\"RID: {dataset_rid} does not belong to dataset_table {self._dataset_table.name}\")\n    # Get association table for nested datasets\n    pb = self._model.catalog.getPathBuilder()\n    atable_path = pb.schemas[self._ml_schema].Dataset_Dataset\n    return [p[\"Dataset\"] for p in atable_path.filter(atable_path.Nested_Dataset == dataset_rid).entities().fetch()]\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.list_feature_values","title":"list_feature_values","text":"<pre><code>list_feature_values(\n    table: Table | str,\n    feature_name: str,\n) -&gt; datapath._ResultSet\n</code></pre> <p>Retrieves all values for a feature.</p> <p>Returns all instances of the specified feature that have been created, including their associated metadata and references.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table | str</code> <p>The table containing the feature, either as name or Table object.</p> required <code>feature_name</code> <code>str</code> <p>Name of the feature to retrieve values for.</p> required <p>Returns:</p> Type Description <code>_ResultSet</code> <p>datapath._ResultSet: A result set containing all feature values and their metadata.</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If the feature doesn't exist or cannot be accessed.</p> Example <p>values = ml.list_feature_values(\"samples\", \"expression_level\") for value in values: ...     print(f\"Sample {value['RID']}: {value['value']}\")</p> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>@validate_call(config=ConfigDict(arbitrary_types_allowed=True))\ndef list_feature_values(self, table: Table | str, feature_name: str) -&gt; datapath._ResultSet:\n    \"\"\"Retrieves all values for a feature.\n\n    Returns all instances of the specified feature that have been created, including their associated\n    metadata and references.\n\n    Args:\n        table: The table containing the feature, either as name or Table object.\n        feature_name: Name of the feature to retrieve values for.\n\n    Returns:\n        datapath._ResultSet: A result set containing all feature values and their metadata.\n\n    Raises:\n        DerivaMLException: If the feature doesn't exist or cannot be accessed.\n\n    Example:\n        &gt;&gt;&gt; values = ml.list_feature_values(\"samples\", \"expression_level\")\n        &gt;&gt;&gt; for value in values:\n        ...     print(f\"Sample {value['RID']}: {value['value']}\")\n    \"\"\"\n    # Get table and feature references\n    table = self.model.name_to_table(table)\n    feature = self.lookup_feature(table, feature_name)\n\n    # Build and execute query for feature values\n    pb = self.catalog.getPathBuilder()\n    return pb.schemas[feature.feature_table.schema.name].tables[feature.feature_table.name].entities().fetch()\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.list_files","title":"list_files","text":"<pre><code>list_files(\n    file_types: list[str] | None = None,\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Lists files in the catalog with their metadata.</p> <p>Returns a list of files with their metadata including URL, MD5 hash, length, description, and associated file types. Files can be optionally filtered by type.</p> <p>Parameters:</p> Name Type Description Default <code>file_types</code> <code>list[str] | None</code> <p>Filter results to only include these file types.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>list[dict[str, Any]]: List of file records, each containing: - RID: Resource identifier - URL: File location - MD5: File hash - Length: File size - Description: File description - File_Types: List of associated file types</p> <p>Examples:</p> <p>List all files:     &gt;&gt;&gt; files = ml.list_files()     &gt;&gt;&gt; for f in files:     ...     print(f\"{f['RID']}: {f['URL']}\")</p> <p>Filter by file type:     &gt;&gt;&gt; image_files = ml.list_files([\"image\", \"png\"])</p> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>def list_files(self, file_types: list[str] | None = None) -&gt; list[dict[str, Any]]:\n    \"\"\"Lists files in the catalog with their metadata.\n\n    Returns a list of files with their metadata including URL, MD5 hash, length, description,\n    and associated file types. Files can be optionally filtered by type.\n\n    Args:\n        file_types: Filter results to only include these file types.\n\n    Returns:\n        list[dict[str, Any]]: List of file records, each containing:\n            - RID: Resource identifier\n            - URL: File location\n            - MD5: File hash\n            - Length: File size\n            - Description: File description\n            - File_Types: List of associated file types\n\n    Examples:\n        List all files:\n            &gt;&gt;&gt; files = ml.list_files()\n            &gt;&gt;&gt; for f in files:\n            ...     print(f\"{f['RID']}: {f['URL']}\")\n\n        Filter by file type:\n            &gt;&gt;&gt; image_files = ml.list_files([\"image\", \"png\"])\n    \"\"\"\n\n    asset_type_atable, file_fk, asset_type_fk = self.model.find_association(\"File\", \"Asset_Type\")\n    ml_path = self.pathBuilder.schemas[self._ml_schema]\n    file = ml_path.File\n    asset_type = ml_path.tables[asset_type_atable.name]\n\n    path = file.path\n    path = path.link(asset_type.alias(\"AT\"), on=file.RID == asset_type.columns[file_fk], join_type=\"left\")\n    if file_types:\n        path = path.filter(asset_type.columns[asset_type_fk] == datapath.Any(*file_types))\n    path = path.attributes(\n        path.File.RID,\n        path.File.URL,\n        path.File.MD5,\n        path.File.Length,\n        path.File.Description,\n        path.AT.columns[asset_type_fk],\n    )\n\n    file_map = {}\n    for f in path.fetch():\n        entry = file_map.setdefault(f[\"RID\"], {**f, \"File_Types\": []})\n        if ft := f.get(\"Asset_Type\"):  # assign-and-test in one go\n            entry[\"File_Types\"].append(ft)\n\n    # Now get rid of the File_Type key and return the result\n    return [(f, f.pop(\"Asset_Type\"))[0] for f in file_map.values()]\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.list_vocabulary_terms","title":"list_vocabulary_terms","text":"<pre><code>list_vocabulary_terms(\n    table: str | Table,\n) -&gt; list[VocabularyTerm]\n</code></pre> <p>Lists all terms in a vocabulary table.</p> <p>Retrieves all terms, their descriptions, and synonyms from a controlled vocabulary table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str | Table</code> <p>Vocabulary table to list terms from (name or Table object).</p> required <p>Returns:</p> Type Description <code>list[VocabularyTerm]</code> <p>list[VocabularyTerm]: List of vocabulary terms with their metadata.</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If table doesn't exist or is not a vocabulary table.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; terms = ml.list_vocabulary_terms(\"tissue_types\")\n&gt;&gt;&gt; for term in terms:\n...     print(f\"{term.name}: {term.description}\")\n...     if term.synonyms:\n...         print(f\"  Synonyms: {', '.join(term.synonyms)}\")\n</code></pre> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>def list_vocabulary_terms(self, table: str | Table) -&gt; list[VocabularyTerm]:\n    \"\"\"Lists all terms in a vocabulary table.\n\n    Retrieves all terms, their descriptions, and synonyms from a controlled vocabulary table.\n\n    Args:\n        table: Vocabulary table to list terms from (name or Table object).\n\n    Returns:\n        list[VocabularyTerm]: List of vocabulary terms with their metadata.\n\n    Raises:\n        DerivaMLException: If table doesn't exist or is not a vocabulary table.\n\n    Examples:\n        &gt;&gt;&gt; terms = ml.list_vocabulary_terms(\"tissue_types\")\n        &gt;&gt;&gt; for term in terms:\n        ...     print(f\"{term.name}: {term.description}\")\n        ...     if term.synonyms:\n        ...         print(f\"  Synonyms: {', '.join(term.synonyms)}\")\n    \"\"\"\n    # Get path builder and table reference\n    pb = self.catalog.getPathBuilder()\n    table = self.model.name_to_table(table.value if isinstance(table, MLVocab) else table)\n\n    # Validate table is a vocabulary table\n    if not (self.model.is_vocabulary(table)):\n        raise DerivaMLException(f\"The table {table} is not a controlled vocabulary\")\n\n    # Fetch and convert all terms to VocabularyTerm objects\n    return [VocabularyTerm(**v) for v in pb.schemas[table.schema.name].tables[table.name].entities().fetch()]\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.list_workflows","title":"list_workflows","text":"<pre><code>list_workflows() -&gt; list[Workflow]\n</code></pre> <p>Lists all workflows in the catalog.</p> <p>Retrieves all workflow definitions, including their names, URLs, types, versions, and descriptions.</p> <p>Returns:</p> Type Description <code>list[Workflow]</code> <p>list[Workflow]: List of workflow objects, each containing: - name: Workflow name - url: Source code URL - workflow_type: Type of workflow - version: Version identifier - description: Workflow description - rid: Resource identifier - checksum: Source code checksum</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; workflows = ml.list_workflows()\n&gt;&gt;&gt; for w in workflows:\n        print(f\"{w.name} (v{w.version}): {w.description}\")\n        print(f\"  Source: {w.url}\")\n</code></pre> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>def list_workflows(self) -&gt; list[Workflow]:\n    \"\"\"Lists all workflows in the catalog.\n\n    Retrieves all workflow definitions, including their names, URLs, types, versions,\n    and descriptions.\n\n    Returns:\n        list[Workflow]: List of workflow objects, each containing:\n            - name: Workflow name\n            - url: Source code URL\n            - workflow_type: Type of workflow\n            - version: Version identifier\n            - description: Workflow description\n            - rid: Resource identifier\n            - checksum: Source code checksum\n\n    Examples:\n        &gt;&gt;&gt; workflows = ml.list_workflows()\n        &gt;&gt;&gt; for w in workflows:\n                print(f\"{w.name} (v{w.version}): {w.description}\")\n                print(f\"  Source: {w.url}\")\n    \"\"\"\n    # Get a workflow table path and fetch all workflows\n    workflow_path = self.pathBuilder.schemas[self.ml_schema].Workflow\n    return [\n        Workflow(\n            name=w[\"Name\"],\n            url=w[\"URL\"],\n            workflow_type=w[\"Workflow_Type\"],\n            version=w[\"Version\"],\n            description=w[\"Description\"],\n            rid=w[\"RID\"],\n            checksum=w[\"Checksum\"],\n        )\n        for w in workflow_path.entities().fetch()\n    ]\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.lookup_feature","title":"lookup_feature","text":"<pre><code>lookup_feature(\n    table: str | Table,\n    feature_name: str,\n) -&gt; Feature\n</code></pre> <p>Retrieves a Feature object.</p> <p>Looks up and returns a Feature object that provides an interface to work with an existing feature definition in the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str | Table</code> <p>The table containing the feature, either as name or Table object.</p> required <code>feature_name</code> <code>str</code> <p>Name of the feature to look up.</p> required <p>Returns:</p> Name Type Description <code>Feature</code> <code>Feature</code> <p>An object representing the feature and its implementation.</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If the feature doesn't exist in the specified table.</p> Example <p>feature = ml.lookup_feature(\"samples\", \"expression_level\") print(feature.feature_name) 'expression_level'</p> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>def lookup_feature(self, table: str | Table, feature_name: str) -&gt; Feature:\n    \"\"\"Retrieves a Feature object.\n\n    Looks up and returns a Feature object that provides an interface to work with an existing feature\n    definition in the catalog.\n\n    Args:\n        table: The table containing the feature, either as name or Table object.\n        feature_name: Name of the feature to look up.\n\n    Returns:\n        Feature: An object representing the feature and its implementation.\n\n    Raises:\n        DerivaMLException: If the feature doesn't exist in the specified table.\n\n    Example:\n        &gt;&gt;&gt; feature = ml.lookup_feature(\"samples\", \"expression_level\")\n        &gt;&gt;&gt; print(feature.feature_name)\n        'expression_level'\n    \"\"\"\n    return self.model.lookup_feature(table, feature_name)\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.lookup_term","title":"lookup_term","text":"<pre><code>lookup_term(\n    table: str | Table, term_name: str\n) -&gt; VocabularyTerm\n</code></pre> <p>Finds a term in a vocabulary table.</p> <p>Searches for a term in the specified vocabulary table, matching either the primary name or any of its synonyms.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str | Table</code> <p>Vocabulary table to search in (name or Table object).</p> required <code>term_name</code> <code>str</code> <p>Name or synonym of the term to find.</p> required <p>Returns:</p> Name Type Description <code>VocabularyTerm</code> <code>VocabularyTerm</code> <p>The matching vocabulary term.</p> <p>Raises:</p> Type Description <code>DerivaMLVocabularyException</code> <p>If the table is not a vocabulary table, or term is not found.</p> <p>Examples:</p> <p>Look up by primary name:     &gt;&gt;&gt; term = ml.lookup_term(\"tissue_types\", \"epithelial\")     &gt;&gt;&gt; print(term.description)</p> <p>Look up by synonym:     &gt;&gt;&gt; term = ml.lookup_term(\"tissue_types\", \"epithelium\")</p> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>@validate_call(config=ConfigDict(arbitrary_types_allowed=True))\ndef lookup_term(self, table: str | Table, term_name: str) -&gt; VocabularyTerm:\n    \"\"\"Finds a term in a vocabulary table.\n\n    Searches for a term in the specified vocabulary table, matching either the primary name\n    or any of its synonyms.\n\n    Args:\n        table: Vocabulary table to search in (name or Table object).\n        term_name: Name or synonym of the term to find.\n\n    Returns:\n        VocabularyTerm: The matching vocabulary term.\n\n    Raises:\n        DerivaMLVocabularyException: If the table is not a vocabulary table, or term is not found.\n\n    Examples:\n        Look up by primary name:\n            &gt;&gt;&gt; term = ml.lookup_term(\"tissue_types\", \"epithelial\")\n            &gt;&gt;&gt; print(term.description)\n\n        Look up by synonym:\n            &gt;&gt;&gt; term = ml.lookup_term(\"tissue_types\", \"epithelium\")\n    \"\"\"\n    # Get and validate vocabulary table reference\n    vocab_table = self.model.name_to_table(table)\n    if not self.model.is_vocabulary(vocab_table):\n        raise DerivaMLException(f\"The table {table} is not a controlled vocabulary\")\n\n    # Get schema and table paths\n    schema_name, table_name = vocab_table.schema.name, vocab_table.name\n    schema_path = self.catalog.getPathBuilder().schemas[schema_name]\n\n    # Search for term by name or synonym\n    for term in schema_path.tables[table_name].entities().fetch():\n        if term_name == term[\"Name\"] or (term[\"Synonyms\"] and term_name in term[\"Synonyms\"]):\n            return VocabularyTerm.model_validate(term)\n\n    # Term not found\n    raise DerivaMLInvalidTerm(table_name, term_name)\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.lookup_workflow","title":"lookup_workflow","text":"<pre><code>lookup_workflow(\n    url_or_checksum: str,\n) -&gt; RID | None\n</code></pre> <p>Finds a workflow by URL.</p> <p>Parameters:</p> Name Type Description Default <code>url_or_checksum</code> <code>str</code> <p>URL or checksum of the workflow.</p> required <p>Returns:     RID: Resource Identifier of the workflow if found, None otherwise.</p> Example <p>rid = ml.lookup_workflow(\"https://github.com/org/repo/workflow.py\") if rid: ...     print(f\"Found workflow: {rid}\")</p> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>def lookup_workflow(self, url_or_checksum: str) -&gt; RID | None:\n    \"\"\"Finds a workflow by URL.\n\n    Args:\n        url_or_checksum: URL or checksum of the workflow.\n    Returns:\n        RID: Resource Identifier of the workflow if found, None otherwise.\n\n    Example:\n        &gt;&gt;&gt; rid = ml.lookup_workflow(\"https://github.com/org/repo/workflow.py\")\n        &gt;&gt;&gt; if rid:\n        ...     print(f\"Found workflow: {rid}\")\n    \"\"\"\n    # Get a workflow table path\n    workflow_path = self.pathBuilder.schemas[self.ml_schema].Workflow\n    try:\n        # Search for workflow by URL\n        url_column = workflow_path.URL\n        checksum_column = workflow_path.Checksum\n        return list(\n            workflow_path.path.filter(\n                (url_column == url_or_checksum) | (checksum_column == url_or_checksum)\n            ).entities()\n        )[0][\"RID\"]\n    except IndexError:\n        return None\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.resolve_rid","title":"resolve_rid","text":"<pre><code>resolve_rid(\n    rid: RID,\n) -&gt; ResolveRidResult\n</code></pre> <p>Resolves RID to catalog location.</p> <p>Looks up a RID and returns information about where it exists in the catalog, including schema, table, and column metadata.</p> <p>Parameters:</p> Name Type Description Default <code>rid</code> <code>RID</code> <p>Resource Identifier to resolve.</p> required <p>Returns:</p> Name Type Description <code>ResolveRidResult</code> <code>ResolveRidResult</code> <p>Named tuple containing: - schema: Schema name - table: Table name - columns: Column definitions - datapath: Path builder for accessing the entity</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If RID doesn't exist in catalog.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = ml.resolve_rid(\"1-abc123\")\n&gt;&gt;&gt; print(f\"Found in {result.schema}.{result.table}\")\n&gt;&gt;&gt; data = result.datapath.entities().fetch()\n</code></pre> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>def resolve_rid(self, rid: RID) -&gt; ResolveRidResult:\n    \"\"\"Resolves RID to catalog location.\n\n    Looks up a RID and returns information about where it exists in the catalog, including schema,\n    table, and column metadata.\n\n    Args:\n        rid: Resource Identifier to resolve.\n\n    Returns:\n        ResolveRidResult: Named tuple containing:\n            - schema: Schema name\n            - table: Table name\n            - columns: Column definitions\n            - datapath: Path builder for accessing the entity\n\n    Raises:\n        DerivaMLException: If RID doesn't exist in catalog.\n\n    Examples:\n        &gt;&gt;&gt; result = ml.resolve_rid(\"1-abc123\")\n        &gt;&gt;&gt; print(f\"Found in {result.schema}.{result.table}\")\n        &gt;&gt;&gt; data = result.datapath.entities().fetch()\n    \"\"\"\n    try:\n        # Attempt to resolve RID using catalog model\n        return self.catalog.resolve_rid(rid, self.model.model)\n    except KeyError as _e:\n        raise DerivaMLException(f\"Invalid RID {rid}\")\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.restore_execution","title":"restore_execution","text":"<pre><code>restore_execution(\n    execution_rid: RID | None = None,\n) -&gt; Execution\n</code></pre> <p>Restores a previous execution.</p> <p>Given an execution RID, retrieves the execution configuration and restores the local compute environment. This routine has a number of side effects.</p> <ol> <li> <p>The datasets specified in the configuration are downloaded and placed in the cache-dir. If a version is not specified in the configuration, then a new minor version number is created for the dataset and downloaded.</p> </li> <li> <p>If any execution assets are provided in the configuration, they are downloaded and placed in the working directory.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>execution_rid</code> <code>RID | None</code> <p>Resource Identifier (RID) of the execution to restore.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Execution</code> <code>Execution</code> <p>An execution object representing the restored execution environment.</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If execution_rid is not valid or execution cannot be restored.</p> Example <p>execution = ml.restore_execution(\"1-abc123\")</p> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>def restore_execution(self, execution_rid: RID | None = None) -&gt; Execution:\n    \"\"\"Restores a previous execution.\n\n    Given an execution RID, retrieves the execution configuration and restores the local compute environment.\n    This routine has a number of side effects.\n\n    1. The datasets specified in the configuration are downloaded and placed in the cache-dir. If a version is\n    not specified in the configuration, then a new minor version number is created for the dataset and downloaded.\n\n    2. If any execution assets are provided in the configuration, they are downloaded and placed\n    in the working directory.\n\n    Args:\n        execution_rid: Resource Identifier (RID) of the execution to restore.\n\n    Returns:\n        Execution: An execution object representing the restored execution environment.\n\n    Raises:\n        DerivaMLException: If execution_rid is not valid or execution cannot be restored.\n\n    Example:\n        &gt;&gt;&gt; execution = ml.restore_execution(\"1-abc123\")\n    \"\"\"\n    # Import here to avoid circular dependency\n    from deriva_ml.execution.execution import Execution\n\n    # If no RID provided, try to find single execution in working directory\n    if not execution_rid:\n        e_rids = execution_rids(self.working_dir)\n        if len(e_rids) != 1:\n            raise DerivaMLException(f\"Multiple execution RIDs were found {e_rids}.\")\n        execution_rid = e_rids[0]\n\n    # Try to load configuration from a file\n    cfile = asset_file_path(\n        prefix=self.working_dir,\n        exec_rid=execution_rid,\n        file_name=\"configuration.json\",\n        asset_table=self.model.name_to_table(\"Execution_Metadata\"),\n        metadata={},\n    )\n\n    # Load configuration from a file or create from an execution record\n    if cfile.exists():\n        configuration = ExecutionConfiguration.load_configuration(cfile)\n    else:\n        execution = self.retrieve_rid(execution_rid)\n        configuration = ExecutionConfiguration(\n            workflow=execution[\"Workflow\"],\n            description=execution[\"Description\"],\n        )\n\n    # Create and return an execution instance\n    return Execution(configuration, self, reload=execution_rid)\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.retrieve_rid","title":"retrieve_rid","text":"<pre><code>retrieve_rid(\n    rid: RID,\n) -&gt; dict[str, Any]\n</code></pre> <p>Retrieves complete record for RID.</p> <p>Fetches all column values for the entity identified by the RID.</p> <p>Parameters:</p> Name Type Description Default <code>rid</code> <code>RID</code> <p>Resource Identifier of the record to retrieve.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: Dictionary containing all column values for the entity.</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If the RID doesn't exist in the catalog.</p> Example <p>record = ml.retrieve_rid(\"1-abc123\") print(f\"Name: {record['name']}, Created: {record['creation_date']}\")</p> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>def retrieve_rid(self, rid: RID) -&gt; dict[str, Any]:\n    \"\"\"Retrieves complete record for RID.\n\n    Fetches all column values for the entity identified by the RID.\n\n    Args:\n        rid: Resource Identifier of the record to retrieve.\n\n    Returns:\n        dict[str, Any]: Dictionary containing all column values for the entity.\n\n    Raises:\n        DerivaMLException: If the RID doesn't exist in the catalog.\n\n    Example:\n        &gt;&gt;&gt; record = ml.retrieve_rid(\"1-abc123\")\n        &gt;&gt;&gt; print(f\"Name: {record['name']}, Created: {record['creation_date']}\")\n    \"\"\"\n    # Resolve RID and fetch the first (only) matching record\n    return self.resolve_rid(rid).datapath.entities().fetch()[0]\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.table_path","title":"table_path","text":"<pre><code>table_path(table: str | Table) -&gt; Path\n</code></pre> <p>Returns a local filesystem path for table CSV files.</p> <p>Generates a standardized path where CSV files should be placed when preparing to upload data to a table. The path follows the project's directory structure conventions.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str | Table</code> <p>Name of the table or Table object to get the path for.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Filesystem path where the CSV file should be placed.</p> Example <p>path = ml.table_path(\"experiment_results\") df.to_csv(path) # Save data for upload</p> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>def table_path(self, table: str | Table) -&gt; Path:\n    \"\"\"Returns a local filesystem path for table CSV files.\n\n    Generates a standardized path where CSV files should be placed when preparing to upload data to a table.\n    The path follows the project's directory structure conventions.\n\n    Args:\n        table: Name of the table or Table object to get the path for.\n\n    Returns:\n        Path: Filesystem path where the CSV file should be placed.\n\n    Example:\n        &gt;&gt;&gt; path = ml.table_path(\"experiment_results\")\n        &gt;&gt;&gt; df.to_csv(path) # Save data for upload\n    \"\"\"\n    return table_path(\n        self.working_dir,\n        schema=self.domain_schema,\n        table=self.model.name_to_table(table).name,\n    )\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaML.user_list","title":"user_list","text":"<pre><code>user_list() -&gt; List[Dict[str, str]]\n</code></pre> <p>Returns catalog user list.</p> <p>Retrieves basic information about all users who have access to the catalog, including their identifiers and full names.</p> <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List[Dict[str, str]]: List of user information dictionaries, each containing: - 'ID': User identifier - 'Full_Name': User's full name</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; users = ml.user_list()\n&gt;&gt;&gt; for user in users:\n...     print(f\"{user['Full_Name']} ({user['ID']})\")\n</code></pre> Source code in <code>src/deriva_ml/core/base.py</code> <pre><code>def user_list(self) -&gt; List[Dict[str, str]]:\n    \"\"\"Returns catalog user list.\n\n    Retrieves basic information about all users who have access to the catalog, including their\n    identifiers and full names.\n\n    Returns:\n        List[Dict[str, str]]: List of user information dictionaries, each containing:\n            - 'ID': User identifier\n            - 'Full_Name': User's full name\n\n    Examples:\n\n        &gt;&gt;&gt; users = ml.user_list()\n        &gt;&gt;&gt; for user in users:\n        ...     print(f\"{user['Full_Name']} ({user['ID']})\")\n    \"\"\"\n    # Get the user table path and fetch basic user info\n    user_path = self.pathBuilder.public.ERMrest_Client.path\n    return [{\"ID\": u[\"ID\"], \"Full_Name\": u[\"Full_Name\"]} for u in user_path.entities().fetch()]\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaMLException","title":"DerivaMLException","text":"<p>               Bases: <code>Exception</code></p> <p>Exception class specific to DerivaML module.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>Optional message for the exception.</p> <code>''</code> Source code in <code>src/deriva_ml/core/exceptions.py</code> <pre><code>class DerivaMLException(Exception):\n    \"\"\"Exception class specific to DerivaML module.\n\n    Args:\n        msg (str): Optional message for the exception.\n    \"\"\"\n\n    def __init__(self, msg=\"\"):\n        super().__init__(msg)\n        self._msg = msg\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaMLInvalidTerm","title":"DerivaMLInvalidTerm","text":"<p>               Bases: <code>DerivaMLException</code></p> <p>Exception class for invalid terms in DerivaML controlled vocabulary.</p> Source code in <code>src/deriva_ml/core/exceptions.py</code> <pre><code>class DerivaMLInvalidTerm(DerivaMLException):\n    \"\"\"Exception class for invalid terms in DerivaML controlled vocabulary.\"\"\"\n    def __init__(self, vocabulary, term: str, msg: str = \"Term doesn't exist\"):\n        \"\"\"Exception indicating undefined term type\"\"\"\n        super().__init__(f\"Invalid term {term} in vocabulary {vocabulary}: {msg}.\")\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaMLInvalidTerm.__init__","title":"__init__","text":"<pre><code>__init__(\n    vocabulary,\n    term: str,\n    msg: str = \"Term doesn't exist\",\n)\n</code></pre> <p>Exception indicating undefined term type</p> Source code in <code>src/deriva_ml/core/exceptions.py</code> <pre><code>def __init__(self, vocabulary, term: str, msg: str = \"Term doesn't exist\"):\n    \"\"\"Exception indicating undefined term type\"\"\"\n    super().__init__(f\"Invalid term {term} in vocabulary {vocabulary}: {msg}.\")\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaMLTableTypeError","title":"DerivaMLTableTypeError","text":"<p>               Bases: <code>DerivaMLException</code></p> <p>RID for table is not of correct type.</p> Source code in <code>src/deriva_ml/core/exceptions.py</code> <pre><code>class DerivaMLTableTypeError(DerivaMLException):\n    \"\"\"RID for table is not of correct type.\"\"\"\n    def __init__(self, table_type, table: str):\n        \"\"\"Exception indicating undefined term type\"\"\"\n        super().__init__(f\"Table  {table} is not of type {table_type}.\")\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.DerivaMLTableTypeError.__init__","title":"__init__","text":"<pre><code>__init__(table_type, table: str)\n</code></pre> <p>Exception indicating undefined term type</p> Source code in <code>src/deriva_ml/core/exceptions.py</code> <pre><code>def __init__(self, table_type, table: str):\n    \"\"\"Exception indicating undefined term type\"\"\"\n    super().__init__(f\"Table  {table} is not of type {table_type}.\")\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.ExecAssetType","title":"ExecAssetType","text":"<p>               Bases: <code>BaseStrEnum</code></p> <p>Execution asset type identifiers.</p> <p>Defines the types of assets that can be produced during an execution.</p> <p>Attributes:</p> Name Type Description <code>input_file</code> <code>str</code> <p>Input file used by the execution.</p> <code>output_file</code> <code>str</code> <p>Output file produced by the execution.</p> <code>notebook_output</code> <code>str</code> <p>Jupyter notebook output from the execution.</p> Source code in <code>src/deriva_ml/core/enums.py</code> <pre><code>class ExecAssetType(BaseStrEnum):\n    \"\"\"Execution asset type identifiers.\n\n    Defines the types of assets that can be produced during an execution.\n\n    Attributes:\n        input_file (str): Input file used by the execution.\n        output_file (str): Output file produced by the execution.\n        notebook_output (str): Jupyter notebook output from the execution.\n    \"\"\"\n\n    input_file = \"Input_File\"\n    output_file = \"Output_File\"\n    notebook_output = \"Notebook_Output\"\n    model_file = \"Model_File\"\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.ExecMetadataType","title":"ExecMetadataType","text":"<p>               Bases: <code>BaseStrEnum</code></p> <p>Execution metadata type identifiers.</p> <p>Defines the types of metadata that can be associated with an execution.</p> <p>Attributes:</p> Name Type Description <code>execution_config</code> <code>str</code> <p>Execution configuration data.</p> <code>runtime_env</code> <code>str</code> <p>Runtime environment information.</p> Source code in <code>src/deriva_ml/core/enums.py</code> <pre><code>class ExecMetadataType(BaseStrEnum):\n    \"\"\"Execution metadata type identifiers.\n\n    Defines the types of metadata that can be associated with an execution.\n\n    Attributes:\n        execution_config (str): Execution configuration data.\n        runtime_env (str): Runtime environment information.\n    \"\"\"\n\n    execution_config = \"Execution_Config\"\n    runtime_env = \"Runtime_Env\"\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.FileSpec","title":"FileSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>An entry into the File table</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The File url to the url.</p> <code>description</code> <code>str | None</code> <p>The description of the file.</p> <code>md5</code> <code>str</code> <p>The MD5 hash of the file.</p> <code>length</code> <code>int</code> <p>The length of the file in bytes.</p> <code>file_types</code> <code>conlist(str) | None</code> <p>A list of file types.  Each files_type should be a defined term in MLVocab.file_type vocabulary.</p> Source code in <code>src/deriva_ml/core/filespec.py</code> <pre><code>class FileSpec(BaseModel):\n    \"\"\"An entry into the File table\n\n    Attributes:\n        url: The File url to the url.\n        description: The description of the file.\n        md5: The MD5 hash of the file.\n        length: The length of the file in bytes.\n        file_types: A list of file types.  Each files_type should be a defined term in MLVocab.file_type vocabulary.\n    \"\"\"\n\n    url: str = Field(alias=\"URL\", validation_alias=\"url\")\n    md5: str = Field(alias=\"MD5\", validation_alias=\"md5\")\n    length: int = Field(alias=\"Length\", validation_alias=\"length\")\n    description: str | None = Field(default=\"\", alias=\"Description\", validation_alias=\"description\")\n    file_types: conlist(str) | None = []\n\n    @field_validator(\"url\")\n    @classmethod\n    def validate_file_url(cls, url: str) -&gt; str:\n        \"\"\"Examine the provided URL. If it's a local path, convert it into a tag URL.\n\n        Args:\n            url: The URL to validate and potentially convert\n\n        Returns:\n            The validated/converted URL\n\n        Raises:\n            ValidationError: If the URL is not a file URL\n        \"\"\"\n        url_parts = urlparse(url)\n        if url_parts.scheme == \"tag\":\n            # Already a tag URL, so just return it.\n            return url\n        elif (not url_parts.scheme) or url_parts.scheme == \"file\":\n            # There is no scheme part of the URL, or it is a file URL, so it is a local file path.\n            # Convert to a tag URL.\n            return f\"tag://{gethostname()},{date.today()}:file://{url_parts.path}\"\n        else:\n            raise ValueError(\"url is not a file URL\")\n\n    @classmethod\n    def create_filespecs(\n        cls, path: Path | str, description: str, file_types: list[str] | Callable[[Path], list[str]] | None = None\n    ) -&gt; Generator[FileSpec, None, None]:\n        \"\"\"Given a file or directory, generate the sequence of corresponding FileSpecs suitable to create a File table.\n\n        Args:\n            path: Path to the file or directory.\n            description: The description of the file(s)\n            file_types: A list of file types or a function that takes a file path and returns a list of file types.\n\n        Returns:\n            An iterable of FileSpecs for each file in the directory.\n        \"\"\"\n\n        path = Path(path)\n        file_types = file_types or []\n        file_types_fn = file_types if callable(file_types) else lambda _x: file_types\n\n        def create_spec(file_path: Path) -&gt; FileSpec:\n            hashes = hash_utils.compute_file_hashes(file_path, hashes=frozenset([\"md5\", \"sha256\"]))\n            md5 = hashes[\"md5\"][0]\n            type_list = file_types_fn(file_path)\n            return FileSpec(\n                length=path.stat().st_size,\n                md5=md5,\n                description=description,\n                url=file_path.as_posix(),\n                file_types=type_list if \"File\" in type_list else [\"File\"] + type_list,\n            )\n\n        files = [path] if path.is_file() else [f for f in Path(path).rglob(\"*\") if f.is_file()]\n        return (create_spec(file) for file in files)\n\n    @staticmethod\n    def read_filespec(path: Path | str) -&gt; Generator[FileSpec, None, None]:\n        \"\"\"Get FileSpecs from a JSON lines file.\n\n        Args:\n         path: Path to the .jsonl file (string or Path).\n\n        Yields:\n             A FileSpec object.\n        \"\"\"\n        path = Path(path)\n        with path.open(\"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                yield FileSpec(**json.loads(line))\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.FileSpec.create_filespecs","title":"create_filespecs  <code>classmethod</code>","text":"<pre><code>create_filespecs(\n    path: Path | str,\n    description: str,\n    file_types: list[str]\n    | Callable[[Path], list[str]]\n    | None = None,\n) -&gt; Generator[FileSpec, None, None]\n</code></pre> <p>Given a file or directory, generate the sequence of corresponding FileSpecs suitable to create a File table.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>Path to the file or directory.</p> required <code>description</code> <code>str</code> <p>The description of the file(s)</p> required <code>file_types</code> <code>list[str] | Callable[[Path], list[str]] | None</code> <p>A list of file types or a function that takes a file path and returns a list of file types.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>An iterable of FileSpecs for each file in the directory.</p> Source code in <code>src/deriva_ml/core/filespec.py</code> <pre><code>@classmethod\ndef create_filespecs(\n    cls, path: Path | str, description: str, file_types: list[str] | Callable[[Path], list[str]] | None = None\n) -&gt; Generator[FileSpec, None, None]:\n    \"\"\"Given a file or directory, generate the sequence of corresponding FileSpecs suitable to create a File table.\n\n    Args:\n        path: Path to the file or directory.\n        description: The description of the file(s)\n        file_types: A list of file types or a function that takes a file path and returns a list of file types.\n\n    Returns:\n        An iterable of FileSpecs for each file in the directory.\n    \"\"\"\n\n    path = Path(path)\n    file_types = file_types or []\n    file_types_fn = file_types if callable(file_types) else lambda _x: file_types\n\n    def create_spec(file_path: Path) -&gt; FileSpec:\n        hashes = hash_utils.compute_file_hashes(file_path, hashes=frozenset([\"md5\", \"sha256\"]))\n        md5 = hashes[\"md5\"][0]\n        type_list = file_types_fn(file_path)\n        return FileSpec(\n            length=path.stat().st_size,\n            md5=md5,\n            description=description,\n            url=file_path.as_posix(),\n            file_types=type_list if \"File\" in type_list else [\"File\"] + type_list,\n        )\n\n    files = [path] if path.is_file() else [f for f in Path(path).rglob(\"*\") if f.is_file()]\n    return (create_spec(file) for file in files)\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.FileSpec.read_filespec","title":"read_filespec  <code>staticmethod</code>","text":"<pre><code>read_filespec(\n    path: Path | str,\n) -&gt; Generator[FileSpec, None, None]\n</code></pre> <p>Get FileSpecs from a JSON lines file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>Path to the .jsonl file (string or Path).</p> required <p>Yields:</p> Type Description <code>FileSpec</code> <p>A FileSpec object.</p> Source code in <code>src/deriva_ml/core/filespec.py</code> <pre><code>@staticmethod\ndef read_filespec(path: Path | str) -&gt; Generator[FileSpec, None, None]:\n    \"\"\"Get FileSpecs from a JSON lines file.\n\n    Args:\n     path: Path to the .jsonl file (string or Path).\n\n    Yields:\n         A FileSpec object.\n    \"\"\"\n    path = Path(path)\n    with path.open(\"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            yield FileSpec(**json.loads(line))\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.FileSpec.validate_file_url","title":"validate_file_url  <code>classmethod</code>","text":"<pre><code>validate_file_url(url: str) -&gt; str\n</code></pre> <p>Examine the provided URL. If it's a local path, convert it into a tag URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to validate and potentially convert</p> required <p>Returns:</p> Type Description <code>str</code> <p>The validated/converted URL</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If the URL is not a file URL</p> Source code in <code>src/deriva_ml/core/filespec.py</code> <pre><code>@field_validator(\"url\")\n@classmethod\ndef validate_file_url(cls, url: str) -&gt; str:\n    \"\"\"Examine the provided URL. If it's a local path, convert it into a tag URL.\n\n    Args:\n        url: The URL to validate and potentially convert\n\n    Returns:\n        The validated/converted URL\n\n    Raises:\n        ValidationError: If the URL is not a file URL\n    \"\"\"\n    url_parts = urlparse(url)\n    if url_parts.scheme == \"tag\":\n        # Already a tag URL, so just return it.\n        return url\n    elif (not url_parts.scheme) or url_parts.scheme == \"file\":\n        # There is no scheme part of the URL, or it is a file URL, so it is a local file path.\n        # Convert to a tag URL.\n        return f\"tag://{gethostname()},{date.today()}:file://{url_parts.path}\"\n    else:\n        raise ValueError(\"url is not a file URL\")\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.FileUploadState","title":"FileUploadState","text":"<p>               Bases: <code>BaseModel</code></p> <p>Tracks the state and result of a file upload operation.</p> <p>Attributes:</p> Name Type Description <code>state</code> <code>UploadState</code> <p>Current state of the upload (success, failed, etc.).</p> <code>status</code> <code>str</code> <p>Detailed status message.</p> <code>result</code> <code>Any</code> <p>Upload result data, if any.</p> <code>rid</code> <code>RID | None</code> <p>Resource identifier of the uploaded file, if successful.</p> Source code in <code>src/deriva_ml/core/ermrest.py</code> <pre><code>class FileUploadState(BaseModel):\n    \"\"\"Tracks the state and result of a file upload operation.\n\n    Attributes:\n        state (UploadState): Current state of the upload (success, failed, etc.).\n        status (str): Detailed status message.\n        result (Any): Upload result data, if any.\n        rid (RID | None): Resource identifier of the uploaded file, if successful.\n    \"\"\"\n    state: UploadState\n    status: str\n    result: Any\n\n    @computed_field\n    @property\n    def rid(self) -&gt; RID | None:\n        return self.result and self.result[\"RID\"]\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.MLAsset","title":"MLAsset","text":"<p>               Bases: <code>BaseStrEnum</code></p> <p>Asset type identifiers.</p> <p>Defines the types of assets that can be associated with executions.</p> <p>Attributes:</p> Name Type Description <code>execution_metadata</code> <code>str</code> <p>Metadata about an execution.</p> <code>execution_asset</code> <code>str</code> <p>Asset produced by an execution.</p> Source code in <code>src/deriva_ml/core/enums.py</code> <pre><code>class MLAsset(BaseStrEnum):\n    \"\"\"Asset type identifiers.\n\n    Defines the types of assets that can be associated with executions.\n\n    Attributes:\n        execution_metadata (str): Metadata about an execution.\n        execution_asset (str): Asset produced by an execution.\n    \"\"\"\n\n    execution_metadata = \"Execution_Metadata\"\n    execution_asset = \"Execution_Asset\"\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.MLVocab","title":"MLVocab","text":"<p>               Bases: <code>BaseStrEnum</code></p> <p>Controlled vocabulary type identifiers.</p> <p>Defines the names of controlled vocabulary tables used in DerivaML for various types of entities and attributes.</p> <p>Attributes:</p> Name Type Description <code>dataset_type</code> <code>str</code> <p>Dataset classification vocabulary.</p> <code>workflow_type</code> <code>str</code> <p>Workflow classification vocabulary.</p> <code>asset_type</code> <code>str</code> <p>Asset classification vocabulary.</p> <code>asset_role</code> <code>str</code> <p>Asset role classification vocabulary.</p> Source code in <code>src/deriva_ml/core/enums.py</code> <pre><code>class MLVocab(BaseStrEnum):\n    \"\"\"Controlled vocabulary type identifiers.\n\n    Defines the names of controlled vocabulary tables used in DerivaML for various types\n    of entities and attributes.\n\n    Attributes:\n        dataset_type (str): Dataset classification vocabulary.\n        workflow_type (str): Workflow classification vocabulary.\n        asset_type (str): Asset classification vocabulary.\n        asset_role (str): Asset role classification vocabulary.\n    \"\"\"\n\n    dataset_type = \"Dataset_Type\"\n    workflow_type = \"Workflow_Type\"\n    asset_type = \"Asset_Type\"\n    asset_role = \"Asset_Role\"\n    feature_name = \"Feature_Name\"\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.TableDefinition","title":"TableDefinition","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines a complete table structure in ERMrest.</p> <p>Provides a Pydantic model for defining tables with their columns, keys, and relationships. Maps to deriva_py's Table.define functionality.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the table.</p> <code>column_defs</code> <code>Iterable[ColumnDefinition]</code> <p>Column definitions.</p> <code>key_defs</code> <code>Iterable[KeyDefinition]</code> <p>Key constraint definitions.</p> <code>fkey_defs</code> <code>Iterable[ForeignKeyDefinition]</code> <p>Foreign key relationship definitions.</p> <code>comment</code> <code>str | None</code> <p>Description of the table's purpose.</p> <code>acls</code> <code>dict</code> <p>Access control lists.</p> <code>acl_bindings</code> <code>dict</code> <p>Dynamic access control bindings.</p> <code>annotations</code> <code>dict</code> <p>Additional metadata annotations.</p> Example <p>table = TableDefinition( ...     name=\"experiment\", ...     column_defs=[ ...         ColumnDefinition(name=\"id\", type=BuiltinTypes.text), ...         ColumnDefinition(name=\"date\", type=BuiltinTypes.date) ...     ], ...     comment=\"Experimental data records\" ... )</p> Source code in <code>src/deriva_ml/core/ermrest.py</code> <pre><code>class TableDefinition(BaseModel):\n    \"\"\"Defines a complete table structure in ERMrest.\n\n    Provides a Pydantic model for defining tables with their columns, keys, and relationships.\n    Maps to deriva_py's Table.define functionality.\n\n    Attributes:\n        name (str): Name of the table.\n        column_defs (Iterable[ColumnDefinition]): Column definitions.\n        key_defs (Iterable[KeyDefinition]): Key constraint definitions.\n        fkey_defs (Iterable[ForeignKeyDefinition]): Foreign key relationship definitions.\n        comment (str | None): Description of the table's purpose.\n        acls (dict): Access control lists.\n        acl_bindings (dict): Dynamic access control bindings.\n        annotations (dict): Additional metadata annotations.\n\n    Example:\n        &gt;&gt;&gt; table = TableDefinition(\n        ...     name=\"experiment\",\n        ...     column_defs=[\n        ...         ColumnDefinition(name=\"id\", type=BuiltinTypes.text),\n        ...         ColumnDefinition(name=\"date\", type=BuiltinTypes.date)\n        ...     ],\n        ...     comment=\"Experimental data records\"\n        ... )\n    \"\"\"\n    name: str\n    column_defs: Iterable[ColumnDefinition]\n    key_defs: Iterable[KeyDefinition] = Field(default_factory=list)\n    fkey_defs: Iterable[ForeignKeyDefinition] = Field(default_factory=list)\n    comment: str | None = None\n    acls: dict = Field(default_factory=dict)\n    acl_bindings: dict = Field(default_factory=dict)\n    annotations: dict = Field(default_factory=dict)\n\n    @model_serializer()\n    def serialize_table_definition(self):\n        return em.Table.define(\n            tname=self.name,\n            column_defs=[c.model_dump() for c in self.column_defs],\n            key_defs=[k.model_dump() for k in self.key_defs],\n            fkey_defs=[fk.model_dump() for fk in self.fkey_defs],\n            comment=self.comment,\n            acls=self.acls,\n            acl_bindings=self.acl_bindings,\n            annotations=self.annotations,\n        )\n</code></pre>"},{"location":"code-docs/deriva_ml_base/#deriva_ml.core.UploadState","title":"UploadState","text":"<p>               Bases: <code>Enum</code></p> <p>File upload operation states.</p> <p>Represents the various states a file upload operation can be in, from initiation to completion.</p> <p>Attributes:</p> Name Type Description <code>success</code> <code>int</code> <p>Upload completed successfully.</p> <code>failed</code> <code>int</code> <p>Upload failed.</p> <code>pending</code> <code>int</code> <p>Upload is queued.</p> <code>running</code> <code>int</code> <p>Upload is in progress.</p> <code>paused</code> <code>int</code> <p>Upload is temporarily paused.</p> <code>aborted</code> <code>int</code> <p>Upload was aborted.</p> <code>cancelled</code> <code>int</code> <p>Upload was cancelled.</p> <code>timeout</code> <code>int</code> <p>Upload timed out.</p> Source code in <code>src/deriva_ml/core/enums.py</code> <pre><code>class UploadState(Enum):\n    \"\"\"File upload operation states.\n\n    Represents the various states a file upload operation can be in, from initiation to completion.\n\n    Attributes:\n        success (int): Upload completed successfully.\n        failed (int): Upload failed.\n        pending (int): Upload is queued.\n        running (int): Upload is in progress.\n        paused (int): Upload is temporarily paused.\n        aborted (int): Upload was aborted.\n        cancelled (int): Upload was cancelled.\n        timeout (int): Upload timed out.\n    \"\"\"\n\n    success = 0\n    failed = 1\n    pending = 2\n    running = 3\n    paused = 4\n    aborted = 5\n    cancelled = 6\n    timeout = 7\n</code></pre>"},{"location":"code-docs/deriva_model/","title":"DerivaModel","text":""},{"location":"code-docs/execution/","title":"Documentation for Execution class in DerivaML","text":""},{"location":"code-docs/execution_configuration/","title":"ExecutionConfiguration class","text":"<p>Configuration management for DerivaML executions.</p> <p>This module provides functionality for configuring and managing execution parameters in DerivaML. It includes:</p> <ul> <li>ExecutionConfiguration class: Core class for execution settings</li> <li>Parameter validation: Handles JSON and file-based parameters</li> <li>Dataset specifications: Manages dataset versions and materialization</li> <li>Asset management: Tracks required input files</li> </ul> <p>The module supports both direct parameter specification and JSON-based configuration files.</p> Typical usage example <p>config = ExecutionConfiguration( ...     workflow=\"analysis_workflow\", ...     datasets=[DatasetSpec(rid=\"1-abc123\", version=\"1.0.0\")], ...     parameters={\"threshold\": 0.5}, ...     description=\"Process sample data\" ... ) execution = ml.create_execution(config)</p>"},{"location":"code-docs/execution_configuration/#deriva_ml.execution.execution_configuration.ExecutionConfiguration","title":"ExecutionConfiguration","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for a DerivaML execution.</p> <p>Defines the complete configuration for a computational or manual process in DerivaML, including required datasets, input assets, workflow definition, and parameters.</p> <p>Attributes:</p> Name Type Description <code>datasets</code> <code>list[DatasetSpec]</code> <p>Dataset specifications, each containing: - rid: Dataset Resource Identifier - version: Version to use - materialize: Whether to extract dataset contents</p> <code>assets</code> <code>list[RID]</code> <p>Resource Identifiers of required input assets.</p> <code>workflow</code> <code>RID | Workflow</code> <p>Workflow definition or its Resource Identifier.</p> <code>parameters</code> <code>dict[str, Any] | Path</code> <p>Execution parameters, either as: - Dictionary of parameter values - Path to JSON file containing parameters</p> <code>description</code> <code>str</code> <p>Description of execution purpose (supports Markdown).</p> <code>argv</code> <code>list[str]</code> <p>Command line arguments used to start execution.</p> Example <p>config = ExecutionConfiguration( ...     workflow=Workflow.create_workflow(\"analysis\", \"python_script\"), ...     datasets=[ ...         DatasetSpec(rid=\"1-abc123\", version=\"1.0.0\", materialize=True) ...     ], ...     parameters={\"threshold\": 0.5, \"max_iterations\": 100}, ...     description=\"Process RNA sequence data\" ... )</p> Source code in <code>src/deriva_ml/execution/execution_configuration.py</code> <pre><code>class ExecutionConfiguration(BaseModel):\n    \"\"\"Configuration for a DerivaML execution.\n\n    Defines the complete configuration for a computational or manual process in DerivaML,\n    including required datasets, input assets, workflow definition, and parameters.\n\n    Attributes:\n        datasets (list[DatasetSpec]): Dataset specifications, each containing:\n            - rid: Dataset Resource Identifier\n            - version: Version to use\n            - materialize: Whether to extract dataset contents\n        assets (list[RID]): Resource Identifiers of required input assets.\n        workflow (RID | Workflow): Workflow definition or its Resource Identifier.\n        parameters (dict[str, Any] | Path): Execution parameters, either as:\n            - Dictionary of parameter values\n            - Path to JSON file containing parameters\n        description (str): Description of execution purpose (supports Markdown).\n        argv (list[str]): Command line arguments used to start execution.\n\n    Example:\n        &gt;&gt;&gt; config = ExecutionConfiguration(\n        ...     workflow=Workflow.create_workflow(\"analysis\", \"python_script\"),\n        ...     datasets=[\n        ...         DatasetSpec(rid=\"1-abc123\", version=\"1.0.0\", materialize=True)\n        ...     ],\n        ...     parameters={\"threshold\": 0.5, \"max_iterations\": 100},\n        ...     description=\"Process RNA sequence data\"\n        ... )\n    \"\"\"\n\n    datasets: list[DatasetSpec] = []\n    assets: list[RID] = []\n    workflow: RID | Workflow\n    parameters: dict[str, Any] | Path = {}\n    description: str = \"\"\n    argv: list[str] = Field(default_factory=lambda: sys.argv)\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @field_validator(\"parameters\", mode=\"before\")\n    @classmethod\n    def validate_parameters(cls, value: Any) -&gt; Any:\n        \"\"\"Validates and loads execution parameters.\n\n        If value is a file path, loads and parses it as JSON. Otherwise, returns\n        the value as is.\n\n        Args:\n            value: Parameter value to validate, either:\n                - Dictionary of parameters\n                - Path to JSON file\n                - String path to JSON file\n\n        Returns:\n            dict[str, Any]: Validated parameter dictionary.\n\n        Raises:\n            ValueError: If JSON file is invalid or cannot be read.\n            FileNotFoundError: If parameter file doesn't exist.\n\n        Example:\n            &gt;&gt;&gt; config = ExecutionConfiguration(parameters=\"params.json\")\n            &gt;&gt;&gt; print(config.parameters)  # Contents of params.json as dict\n        \"\"\"\n        if isinstance(value, str) or isinstance(value, Path):\n            with Path(value).open(\"r\") as f:\n                return json.load(f)\n        else:\n            return value\n\n    @field_validator(\"workflow\", mode=\"before\")\n    @classmethod\n    def validate_workflow(cls, value: Any) -&gt; Any:\n        \"\"\"Validates workflow specification.\n\n        Args:\n            value: Workflow value to validate (RID or Workflow object).\n\n        Returns:\n            RID | Workflow: Validated workflow specification.\n        \"\"\"\n        return value\n\n    @staticmethod\n    def load_configuration(path: Path) -&gt; ExecutionConfiguration:\n        \"\"\"Creates an ExecutionConfiguration from a JSON file.\n\n        Loads and parses a JSON configuration file into an ExecutionConfiguration\n        instance. The file should contain a valid configuration specification.\n\n        Args:\n            path: Path to JSON configuration file.\n\n        Returns:\n            ExecutionConfiguration: Loaded configuration instance.\n\n        Raises:\n            ValueError: If JSON file is invalid or missing required fields.\n            FileNotFoundError: If configuration file doesn't exist.\n\n        Example:\n            &gt;&gt;&gt; config = ExecutionConfiguration.load_configuration(Path(\"config.json\"))\n            &gt;&gt;&gt; print(f\"Workflow: {config.workflow}\")\n            &gt;&gt;&gt; print(f\"Datasets: {len(config.datasets)}\")\n        \"\"\"\n        with Path(path).open() as fd:\n            config = json.load(fd)\n        return ExecutionConfiguration.model_validate(config)\n</code></pre>"},{"location":"code-docs/execution_configuration/#deriva_ml.execution.execution_configuration.ExecutionConfiguration.load_configuration","title":"load_configuration  <code>staticmethod</code>","text":"<pre><code>load_configuration(\n    path: Path,\n) -&gt; ExecutionConfiguration\n</code></pre> <p>Creates an ExecutionConfiguration from a JSON file.</p> <p>Loads and parses a JSON configuration file into an ExecutionConfiguration instance. The file should contain a valid configuration specification.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to JSON configuration file.</p> required <p>Returns:</p> Name Type Description <code>ExecutionConfiguration</code> <code>ExecutionConfiguration</code> <p>Loaded configuration instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If JSON file is invalid or missing required fields.</p> <code>FileNotFoundError</code> <p>If configuration file doesn't exist.</p> Example <p>config = ExecutionConfiguration.load_configuration(Path(\"config.json\")) print(f\"Workflow: {config.workflow}\") print(f\"Datasets: {len(config.datasets)}\")</p> Source code in <code>src/deriva_ml/execution/execution_configuration.py</code> <pre><code>@staticmethod\ndef load_configuration(path: Path) -&gt; ExecutionConfiguration:\n    \"\"\"Creates an ExecutionConfiguration from a JSON file.\n\n    Loads and parses a JSON configuration file into an ExecutionConfiguration\n    instance. The file should contain a valid configuration specification.\n\n    Args:\n        path: Path to JSON configuration file.\n\n    Returns:\n        ExecutionConfiguration: Loaded configuration instance.\n\n    Raises:\n        ValueError: If JSON file is invalid or missing required fields.\n        FileNotFoundError: If configuration file doesn't exist.\n\n    Example:\n        &gt;&gt;&gt; config = ExecutionConfiguration.load_configuration(Path(\"config.json\"))\n        &gt;&gt;&gt; print(f\"Workflow: {config.workflow}\")\n        &gt;&gt;&gt; print(f\"Datasets: {len(config.datasets)}\")\n    \"\"\"\n    with Path(path).open() as fd:\n        config = json.load(fd)\n    return ExecutionConfiguration.model_validate(config)\n</code></pre>"},{"location":"code-docs/execution_configuration/#deriva_ml.execution.execution_configuration.ExecutionConfiguration.validate_parameters","title":"validate_parameters  <code>classmethod</code>","text":"<pre><code>validate_parameters(value: Any) -&gt; Any\n</code></pre> <p>Validates and loads execution parameters.</p> <p>If value is a file path, loads and parses it as JSON. Otherwise, returns the value as is.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>Parameter value to validate, either: - Dictionary of parameters - Path to JSON file - String path to JSON file</p> required <p>Returns:</p> Type Description <code>Any</code> <p>dict[str, Any]: Validated parameter dictionary.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If JSON file is invalid or cannot be read.</p> <code>FileNotFoundError</code> <p>If parameter file doesn't exist.</p> Example <p>config = ExecutionConfiguration(parameters=\"params.json\") print(config.parameters)  # Contents of params.json as dict</p> Source code in <code>src/deriva_ml/execution/execution_configuration.py</code> <pre><code>@field_validator(\"parameters\", mode=\"before\")\n@classmethod\ndef validate_parameters(cls, value: Any) -&gt; Any:\n    \"\"\"Validates and loads execution parameters.\n\n    If value is a file path, loads and parses it as JSON. Otherwise, returns\n    the value as is.\n\n    Args:\n        value: Parameter value to validate, either:\n            - Dictionary of parameters\n            - Path to JSON file\n            - String path to JSON file\n\n    Returns:\n        dict[str, Any]: Validated parameter dictionary.\n\n    Raises:\n        ValueError: If JSON file is invalid or cannot be read.\n        FileNotFoundError: If parameter file doesn't exist.\n\n    Example:\n        &gt;&gt;&gt; config = ExecutionConfiguration(parameters=\"params.json\")\n        &gt;&gt;&gt; print(config.parameters)  # Contents of params.json as dict\n    \"\"\"\n    if isinstance(value, str) or isinstance(value, Path):\n        with Path(value).open(\"r\") as f:\n            return json.load(f)\n    else:\n        return value\n</code></pre>"},{"location":"code-docs/execution_configuration/#deriva_ml.execution.execution_configuration.ExecutionConfiguration.validate_workflow","title":"validate_workflow  <code>classmethod</code>","text":"<pre><code>validate_workflow(value: Any) -&gt; Any\n</code></pre> <p>Validates workflow specification.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>Workflow value to validate (RID or Workflow object).</p> required <p>Returns:</p> Type Description <code>Any</code> <p>RID | Workflow: Validated workflow specification.</p> Source code in <code>src/deriva_ml/execution/execution_configuration.py</code> <pre><code>@field_validator(\"workflow\", mode=\"before\")\n@classmethod\ndef validate_workflow(cls, value: Any) -&gt; Any:\n    \"\"\"Validates workflow specification.\n\n    Args:\n        value: Workflow value to validate (RID or Workflow object).\n\n    Returns:\n        RID | Workflow: Validated workflow specification.\n    \"\"\"\n    return value\n</code></pre>"},{"location":"code-docs/feature/","title":"Feature","text":"<p>Feature implementation for deriva-ml.</p> <p>This module provides classes for defining and managing features in deriva-ml. Features represent measurable properties or characteristics that can be associated with records in a table. The module includes:</p> <ul> <li>Feature: Main class for defining and managing features</li> <li>FeatureRecord: Base class for feature records using pydantic models</li> </ul> Typical usage example <p>feature = Feature(association_result, model) FeatureClass = feature.feature_record_class() record = FeatureClass(value=\"high\", confidence=0.95)</p>"},{"location":"code-docs/feature/#deriva_ml.feature.Feature","title":"Feature","text":"<p>Manages feature definitions and their relationships in the catalog.</p> <p>A Feature represents a measurable property or characteristic that can be associated with records in a table. Features can include asset references, controlled vocabulary terms, and custom metadata fields.</p> <p>Attributes:</p> Name Type Description <code>feature_table</code> <p>Table containing the feature implementation.</p> <code>target_table</code> <p>Table that the feature is associated with.</p> <code>feature_name</code> <p>Name of the feature (from Feature_Name column default).</p> <code>feature_columns</code> <p>Set of columns specific to this feature.</p> <code>asset_columns</code> <p>Set of columns referencing asset tables.</p> <code>term_columns</code> <p>Set of columns referencing vocabulary tables.</p> <code>value_columns</code> <p>Set of columns containing direct values.</p> Example <p>feature = Feature(association_result, model) print(f\"Feature {feature.feature_name} on {feature.target_table.name}\") print(\"Asset columns:\", [c.name for c in feature.asset_columns])</p> Source code in <code>src/deriva_ml/feature.py</code> <pre><code>class Feature:\n    \"\"\"Manages feature definitions and their relationships in the catalog.\n\n    A Feature represents a measurable property or characteristic that can be associated with records in a table.\n    Features can include asset references, controlled vocabulary terms, and custom metadata fields.\n\n    Attributes:\n        feature_table: Table containing the feature implementation.\n        target_table: Table that the feature is associated with.\n        feature_name: Name of the feature (from Feature_Name column default).\n        feature_columns: Set of columns specific to this feature.\n        asset_columns: Set of columns referencing asset tables.\n        term_columns: Set of columns referencing vocabulary tables.\n        value_columns: Set of columns containing direct values.\n\n    Example:\n        &gt;&gt;&gt; feature = Feature(association_result, model)\n        &gt;&gt;&gt; print(f\"Feature {feature.feature_name} on {feature.target_table.name}\")\n        &gt;&gt;&gt; print(\"Asset columns:\", [c.name for c in feature.asset_columns])\n    \"\"\"\n\n    def __init__(self, atable: FindAssociationResult, model: \"DerivaModel\") -&gt; None:\n        self.feature_table = atable.table\n        self.target_table = atable.self_fkey.pk_table\n        self.feature_name = atable.table.columns[\"Feature_Name\"].default\n        self._model = model\n\n        skip_columns = {\n            \"RID\",\n            \"RMB\",\n            \"RCB\",\n            \"RCT\",\n            \"RMT\",\n            \"Feature_Name\",\n            self.target_table.name,\n            \"Execution\",\n        }\n        self.feature_columns = {c for c in self.feature_table.columns if c.name not in skip_columns}\n\n        assoc_fkeys = {atable.self_fkey} | atable.other_fkeys\n\n        # Determine the role of each column in the feature outside the FK columns.\n        self.asset_columns = {\n            fk.foreign_key_columns[0]\n            for fk in self.feature_table.foreign_keys\n            if fk not in assoc_fkeys and self._model.is_asset(fk.pk_table)\n        }\n\n        self.term_columns = {\n            fk.foreign_key_columns[0]\n            for fk in self.feature_table.foreign_keys\n            if fk not in assoc_fkeys and self._model.is_vocabulary(fk.pk_table)\n        }\n\n        self.value_columns = self.feature_columns - (self.asset_columns | self.term_columns)\n\n    def feature_record_class(self) -&gt; type[FeatureRecord]:\n        \"\"\"Create a pydantic model for entries into the specified feature table\n\n        Returns:\n            A Feature class that can be used to create instances of the feature.\n        \"\"\"\n\n        def map_type(c: Column) -&gt; UnionType | Type[str] | Type[int] | Type[float]:\n            \"\"\"Maps a Deriva column type to a Python/pydantic type.\n\n            Converts ERMrest column types to appropriate Python types for use in pydantic models.\n            Special handling is provided for asset columns which can accept either strings or Path objects.\n\n            Args:\n                c: ERMrest column to map to a Python type.\n\n            Returns:\n                UnionType | Type[str] | Type[int] | Type[float]: Appropriate Python type for the column:\n                    - str | Path for asset columns\n                    - str for text columns\n                    - int for integer columns\n                    - float for floating point columns\n                    - str for all other types\n\n            Example:\n                &gt;&gt;&gt; col = Column(name=\"score\", type=\"float4\")\n                &gt;&gt;&gt; typ = map_type(col)  # Returns float\n            \"\"\"\n            if c.name in {c.name for c in self.asset_columns}:\n                return str | Path\n\n            match c.type.typename:\n                case \"text\":\n                    return str\n                case \"int2\" | \"int4\" | \"int8\":\n                    return int\n                case \"float4\" | \"float8\":\n                    return float\n                case _:\n                    return str\n\n        featureclass_name = f\"{self.target_table.name}Feature{self.feature_name}\"\n\n        # Create feature class. To do this, we must determine the python type for each column and also if the\n        # column is optional or not based on its nullability.\n        feature_columns = {\n            c.name: (\n                Optional[map_type(c)] if c.nullok else map_type(c),\n                c.default or None,\n            )\n            for c in self.feature_columns\n        } | {\n            \"Feature_Name\": (\n                str,\n                self.feature_name,\n            ),  # Set default value for Feature_Name\n            self.target_table.name: (str, ...),\n        }\n        docstring = (\n            f\"Class to capture fields in a feature {self.feature_name} on table {self.target_table}. \"\n            \"Feature columns include:\\n\"\n        )\n        docstring += \"\\n\".join([f\"    {c.name}\" for c in self.feature_columns])\n\n        model = create_model(\n            featureclass_name,\n            __base__=FeatureRecord,\n            __doc__=docstring,\n            **feature_columns,\n        )\n        model.feature = self  # Set value of class variable within the feature class definition.\n\n        return model\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"Feature(target_table={self.target_table.name}, feature_name={self.feature_name}, \"\n            f\"feature_table={self.feature_table.name})\"\n        )\n</code></pre>"},{"location":"code-docs/feature/#deriva_ml.feature.Feature.feature_record_class","title":"feature_record_class","text":"<pre><code>feature_record_class() -&gt; type[\n    FeatureRecord\n]\n</code></pre> <p>Create a pydantic model for entries into the specified feature table</p> <p>Returns:</p> Type Description <code>type[FeatureRecord]</code> <p>A Feature class that can be used to create instances of the feature.</p> Source code in <code>src/deriva_ml/feature.py</code> <pre><code>def feature_record_class(self) -&gt; type[FeatureRecord]:\n    \"\"\"Create a pydantic model for entries into the specified feature table\n\n    Returns:\n        A Feature class that can be used to create instances of the feature.\n    \"\"\"\n\n    def map_type(c: Column) -&gt; UnionType | Type[str] | Type[int] | Type[float]:\n        \"\"\"Maps a Deriva column type to a Python/pydantic type.\n\n        Converts ERMrest column types to appropriate Python types for use in pydantic models.\n        Special handling is provided for asset columns which can accept either strings or Path objects.\n\n        Args:\n            c: ERMrest column to map to a Python type.\n\n        Returns:\n            UnionType | Type[str] | Type[int] | Type[float]: Appropriate Python type for the column:\n                - str | Path for asset columns\n                - str for text columns\n                - int for integer columns\n                - float for floating point columns\n                - str for all other types\n\n        Example:\n            &gt;&gt;&gt; col = Column(name=\"score\", type=\"float4\")\n            &gt;&gt;&gt; typ = map_type(col)  # Returns float\n        \"\"\"\n        if c.name in {c.name for c in self.asset_columns}:\n            return str | Path\n\n        match c.type.typename:\n            case \"text\":\n                return str\n            case \"int2\" | \"int4\" | \"int8\":\n                return int\n            case \"float4\" | \"float8\":\n                return float\n            case _:\n                return str\n\n    featureclass_name = f\"{self.target_table.name}Feature{self.feature_name}\"\n\n    # Create feature class. To do this, we must determine the python type for each column and also if the\n    # column is optional or not based on its nullability.\n    feature_columns = {\n        c.name: (\n            Optional[map_type(c)] if c.nullok else map_type(c),\n            c.default or None,\n        )\n        for c in self.feature_columns\n    } | {\n        \"Feature_Name\": (\n            str,\n            self.feature_name,\n        ),  # Set default value for Feature_Name\n        self.target_table.name: (str, ...),\n    }\n    docstring = (\n        f\"Class to capture fields in a feature {self.feature_name} on table {self.target_table}. \"\n        \"Feature columns include:\\n\"\n    )\n    docstring += \"\\n\".join([f\"    {c.name}\" for c in self.feature_columns])\n\n    model = create_model(\n        featureclass_name,\n        __base__=FeatureRecord,\n        __doc__=docstring,\n        **feature_columns,\n    )\n    model.feature = self  # Set value of class variable within the feature class definition.\n\n    return model\n</code></pre>"},{"location":"code-docs/feature/#deriva_ml.feature.FeatureRecord","title":"FeatureRecord","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for dynamically generated feature record models.</p> <p>This class serves as the base for pydantic models that represent feature records. Each feature record contains the values and metadata associated with a feature instance.</p> <p>Attributes:</p> Name Type Description <code>Execution</code> <code>Optional[str]</code> <p>RID of the execution that created this feature record.</p> <code>Feature_Name</code> <code>str</code> <p>Name of the feature this record belongs to.</p> <code>feature</code> <code>ClassVar[Optional[Feature]]</code> <p>Reference to the Feature object that created this record.</p> Example <p>class GeneFeature(FeatureRecord): ...     value: str ...     confidence: float record = GeneFeature( ...     Feature_Name=\"expression\", ...     value=\"high\", ...     confidence=0.95 ... )</p> Source code in <code>src/deriva_ml/feature.py</code> <pre><code>class FeatureRecord(BaseModel):\n    \"\"\"Base class for dynamically generated feature record models.\n\n    This class serves as the base for pydantic models that represent feature records. Each feature record\n    contains the values and metadata associated with a feature instance.\n\n    Attributes:\n        Execution (Optional[str]): RID of the execution that created this feature record.\n        Feature_Name (str): Name of the feature this record belongs to.\n        feature (ClassVar[Optional[Feature]]): Reference to the Feature object that created this record.\n\n    Example:\n        &gt;&gt;&gt; class GeneFeature(FeatureRecord):\n        ...     value: str\n        ...     confidence: float\n        &gt;&gt;&gt; record = GeneFeature(\n        ...     Feature_Name=\"expression\",\n        ...     value=\"high\",\n        ...     confidence=0.95\n        ... )\n    \"\"\"\n\n    # model_dump of this feature should be compatible with feature table columns.\n    Execution: Optional[str] = None\n    Feature_Name: str\n    feature: ClassVar[Optional[\"Feature\"]] = None\n\n    class Config:\n        arbitrary_types_allowed = True\n        extra = \"forbid\"\n\n    @classmethod\n    def feature_columns(cls) -&gt; set[Column]:\n        \"\"\"Returns all columns specific to this feature.\n\n        Returns:\n            set[Column]: Set of feature-specific columns, excluding system and relationship columns.\n        \"\"\"\n        return cls.feature.feature_columns\n\n    @classmethod\n    def asset_columns(cls) -&gt; set[Column]:\n        \"\"\"Returns columns that reference asset tables.\n\n        Returns:\n            set[Column]: Set of columns that contain references to asset tables.\n        \"\"\"\n        return cls.feature.asset_columns\n\n    @classmethod\n    def term_columns(cls) -&gt; set[Column]:\n        \"\"\"Returns columns that reference vocabulary terms.\n\n        Returns:\n            set[Column]: Set of columns that contain references to controlled vocabulary terms.\n        \"\"\"\n        return cls.feature.term_columns\n\n    @classmethod\n    def value_columns(cls) -&gt; set[Column]:\n        \"\"\"Returns columns that contain direct values.\n\n        Returns:\n            set[Column]: Set of columns containing direct values (not references to assets or terms).\n        \"\"\"\n        return cls.feature.value_columns\n</code></pre>"},{"location":"code-docs/feature/#deriva_ml.feature.FeatureRecord.asset_columns","title":"asset_columns  <code>classmethod</code>","text":"<pre><code>asset_columns() -&gt; set[Column]\n</code></pre> <p>Returns columns that reference asset tables.</p> <p>Returns:</p> Type Description <code>set[Column]</code> <p>set[Column]: Set of columns that contain references to asset tables.</p> Source code in <code>src/deriva_ml/feature.py</code> <pre><code>@classmethod\ndef asset_columns(cls) -&gt; set[Column]:\n    \"\"\"Returns columns that reference asset tables.\n\n    Returns:\n        set[Column]: Set of columns that contain references to asset tables.\n    \"\"\"\n    return cls.feature.asset_columns\n</code></pre>"},{"location":"code-docs/feature/#deriva_ml.feature.FeatureRecord.feature_columns","title":"feature_columns  <code>classmethod</code>","text":"<pre><code>feature_columns() -&gt; set[Column]\n</code></pre> <p>Returns all columns specific to this feature.</p> <p>Returns:</p> Type Description <code>set[Column]</code> <p>set[Column]: Set of feature-specific columns, excluding system and relationship columns.</p> Source code in <code>src/deriva_ml/feature.py</code> <pre><code>@classmethod\ndef feature_columns(cls) -&gt; set[Column]:\n    \"\"\"Returns all columns specific to this feature.\n\n    Returns:\n        set[Column]: Set of feature-specific columns, excluding system and relationship columns.\n    \"\"\"\n    return cls.feature.feature_columns\n</code></pre>"},{"location":"code-docs/feature/#deriva_ml.feature.FeatureRecord.term_columns","title":"term_columns  <code>classmethod</code>","text":"<pre><code>term_columns() -&gt; set[Column]\n</code></pre> <p>Returns columns that reference vocabulary terms.</p> <p>Returns:</p> Type Description <code>set[Column]</code> <p>set[Column]: Set of columns that contain references to controlled vocabulary terms.</p> Source code in <code>src/deriva_ml/feature.py</code> <pre><code>@classmethod\ndef term_columns(cls) -&gt; set[Column]:\n    \"\"\"Returns columns that reference vocabulary terms.\n\n    Returns:\n        set[Column]: Set of columns that contain references to controlled vocabulary terms.\n    \"\"\"\n    return cls.feature.term_columns\n</code></pre>"},{"location":"code-docs/feature/#deriva_ml.feature.FeatureRecord.value_columns","title":"value_columns  <code>classmethod</code>","text":"<pre><code>value_columns() -&gt; set[Column]\n</code></pre> <p>Returns columns that contain direct values.</p> <p>Returns:</p> Type Description <code>set[Column]</code> <p>set[Column]: Set of columns containing direct values (not references to assets or terms).</p> Source code in <code>src/deriva_ml/feature.py</code> <pre><code>@classmethod\ndef value_columns(cls) -&gt; set[Column]:\n    \"\"\"Returns columns that contain direct values.\n\n    Returns:\n        set[Column]: Set of columns containing direct values (not references to assets or terms).\n    \"\"\"\n    return cls.feature.value_columns\n</code></pre>"},{"location":"code-docs/upload/","title":"Upload","text":"<p>This module provides functions that help structure local directories for uploading to a DerivaML catalog, and generating an upload specification for those directories.</p> <p>Here is the directory layout we support:</p> <p>deriva-ml/        execution                            execution-asset                                            file1, file2, ....   &lt;- Need to update execution_asset association table.                execution-metadata                                    feature                                                         asset                                                                                    file1, file2, ...                            .jsonl    &lt;- needs to have asset_name column remapped before uploading                 table                                                record_table.csv                 asset                                                           file1, file2, ....                 asset-type                                              file1.jsonl, file2.jsonl"},{"location":"code-docs/upload/#deriva_ml.dataset.upload.asset_file_path","title":"asset_file_path","text":"<pre><code>asset_file_path(\n    prefix: Path | str,\n    exec_rid: RID,\n    asset_table: Table,\n    file_name: str,\n    metadata: dict[str, Any],\n) -&gt; Path\n</code></pre> <p>Return the file in which to place  assets of a specified type are to be uploaded.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>Path | str</code> <p>Path prefix to use.</p> required <code>exec_rid</code> <code>RID</code> <p>RID to use.</p> required <code>asset_table</code> <code>Table</code> <p>Table in which to place assets.</p> required <code>file_name</code> <code>str</code> <p>File name to use.</p> required <code>metadata</code> <code>dict[str, Any]</code> <p>Any additional metadata to add to the asset</p> required <p>Returns:     Path to directory in which to place assets of type asset_type.</p> Source code in <code>src/deriva_ml/dataset/upload.py</code> <pre><code>def asset_file_path(\n    prefix: Path | str,\n    exec_rid: RID,\n    asset_table: Table,\n    file_name: str,\n    metadata: dict[str, Any],\n) -&gt; Path:\n    \"\"\"Return the file in which to place  assets of a specified type are to be uploaded.\n\n    Args:\n        prefix: Path prefix to use.\n        exec_rid: RID to use.\n        asset_table: Table in which to place assets.\n        file_name: File name to use.\n        metadata: Any additional metadata to add to the asset\n    Returns:\n        Path to directory in which to place assets of type asset_type.\n    \"\"\"\n    schema = asset_table.schema.name\n    asset_name = asset_table.name\n\n    path = execution_root(prefix, exec_rid) / \"asset\" / schema / asset_name\n    metadata = metadata or {}\n    asset_columns = {\n        \"Filename\",\n        \"URL\",\n        \"Length\",\n        \"MD5\",\n        \"Description\",\n    }.union(set(DerivaSystemColumns))\n    asset_metadata = {c.name for c in asset_table.columns} - asset_columns\n    if not (asset_metadata &gt;= set(metadata.keys())):\n        raise DerivaMLException(f\"Metadata {metadata} does not match asset metadata {asset_metadata}\")\n\n    for m in asset_metadata:\n        path = path / metadata.get(m, \"None\")\n    path.mkdir(parents=True, exist_ok=True)\n    return path / file_name\n</code></pre>"},{"location":"code-docs/upload/#deriva_ml.dataset.upload.asset_root","title":"asset_root","text":"<pre><code>asset_root(\n    prefix: Path | str, exec_rid: str\n) -&gt; Path\n</code></pre> <p>Return the path to the directory in which features for the specified execution should be placed.</p> Source code in <code>src/deriva_ml/dataset/upload.py</code> <pre><code>def asset_root(prefix: Path | str, exec_rid: str) -&gt; Path:\n    \"\"\"Return the path to the directory in which features for the specified execution should be placed.\"\"\"\n    path = execution_root(prefix, exec_rid) / \"asset\"\n    path.mkdir(parents=True, exist_ok=True)\n    return path\n</code></pre>"},{"location":"code-docs/upload/#deriva_ml.dataset.upload.asset_table_upload_spec","title":"asset_table_upload_spec","text":"<pre><code>asset_table_upload_spec(\n    model: DerivaModel,\n    asset_table: str | Table,\n)\n</code></pre> <p>Generate upload specification for an asset table.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>DerivaModel</code> <p>The DerivaModel instance.</p> required <code>asset_table</code> <code>str | Table</code> <p>The asset table name or Table object.</p> required <p>Returns:</p> Type Description <p>A dictionary containing the upload specification for the asset table.</p> Source code in <code>src/deriva_ml/dataset/upload.py</code> <pre><code>def asset_table_upload_spec(model: DerivaModel, asset_table: str | Table):\n    \"\"\"Generate upload specification for an asset table.\n\n    Args:\n        model: The DerivaModel instance.\n        asset_table: The asset table name or Table object.\n\n    Returns:\n        A dictionary containing the upload specification for the asset table.\n    \"\"\"\n    metadata_columns = model.asset_metadata(asset_table)\n    asset_table = model.name_to_table(asset_table)\n    schema = model.name_to_table(asset_table).schema.name\n    metadata_path = \"/\".join([rf\"(?P&lt;{c}&gt;[-\\w]+)\" for c in metadata_columns])\n    asset_path = f\"{exec_dir_regex}/asset/{schema}/{asset_table.name}/{metadata_path}/{asset_file_regex}\"\n    asset_table = model.name_to_table(asset_table)\n    schema = model.name_to_table(asset_table).schema.name\n\n    # Create upload specification\n    spec = {\n        # Upload assets into an asset table of an asset table.\n        \"column_map\": {\n            \"MD5\": \"{md5}\",\n            \"URL\": \"{URI}\",\n            \"Length\": \"{file_size}\",\n            \"Filename\": \"{file_name}\",\n        }\n        | {c: f\"{{{c}}}\" for c in metadata_columns},\n        \"file_pattern\": asset_path,  # Sets schema, asset_table, file\n        \"asset_type\": \"file\",\n        \"target_table\": [schema, asset_table.name],\n        \"checksum_types\": [\"sha256\", \"md5\"],\n        \"hatrac_options\": {\"versioned_urls\": True},\n        \"hatrac_templates\": {\n            \"hatrac_uri\": f\"/hatrac/{asset_table.name}/{{md5}}.{{file_name}}\",\n            \"content-disposition\": \"filename*=UTF-8''{file_name}\",\n        },\n        \"record_query_template\": \"/entity/{target_table}/MD5={md5}&amp;Filename={file_name}\",\n    }\n    return spec\n</code></pre>"},{"location":"code-docs/upload/#deriva_ml.dataset.upload.asset_type_path","title":"asset_type_path","text":"<pre><code>asset_type_path(\n    prefix: Path | str,\n    exec_rid: RID,\n    asset_table: Table,\n) -&gt; Path\n</code></pre> <p>Return the path to a JSON line file in which to place asset_type information.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>Path | str</code> <p>Location of upload root directory</p> required <code>exec_rid</code> <code>RID</code> <p>Execution RID</p> required <code>asset_table</code> <code>Table</code> <p>Table in which to place assets.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to the file in which to place asset_type values for the named asset.</p> Source code in <code>src/deriva_ml/dataset/upload.py</code> <pre><code>def asset_type_path(prefix: Path | str, exec_rid: RID, asset_table: Table) -&gt; Path:\n    \"\"\"Return the path to a JSON line file in which to place asset_type information.\n\n    Args:\n        prefix: Location of upload root directory\n        exec_rid: Execution RID\n        asset_table: Table in which to place assets.\n\n    Returns:\n        Path to the file in which to place asset_type values for the named asset.\n    \"\"\"\n    path = execution_root(prefix, exec_rid=exec_rid) / \"asset-type\" / asset_table.schema.name\n    path.mkdir(parents=True, exist_ok=True)\n    return path / f\"{asset_table.name}.jsonl\"\n</code></pre>"},{"location":"code-docs/upload/#deriva_ml.dataset.upload.bulk_upload_configuration","title":"bulk_upload_configuration","text":"<pre><code>bulk_upload_configuration(\n    model: DerivaModel,\n) -&gt; dict[str, Any]\n</code></pre> <p>Return an upload specification for deriva-ml Arguments:     model: Model from which to generate the upload configuration</p> Source code in <code>src/deriva_ml/dataset/upload.py</code> <pre><code>def bulk_upload_configuration(model: DerivaModel) -&gt; dict[str, Any]:\n    \"\"\"Return an upload specification for deriva-ml\n    Arguments:\n        model: Model from which to generate the upload configuration\n    \"\"\"\n    asset_tables_with_metadata = [\n        asset_table_upload_spec(model=model, asset_table=t) for t in model.find_assets() if model.asset_metadata(t)\n    ]\n    return {\n        \"asset_mappings\": asset_tables_with_metadata\n        + [\n            {\n                # Upload assets into an asset table of an asset table without any metadata\n                \"column_map\": {\n                    \"MD5\": \"{md5}\",\n                    \"URL\": \"{URI}\",\n                    \"Length\": \"{file_size}\",\n                    \"Filename\": \"{file_name}\",\n                },\n                \"asset_type\": \"file\",\n                \"target_table\": [\"{schema}\", \"{asset_table}\"],\n                \"file_pattern\": asset_path_regex + \"/\" + asset_file_regex,  # Sets schema, asset_table, name, ext\n                \"checksum_types\": [\"sha256\", \"md5\"],\n                \"hatrac_options\": {\"versioned_urls\": True},\n                \"hatrac_templates\": {\n                    \"hatrac_uri\": \"/hatrac/{asset_table}/{md5}.{file_name}\",\n                    \"content-disposition\": \"filename*=UTF-8''{file_name}\",\n                },\n                \"record_query_template\": \"/entity/{target_table}/MD5={md5}&amp;Filename={file_name}\",\n            },\n            # {\n            #  Upload the records into a  table\n            #   \"asset_type\": \"skip\",\n            ##   \"default_columns\": [\"RID\", \"RCB\", \"RMB\", \"RCT\", \"RMT\"],\n            #  \"file_pattern\": feature_value_regex,  # Sets schema, table,\n            #  \"ext_pattern\": \"^.*[.](?P&lt;file_ext&gt;json|csv)$\",\n            #  \"target_table\": [\"{schema}\", \"{table}\"],\n            # },\n            {\n                #  Upload the records into a  table\n                \"asset_type\": \"table\",\n                \"default_columns\": [\"RID\", \"RCB\", \"RMB\", \"RCT\", \"RMT\"],\n                \"file_pattern\": table_regex,  # Sets schema, table,\n                \"ext_pattern\": \"^.*[.](?P&lt;file_ext&gt;json|csv)$\",\n                \"target_table\": [\"{schema}\", \"{table}\"],\n            },\n        ],\n        \"version_update_url\": \"https://github.com/informatics-isi-edu/deriva-client\",\n        \"version_compatibility\": [[\"&gt;=1.4.0\", \"&lt;2.0.0\"]],\n    }\n</code></pre>"},{"location":"code-docs/upload/#deriva_ml.dataset.upload.execution_rids","title":"execution_rids","text":"<pre><code>execution_rids(\n    prefix: Path | str,\n) -&gt; list[RID]\n</code></pre> <p>Return a list of all the execution RIDS that have files waiting to be uploaded.</p> Source code in <code>src/deriva_ml/dataset/upload.py</code> <pre><code>def execution_rids(prefix: Path | str) -&gt; list[RID]:\n    \"\"\"Return a list of all the execution RIDS that have files waiting to be uploaded.\"\"\"\n    path = upload_root(prefix) / \"execution\"\n    return [d.name for d in path.iterdir()]\n</code></pre>"},{"location":"code-docs/upload/#deriva_ml.dataset.upload.execution_root","title":"execution_root","text":"<pre><code>execution_root(\n    prefix: Path | str, exec_rid\n) -&gt; Path\n</code></pre> <p>Path to directory to place execution specific upload files.</p> Source code in <code>src/deriva_ml/dataset/upload.py</code> <pre><code>def execution_root(prefix: Path | str, exec_rid) -&gt; Path:\n    \"\"\"Path to directory to place execution specific upload files.\"\"\"\n    path = upload_root(prefix) / \"execution\" / exec_rid\n    path.mkdir(exist_ok=True, parents=True)\n    return path\n</code></pre>"},{"location":"code-docs/upload/#deriva_ml.dataset.upload.feature_dir","title":"feature_dir","text":"<pre><code>feature_dir(\n    prefix: Path | str,\n    exec_rid: str,\n    schema: str,\n    target_table: str,\n    feature_name: str,\n) -&gt; Path\n</code></pre> <p>Return the path to eht directory in which a named feature for an execution should be placed.</p> Source code in <code>src/deriva_ml/dataset/upload.py</code> <pre><code>def feature_dir(prefix: Path | str, exec_rid: str, schema: str, target_table: str, feature_name: str) -&gt; Path:\n    \"\"\"Return the path to eht directory in which a named feature for an execution should be placed.\"\"\"\n    path = feature_root(prefix, exec_rid) / schema / target_table / feature_name\n    path.mkdir(parents=True, exist_ok=True)\n    return path\n</code></pre>"},{"location":"code-docs/upload/#deriva_ml.dataset.upload.feature_root","title":"feature_root","text":"<pre><code>feature_root(\n    prefix: Path | str, exec_rid: str\n) -&gt; Path\n</code></pre> <p>Return the path to the directory in which features for the specified execution should be placed.</p> Source code in <code>src/deriva_ml/dataset/upload.py</code> <pre><code>def feature_root(prefix: Path | str, exec_rid: str) -&gt; Path:\n    \"\"\"Return the path to the directory in which features for the specified execution should be placed.\"\"\"\n    path = execution_root(prefix, exec_rid) / \"feature\"\n    path.mkdir(parents=True, exist_ok=True)\n    return path\n</code></pre>"},{"location":"code-docs/upload/#deriva_ml.dataset.upload.feature_value_path","title":"feature_value_path","text":"<pre><code>feature_value_path(\n    prefix: Path | str,\n    exec_rid: str,\n    schema: str,\n    target_table: str,\n    feature_name: str,\n) -&gt; Path\n</code></pre> <p>Return the path to a CSV file in which to place feature values that are to be uploaded.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>Path | str</code> <p>Location of upload root directory</p> required <code>exec_rid</code> <code>str</code> <p>RID of the execution to be associated with this feature.</p> required <code>schema</code> <code>str</code> <p>Domain schema name</p> required <code>target_table</code> <code>str</code> <p>Target table name for the feature.</p> required <code>feature_name</code> <code>str</code> <p>Name of the feature.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to CSV file in which to place feature values</p> Source code in <code>src/deriva_ml/dataset/upload.py</code> <pre><code>def feature_value_path(prefix: Path | str, exec_rid: str, schema: str, target_table: str, feature_name: str) -&gt; Path:\n    \"\"\"Return the path to a CSV file in which to place feature values that are to be uploaded.\n\n    Args:\n        prefix: Location of upload root directory\n        exec_rid: RID of the execution to be associated with this feature.\n        schema: Domain schema name\n        target_table: Target table name for the feature.\n        feature_name: Name of the feature.\n\n    Returns:\n        Path to CSV file in which to place feature values\n    \"\"\"\n    return feature_dir(prefix, exec_rid, schema, target_table, feature_name) / f\"{feature_name}.jsonl\"\n</code></pre>"},{"location":"code-docs/upload/#deriva_ml.dataset.upload.is_feature_dir","title":"is_feature_dir","text":"<pre><code>is_feature_dir(\n    path: Path,\n) -&gt; Optional[re.Match]\n</code></pre> <p>Path matches the pattern for where the table for a feature would go.</p> Source code in <code>src/deriva_ml/dataset/upload.py</code> <pre><code>def is_feature_dir(path: Path) -&gt; Optional[re.Match]:\n    \"\"\"Path matches the pattern for where the table for a feature would go.\"\"\"\n    return re.match(feature_table_dir_regex + \"$\", path.as_posix())\n</code></pre>"},{"location":"code-docs/upload/#deriva_ml.dataset.upload.normalize_asset_dir","title":"normalize_asset_dir","text":"<pre><code>normalize_asset_dir(\n    path: str,\n) -&gt; Optional[tuple[str, str]]\n</code></pre> <p>Parse a path to an asset file and return the asset table name and file name.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the asset file</p> required <p>Returns:</p> Type Description <code>Optional[tuple[str, str]]</code> <p>Tuple of (schema/table, filename) or None if path doesn't match pattern</p> Source code in <code>src/deriva_ml/dataset/upload.py</code> <pre><code>def normalize_asset_dir(path: str) -&gt; Optional[tuple[str, str]]:\n    \"\"\"Parse a path to an asset file and return the asset table name and file name.\n\n    Args:\n        path: Path to the asset file\n\n    Returns:\n        Tuple of (schema/table, filename) or None if path doesn't match pattern\n    \"\"\"\n    path = Path(path)\n    if not (m := re.match(asset_path_regex, str(path))):\n        return None\n    return f\"{m['schema']}/{m['asset_table']}\", path.name\n</code></pre>"},{"location":"code-docs/upload/#deriva_ml.dataset.upload.table_path","title":"table_path","text":"<pre><code>table_path(\n    prefix: Path | str,\n    schema: str,\n    table: str,\n) -&gt; Path\n</code></pre> <p>Return the path to a CSV file in which to place table values that are to be uploaded.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>Path | str</code> <p>Location of upload root directory</p> required <code>schema</code> <code>str</code> <p>Domain schema</p> required <code>table</code> <code>str</code> <p>Name of the table to be uploaded.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to the file in which to place table values that are to be uploaded.</p> Source code in <code>src/deriva_ml/dataset/upload.py</code> <pre><code>def table_path(prefix: Path | str, schema: str, table: str) -&gt; Path:\n    \"\"\"Return the path to a CSV file in which to place table values that are to be uploaded.\n\n    Args:\n        prefix: Location of upload root directory\n        schema: Domain schema\n        table: Name of the table to be uploaded.\n\n    Returns:\n        Path to the file in which to place table values that are to be uploaded.\n    \"\"\"\n    path = upload_root(prefix) / \"table\" / schema / table\n    path.mkdir(parents=True, exist_ok=True)\n    return path / f\"{table}.csv\"\n</code></pre>"},{"location":"code-docs/upload/#deriva_ml.dataset.upload.upload_asset","title":"upload_asset","text":"<pre><code>upload_asset(\n    model: DerivaModel,\n    file: Path | str,\n    table: Table,\n    **kwargs: Any,\n) -&gt; dict\n</code></pre> <p>Upload the specified file into Hatrac and update the associated asset table.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Path | str</code> <p>path to the file to upload.</p> required <code>table</code> <code>Table</code> <p>Name of the asset table</p> required <code>model</code> <code>DerivaModel</code> <p>Model to upload assets to.</p> required <code>kwargs</code> <code>Any</code> <p>Keyword arguments for values of additional columns to be added to the asset table.</p> <code>{}</code> <p>Returns:</p> Source code in <code>src/deriva_ml/dataset/upload.py</code> <pre><code>@validate_call(config=ConfigDict(arbitrary_types_allowed=True))\ndef upload_asset(model: DerivaModel, file: Path | str, table: Table, **kwargs: Any) -&gt; dict:\n    \"\"\"Upload the specified file into Hatrac and update the associated asset table.\n\n    Args:\n        file: path to the file to upload.\n        table: Name of the asset table\n        model: Model to upload assets to.\n        kwargs: Keyword arguments for values of additional columns to be added to the asset table.\n\n    Returns:\n\n    \"\"\"\n    if not model.is_asset(table):\n        raise DerivaMLException(f\"Table {table} is not an asset table.\")\n\n    file_path = Path(file)\n    file_name = file_path.name\n    file_size = file_path.stat().st_size\n\n    hatrac_path = f\"/hatrac/{table.name}/\"\n    hs = HatracStore(\n        \"https\",\n        server=model.catalog.deriva_server.server,\n        credentials=model.catalog.deriva_server.credentials,\n    )\n    md5_hashes = hash_utils.compute_file_hashes(file, [\"md5\"])[\"md5\"]\n    sanitized_filename = urlquote(re.sub(\"[^a-zA-Z0-9_.-]\", \"_\", md5_hashes[0] + \".\" + file_name))\n    hatrac_path = f\"{hatrac_path}{sanitized_filename}\"\n\n    try:\n        # Upload the file to hatrac.\n        hatrac_uri = hs.put_obj(\n            hatrac_path,\n            file,\n            md5=md5_hashes[1],\n            content_type=mime_utils.guess_content_type(file),\n            content_disposition=\"filename*=UTF-8''\" + file_name,\n        )\n    except Exception as e:\n        raise e\n    try:\n        # Now update the asset table.\n        ipath = model.catalog.getPathBuilder().schemas[table.schema.name].tables[table.name]\n        return list(\n            ipath.insert(\n                [\n                    {\n                        \"URL\": hatrac_uri,\n                        \"Filename\": file_name,\n                        \"Length\": file_size,\n                        \"MD5\": md5_hashes[0],\n                    }\n                    | kwargs\n                ]\n            )\n        )[0]\n    except Exception as e:\n        raise e\n</code></pre>"},{"location":"code-docs/upload/#deriva_ml.dataset.upload.upload_directory","title":"upload_directory","text":"<pre><code>upload_directory(\n    model: DerivaModel,\n    directory: Path | str,\n) -&gt; dict[Any, FileUploadState] | None\n</code></pre> <p>Upload assets from a directory. This routine assumes that the current upload specification includes a configuration for the specified directory.  Every asset in the specified directory is uploaded</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>DerivaModel</code> <p>Model to upload assets to.</p> required <code>directory</code> <code>Path | str</code> <p>Directory containing the assets and tables to upload.</p> required <p>Returns:</p> Type Description <code>dict[Any, FileUploadState] | None</code> <p>Results of the upload operation.</p> <p>Raises:</p> Type Description <code>DerivaMLException</code> <p>If there is an issue with uploading the assets.</p> Source code in <code>src/deriva_ml/dataset/upload.py</code> <pre><code>@validate_call(config=ConfigDict(arbitrary_types_allowed=True))\ndef upload_directory(model: DerivaModel, directory: Path | str) -&gt; dict[Any, FileUploadState] | None:\n    \"\"\"Upload assets from a directory. This routine assumes that the current upload specification includes a\n    configuration for the specified directory.  Every asset in the specified directory is uploaded\n\n    Args:\n        model: Model to upload assets to.\n        directory: Directory containing the assets and tables to upload.\n\n    Returns:\n        Results of the upload operation.\n\n    Raises:\n        DerivaMLException: If there is an issue with uploading the assets.\n    \"\"\"\n    directory = Path(directory)\n    if not directory.is_dir():\n        raise DerivaMLException(\"Directory does not exist\")\n\n    # Now upload the files by creating an upload spec and then calling the uploader.\n    with TemporaryDirectory() as temp_dir:\n        spec_file = Path(temp_dir) / \"config.json\"\n\n        with spec_file.open(\"w+\") as cfile:\n            json.dump(bulk_upload_configuration(model), cfile)\n        uploader = GenericUploader(\n            server={\n                \"host\": model.hostname,\n                \"protocol\": \"https\",\n                \"catalog_id\": model.catalog.catalog_id,\n            },\n            config_file=spec_file,\n        )\n        try:\n            uploader.getUpdatedConfig()\n            uploader.scanDirectory(directory, purge_state=True)\n            results = {\n                path: FileUploadState(\n                    state=UploadState(result[\"State\"]),\n                    status=result[\"Status\"],\n                    result=result[\"Result\"],\n                )\n                for path, result in uploader.uploadFiles().items()\n            }\n        finally:\n            uploader.cleanup()\n        return results\n</code></pre>"},{"location":"code-docs/upload/#deriva_ml.dataset.upload.upload_root","title":"upload_root","text":"<pre><code>upload_root(prefix: Path | str) -&gt; Path\n</code></pre> <p>Return the top level directory of where to put files to be uploaded.</p> Source code in <code>src/deriva_ml/dataset/upload.py</code> <pre><code>def upload_root(prefix: Path | str) -&gt; Path:\n    \"\"\"Return the top level directory of where to put files to be uploaded.\"\"\"\n    path = Path(prefix) / \"deriva-ml\"\n    path.mkdir(exist_ok=True, parents=True)\n    return path\n</code></pre>"},{"location":"user-guide/datasets/","title":"Datasets","text":"<p>When working with ML models, it is often convenient to collect various input data into named, identifiable collections: A dataset DerivaML provides a very flexible set of mechanisms for creating and manipulating datasets.</p> <p>A dataset is a collection of objects that appear within a DerivaML catalog. Datasets can be heterogeneous, containing  sets of different object types. Its possible that you can have an object referenced twice, for example a collection of  subjects of a study and a collection of observations. DerivaML sorts out these repeated references and makes it possible to view them from all of the possible paths. DerivaML datasets allow reproducible grouping of objects to be provided to ML code</p> <p>As with any other object in DerivaML, each dataset is identified by its Resource Identifier or RID. In addition, a dataset may have one or more dataset types, and also a version</p>"},{"location":"user-guide/datasets/#dataset-types","title":"Dataset Types","text":"<p>Dataset types are assigned from a controlled vocabulary called <code>MLVocab.dataset_type</code>. You can define new dataset types as you need:</p> <pre><code>from deriva_ml import MLVocab\nml_instance.add_term(MLVocab.dataset_type, \"DemoSet\", description=\"A test dataset_table\")\n</code></pre> <p>When you create a dataset, you can provide as many dataset types as required to streamline organizing and discovering them in your code.</p>"},{"location":"user-guide/datasets/#creating-datasets","title":"Creating Datasets","text":"<p>Its important to know how a dataset was created, so the most common way to create a dataset is within an execution:</p> <pre><code># Now lets create model configuration for our program.\napi_workflow = Workflow(\n    name=\"API Workflow\",\n    workflow_type=\"Create Dataset Notebook\"\n)\n\ndataset_execution = ml_instance.create_execution(\n    ExecutionConfiguration(\n        workflow=api_workflow,\n        description=\"Our Sample Workflow instance\")\n)\n\nsubject_dataset = dataset_execution.create_dataset(\n    dataset_types=['DemoSet', 'Subject'], \n    description=\"A subject dataset_table\")\nimage_dataset = dataset_execution.create_dataset(\n    dataset_types=['DemoSet', 'Image'], description=\"A image training dataset_table\")\n</code></pre>"},{"location":"user-guide/datasets/#dataset-element-types","title":"Dataset element types","text":"<p>In DerivaML, a dataet may consist of many different types of objects.  In general, any element of a domain model may be included in a dataset.  Howerver, tts important to know how a  Datasets can contain elements from the domain model. or other datasets.</p>"},{"location":"user-guide/datasets/#adding-members-to-a-dataset","title":"Adding members to a dataset","text":"<p>Dataset members, type and description.</p>"},{"location":"user-guide/datasets/#nested-datasets","title":"Nested Datasets","text":"<p>Listing datasets Adding new element types to the dataset</p> <p>Adding members to a dataset</p> <p>Listing members of a dataset</p>"},{"location":"user-guide/datasets/#dataset-versioning","title":"Dataset Versioning","text":"<p>Every dataset is assigned a version number which will change over the lifetime of the dataset. DerivaML uses semantic versioning to differntiate between versions of the dataset over time.  A semantic version consists of three parts. Each part is an integer seperated by a dot:<code>major.minor.patch</code>. 0  The major part will change when there is a schema change to any object included in the dataset.  The minor part will change when new elements are added to a dataset, and the patch parch changes for minor alterations, such as adding or changing a comment or data cleaning.</p> <p>DerivaML will automatically assign an initial version of <code>0.1.0</code> when a dataaset is first created, and increment the minor part of the version number whenever new elements are added.  It is up to the DerivaML user to otherwise increment the dataset version.</p> <p>The version of a dataset can be incremented by the method: <code>DerivaML.increment_version()</code> The current version of a dataset can be returned: <code>DerivaML.dataset_version()</code>, as can the version history of a dataset: <code>DerivaML.dataset_history</code>.</p> <p>Dataset versions must be specified when a dataset is downloaded to a compute platform for processing by ML code. It is important to know that the values in the dataset are the values that were in place at the time the dataset was created.  This is true for the current dataset as well.  If you want to use the current values in a catalog, you must create a new dataset as part of the execution configuration. This is easily accomplished using the <code>increment_version</code> method.</p>"},{"location":"user-guide/datasets/#assets","title":"Assets","text":"<p>A core function of datasets is to help orginize collections of files or assets that are stored in the catalog.  The actual contents of all of the assets are stored in the DerivaML object store, which is call Hatrac.  As far as the object store is concerned, each asset is just a blob of bits, which are characterized by a versioned URL, a length and a checksum.</p> <p>Within the DerivaML catalog, assets are represented by an asset table.  Each different asset type is stored in a different table, and the entry includes the URL to the object, the length, checksum and other general metadata, along with any additional asset type specific metadata that might have been defined.</p>"},{"location":"user-guide/datasets/#downloaded-datasets-and-assets","title":"Downloaded Datasets and Assets","text":"<p>While it is possible to retrieve the assets associated with a dataset directly from the catalog, if your computation requres access to many assets, its typically much more effective to download the asset along with all the other elements of the dataset into the local compute environment. For this reason, creating a new execution using <code>DerivaML.create_execution</code> will automatically download all of the datasets listed in the <code>datasets</code> component of an <code>ExecutionConfiguration</code>.</p> <p>Once downloaded, the dataset is represented by a <code>Dataset_Bag</code> object. </p> <p>Every asset in a dataset is recorded in an asset table, which has its location on the local file system, along with other metadata. Asset tables can be retrieved from the dataset_bag via the get_table_as_?() method.</p> <p>The interface will ensure that all assets of the same type are in the same directory, and that directory will be named by the asset type.</p> <p>It is unwise to use the name of the asset file for anything. Rather, you should figure out what asset you want via the metadata that connects that asset to more interesting things.</p> <p>If you need to reorganize assets in an application-specific way, using symbolic links is probably the most efficient thing to do with respect to time and disk space. </p>"},{"location":"user-guide/datasets/#downloading-datasets","title":"Downloading Datasets","text":"<p>Datasets are automatically downloaded as part of creating a new execution. Downloading datasets for the ML code.  Caching</p> <p>MINIDs</p>"},{"location":"user-guide/deriva_ml_structure/","title":"Using the Deriva-ML library","text":"<p>While the deriva-ml library is quite flexible, its use can be simplified if a few basic guidelines are followed when structuring an ML project.</p> <p>When creating a new project, we suggest that you create two new GitHub repositories.  If your project is named \"foo,\" create GitHub repositories <code>foo-ml</code> and <code>foo-exec</code>.  GitHub templates for these repositories can be found here.</p> <p><code>foo-ml</code> will contain all of the code related to the actual ML models. We can split this code into two classes.  The first is code, which needs to understand the structure of the data model of the deriva catalog. These functions are best implemented by creating a derived class using DerivaML as a base class.  Any catalog-specific code or utility functions that might be useful to any ML model should be placed here.</p> <p>Code that implements a specific model should be implemented as a module in the models' directory. YOu can make these stand-alone functions or make them derived classes from the class implemented above.  Templates for a model module are provided.  In general, best practice suggests that these modules should be able to execute stand-alone (i.e., have a </p> <pre><code>def main():\n  ...\n\nif __file__ == \"main\":\n  main()\n</code></pre> <p>conditional at the bottom) and provide a single entry point for the model function.  Python prototype definitions are provided  for common ML functions in the library. We also recommend that you avoid the temptation to hardcode model parameters and paths but rather pass everything in as an argument to the entry points, as indicated by the prototypes. Finally, we advocate the liberal use of Python-type hints and the use of either Python data classes or pedantic class descriptions.  Pytantic is included as a standard part of the base Conda environment.</p> <p>The last piece of this puzzle is the <code>foo-exec</code> repository. This repository will house the code required to execute a specific instance of a model with a specific dataset. This repository is organized into notebook code or scripts, with corresponding subdirectories.</p> <p>Before running a model in production, we recommend that you commit all of your code in the <code>foo-ml</code> and <code>foo-exec</code> repositories and optionally provide a GitHub tag for your workflow. At that point, you can update your workflow specification [link] and run your code using the workflow_term component of the specification to differentiate the current execution from others.</p>"},{"location":"user-guide/execution-configuration/","title":"Configuring an execution","text":"<p>One of the essential functions of DerivaML is to help keep track how ML model results are created so that hey can be shared and reproduced. Every execution in DerivaML is represented by an Execution object, whick keeps track of all of the paramemters associated with and execution and provides a number of functions that enable a program to help keep track of the configuation and results of a model execution.</p> <p>The first step in creating a DerivaML execution is to create an <code>ExectuionConfiguration</code>.  The <code>ExecutionConfiguration</code> class is used to specify the inputs that go are to be used by an Execution. These inputs include * A list of datasets that are used * A list of other files (assets) that are to be used. This can include existing models, or any other infomration that the execution might need. * The actual code that is being executed.</p> <p><code>ExecutionConfiguration</code>  is a Pydantic dataclass. As part of initializing an execution, the assets and datasets in the configuration object are downloaded and cached.  The datasets are provided as a list of DatasetSpecw which  <code>DatasetSpec(dataset_rid:RID, version:DatasetVersion, materialize:bool)</code></p> <p>it will be common to just want to use the latest version of the dataset, in which case you would use: ` <code>` deriva_nl = DerivaML(...) dataset_rid = ... datasets = [DatasetSpec(dataset_rid, version=deriva_ml.dataset_version(dataset_rid))]</code></p> <p>If a dataset is large, downloading from the catalog might take a signficant amount of time.</p>"},{"location":"user-guide/file-assets/","title":"File Assets","text":""},{"location":"user-guide/identifiers/","title":"Identifiers in Deriva-ML","text":"<p>Having global unique identifiers is a critical aspect of data that is FAIR.  Within DerivaML every object is given a unique name wich we call a Resource Identifier or RID. The RID itself takes the from of a string with a dash seperated set of four character blocks. Unqualified, the RID refers to the current values in the catalog, for example <code>1-000C</code></p> <p>A RID may also specify a catalog snapshot ID, in which case it refers to a value at a specific point in time. Here is an example of a fully qualified RID: <code>1-000C@32S-W6DS-GAMG</code> which specifies an the same object as above but with a prior value. </p> <p>Within a catalog, we can just use the RID, with or without the snapshot ID. However, if we want to refer to a RID outside the catalog, we can use a URI form:</p> <pre><code>https://www.eye-ai.org/id/1-000C@32S-W6DS-GAMG\n</code></pre> <p>We obtained this RID using the Cite button on the user interface.   </p> <p>Within the DerivaML class, a URI version of a RID can be obtained using the DerivaML.cite method.</p>"},{"location":"user-guide/install/","title":"Installing deriva-ml","text":"<p>Deriva ML is a python package that consists of the deriva ML libary  along with a number of Jupyter notebooks that demonstrate how to use various deriva-ml features.</p> <p>The latest working version of deriva-ml can be found on pypy, can can be installed using pip:</p> <pre><code>`pip install deriva-ml`\n</code></pre> <p><code>deriva-ml</code> uses semantic versioning.  The <code>pip</code> command</p> <pre><code>pip show deriva-ml\n</code></pre> <p>can be used to find the current installed version of <code>deriva-ml</code>.  The installed version can be updated to the latest version using the command</p> <pre><code>pip install --upgrade deriva-ml\n</code></pre> <p>Once deriva-ml is installed, it can be imported into your Python ennvironment.  The library is orginized into a single package with all of the essential routines directly accessable from the top level package. A typical import statement would be</p> <pre><code>from deriva_ml import DerivaML, MLVocab, DatasetBag, ExecutionConfiguration, Workflow, DerivaSystemColumns\n</code></pre> <p>We note that in most situations, you will not use the DerivaML class directly, but rather it will be the base class for a derived class that has domain specific functions in it. For example:</p> <pre><code>from eye-ai-ml import EyeAI\nfrom deriva_ml import MLVocab, DatasetBag, ExecutionConfiguration, Workflow, DerivaSystemColumns\n</code></pre>"},{"location":"user-guide/notebooks/","title":"Using Jupyter Notebooks With DerivaML","text":"<p>DerivaML can be used to execute Jupyter notebooks in a reproducible and structured manner.  Although DerivaML provides numerous tools to support reproducible machine learning, users must adopt and maintain standardized development practices to fully benefit from these tools.</p> <p>In general, achieving reproducablity with Jupyter notebooks will require some disiplane on the part of the developer. For an amuzing take on some of the chalages assocated with Jupyter notebooks, the follow presentation is very helpful: I Don't Like Notebooks</p> <p>To ensure that your Jupyter notebooks are reproducible, follow these recommended guidelines:</p>"},{"location":"user-guide/notebooks/#version-control-and-semantic-versioning","title":"Version Control and Semantic Versioning","text":"<p>Always store your notebook in a GitHub repository. A repository template for DerivaML project can be found at DerivaML Repository Template. To use a GitHub template select the Use This Template dropdown from the GitHub user interface, rather than clone. The template contains examples of both a DerivaML Python script and Jupyter notebook.</p> <p>Adopt semantic versioning for your notebooks.  In addition to semantic versions, Git tags are also quite helpful. The repository templates provides a script to simplify managing version numbers and tags.</p> <pre><code>bumpversion major|minor|patch\n</code></pre>"},{"location":"user-guide/notebooks/#clearing-notebook-outputs","title":"Clearing Notebook Outputs","text":"<p>Normal operation of a Jupyter notebook puts results in output cells in the notebook, modifying the notebook file and complicating reproducablity. For this reason, we recommend that before committing a notebook to Git, to clear all output cells, ensuring that only code and markdown cells are version-controlled.</p> <p>While you can always clear output cells manaally from the notebook, DerivaML includes a script which automatically strips output cells upon commit.  To set this up, execute the following command once in your repository:</p> <pre><code>nboutputstrip --install\n</code></pre> <p>You only need to perform the install once per repository, and after that, the notebook output will be striped before every commit.</p>"},{"location":"user-guide/notebooks/#setting-notebook-parameters","title":"Setting Notebook Parameters","text":"<p>Another challange for reproducibility is that the behavior of cells in a notebook is often modifified by changing the values of global variables assigned in a code cell. In order to impose some order on this potentially chaotic process, DerivaML adopts the use of Papermill to help manage configuring notebooks prior to execution. The basic idea behind Papermill is to place all of the configuration variables for a notebook in a single cell, and then provide and interface that will substitute values in for those variables and run the notebook in its entirety.</p> <p>To use Papermill in DerivaML: - Define all configurable variables in a single \"parameters\" cell located immediately after your imports at the top of your notebook. The contents of this cell can be automatically updated when the notebook is executed.  For Papermill to work, you must have a Jupyter cell tagged with <code>parameters</code> to indicate which cell contains parameter values. The DerivaML template already has this cell tagged. See Papermill for instructions on how to do this.  . - The parameters cell should contain only comments and variable assignments.  It is recommended to include type hints for clarity and usability. - Avoid setting configuration variables elsewhere in your notebook.</p>"},{"location":"user-guide/notebooks/#notebook-structure-and-execution-flow","title":"Notebook Structure and Execution Flow","text":"<p>The overall workflow supported by DerivaML is a phase in which notebooks are developed and debugged, followed by an experimental phase in which multiple model parameters might be evauated, or alternative approches explored. The boundary between debugging and experimentation can be fuzzy, in general it is better to err on the side of considering a run of a notebook to be an experiment rather than debugging.</p> <p>The following guidelines can help facilitate notebook reproducibility: - Structure your notebook so that it runs sequentially, from the first to the last cell. - Regularly restart the kernel, clear outputs, and execute all cells sequentially to confirm reproducibility. - Keep each notebook focused on a single task; avoid combining multiple tasks within one notebook. - Utilize the <code>dry_run</code> mode during debugging to avoid cluttering the catalog with unnecessary execution records. Example:</p> <pre><code>exe = ml_instance.create_execution(configuration=ExecutionConfiguration(...), dry_run=True)\n</code></pre> <p>Use <code>dry_run</code> only for debugging, not during model tuning, as recording all tuning attempts is crucial for transparency and reproducibility.</p>"},{"location":"user-guide/notebooks/#commit-and-tagging-procedures","title":"Commit and Tagging Procedures","text":"<p>After validating your notebook, commit it and generate a corresponding version tag using the provided scripts. For example:</p> <pre><code>./bump-version.sh  major|minor|patch\n</code></pre>"},{"location":"user-guide/notebooks/#executing-notebooks-with-derivaml","title":"Executing Notebooks with DerivaML","text":"<p>A reproducable notebook execution has three components.  1. A commited notebook file is specified. 2. Per-execution specific values for variables in the <code>parameters</code> cell are specfified and a new cell with the specified parameter values is injected into the notebock. 3. The modified notebook is executed in its entirety, including the uploading of any notebook generated assets. 4. On conclusion of the notebook executiong, the resulting notebook file, including output cells is uploaded into the DerivaML catalog and stored in the Execution_Assets table.</p> <p>DerivaML includes the <code>deriva-ml-run-notebook</code> command to conveniently execute notebooks, substitute parameters dynamically, and store the execution results as assets. AN example of how this command is used is:</p> <pre><code>deriva-ml-run-notebook --parameter parameter1 value1 --parameter parameter2 value2 my-notebook.ipynb\n</code></pre> <p>This command substitutes <code>value1</code> and <code>value2</code> into the corresponding parameters within the notebook's parameters cell, executes the notebook entirely, and saves the resulting notebook as an execution asset in the catalog.</p> <p>Alternatively, parameters can be specified via a JSON configuration file using the <code>--file filename</code> option. You may also automate experiments using scripts stored in GitHub, ensuring reproducibility through version control and clear documentation.</p>"},{"location":"user-guide/overview/","title":"Overview of DerivaML","text":""},{"location":"user-guide/overview/#overview","title":"Overview","text":"<p>Deriva-ML is a Python utility designed to streamline end-to-end machine learning (ML) experiments by integrating data  and metadata stored on the Deriva platform. It facilitates a seamless workflow for managing data catalogs, preprocessing,  model execution, and result documentation.</p> <p>Key components for Deriva-ML:</p> <ol> <li>Data Catalog: The catalog must include both the domain schema and a standard ML schema for effective data management.   </li> <li>Domain schema: The domain schema includes the data collected or generated by domain-specific experiments or systems.</li> <li> <p>ML schema: Each entity in the ML schema is designed to capture details of the ML development process. It including the following tables</p> <ul> <li>A Dataset represents a data collection, such as aggregation identified for training, validation, and testing purposes.</li> <li>A Workflow represents a specific sequence of computational steps or human interactions.</li> <li>An Execution is an instance of a workflow that a user instantiates at a specific time. </li> <li>An Execution Asset is an output file that results from the execution of a workflow.</li> <li>An Execution Metadata is an asset entity for saving metadata files referencing a given execution.</li> </ul> </li> <li> <p>Deriva-ML library: including the following process</p> </li> <li>Execution initiate</li> <li>ML execution context manager</li> <li> <p>Execution upload</p> </li> <li> <p>Catalog-ML library: A tool derived from Deriva-ML for configuring data curation tasks tailored to specific data catalogs. The method including the following categories:</p> </li> <li>Data pre-processing</li> <li>Data Analysis </li> </ol>"},{"location":"user-guide/overview/#installation","title":"Installation","text":"<p>To install Deriva-ML and its dependencies, use the following commands:</p> <pre><code>$ pip install deriva\n$ pip install deirva-ml \n$ pip install &lt;catalog-ML&gt;\n</code></pre>"},{"location":"user-guide/overview/#a-notebook-workflow","title":"A Notebook workflow","text":"<p>The Deriva-ML notebook workflow consists of an end-to-end process that includes: - Data migration from the catalog to the runtime environment.  - Data preprocessing.  - Execution of ML experiments.  - Storing outputs and metadata back into the catalog.</p>"},{"location":"user-guide/overview/#step-1-setup-the-environment","title":"Step 1: Setup the environment","text":""},{"location":"user-guide/overview/#load-prerequisite-packages","title":"Load prerequisite packages","text":"<pre><code># Prerequisites\nfrom pathlib import Path, PurePath\nimport logging\n\nfrom deriva_ml import DatasetBag, Workflow, ExecutionConfiguration\nfrom deriva_ml import MLVocab as vc\nimport &lt;catalog-ml&gt;\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)\n</code></pre>"},{"location":"user-guide/overview/#login-to-the-deriva-catalog","title":"Login to the Deriva catalog","text":"<pre><code>from deriva.core.utils.globus_auth_utils import GlobusNativeLogin\nhost = 'www.***'\ncatalog_id = \"***\"\n\ngnl = GlobusNativeLogin(host=host)\nif gnl.is_logged_in([host]):\n    print(\"You are already logged in.\")\nelse:\n    gnl.login([host], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n    print(\"Login Successful\")\n</code></pre>"},{"location":"user-guide/overview/#set-working-and-caching-directories","title":"Set working and caching directories","text":"<ul> <li>Cache Directory: Used for saving materialized dataset bags. </li> <li>Working Directory: Stores temporary files for the execution, such as metadata and features, which are uploaded to the catalog.   (The content under this directory may be deleted after successfully upload to catalog by default.)</li> </ul> <pre><code>cache_dir = Path.home() / 'data'\nworking_dir = Path.home() / 'data'\nml_instance = catalogML(hostname = host, catalog_id = catalog_id, cache_dir= cache_dir, working_dir=working_dir)\n</code></pre>"},{"location":"user-guide/overview/#step2-execution-configuration","title":"Step2: Execution Configuration","text":"<p>Before starting execution: - Define the workflow and related objects (datasets, models). - Cache all required files in the caching directory.</p> <ul> <li>Configuration Parameters</li> <li>datasets: List of dictionary that contains dataset rids found in the catalog's Dataset table, and a flag to    indicate whether download the assets or not.</li> <li>assets: List of execution asset rids from the Execution_Assets table.</li> <li>execution: A descriptive Execution object.</li> <li>workflow: A Workflow object, including its type and associated code scripts.</li> <li>Instanciate the workflow by <code>Workflow()</code></li> <li>The workflow type is a controlled vocabulary term. Make sure the term is already existed in the catalog,    or create the new term before use. To create a new workflow_term, you can use    <code>ml_instance.add_term(vc.workflow_type, &lt;term_name&gt;, description=\"***\")</code>. Detailed description of how to add a new control   vocabulary term in the catalog see ##ToDo</li> <li>description: A textual summary of the current execution.</li> </ul> <p>Initialize the execution by <code>ml_instance.initialize_execution(config)</code>. A full record of directory of cached data and files, and rids of current execution will be returned.\\ Example:</p> <pre><code># RID of source dataset_table, if any.\nsource_dataset = &lt;dataset_rid&gt;\n\n# Add Workflow Vocab terms\nml_instance.add_term(vc.workflow_type, \"Test Workflow\", description=\"A test Workflow for new DM\")\n\n# Workflow instance\ntest_workflow = Workflow(\n    name=\"Test Sample Workflow\",\n    url=\"https://github.com/***\",\n    workflow_type=\"Test Workflow\"\n)\n\n# Configuration instance.\nconfig = ExecutionConfiguration(\n    # Comment out the following line if you don't need the assets.\n    # Materialize set to False if you only need the metadata from the bag, and not the assets.\n    datasets= [{'rid':source_dataset, 'materialize':True}],\n    assets=[asset_rid1, asset_rid2, ...],\n    workflow=test_workflow,\n    description=\"Our Test Workflow instance\")\n\n# Initialize execution\nexecution = ml_instance.create_execution(config)\nprint(execution)\n</code></pre> <p>Sample return:</p> <pre><code>caching_dir: /data\nworking_dir: /data/&lt;username&gt;/&lt;catalog&gt;_working\nexecution_rid: 5-SJ9Y\nworkflow_rid: 5-SG9W\ndataset_paths: [PosixPath('/data/2-AGAW_c3a8bcbd37b4c454471d0e057c550312719006f76604db0e7f65d4a539974b12/Dataset_2-AGAW')]\nasset_paths: [PosixPath('/data/.../EyeAI_working/5-SJ9Y/asset/optic_disk_crop_model.hdf5')]\nconfiguration: datasets=['2-AGAW'] \n               assets=['2-4JR6'] \n               workflow=Workflow(name='Test New Workflow-multimodal', \n                                 url='https://github.com/.../.../template.ipynb', \n                                 workflow_type='Test Workflow', \n                                 version=None, \n                                 description=None) \n               description='Template instance of a feature creation workflow'\n</code></pre>"},{"location":"user-guide/overview/#step3-access-datasets","title":"Step3: Access Datasets","text":"<p>In the notebook environment, you can save the downloaded dataset in to a <code>DatasetBag</code>. It is built on top of an <code>sqLite</code> database, enabling easy access to the tables by table name and datasets curation. - Build DatasetBag from downloaded data: <code>ds_bag = DatasetBag(configuration_record.bag_paths[i])</code> - Find all the tables in the database by <code>ds_bag.list_tables()</code> - Load the data in a table to pandas.DataFrame by <code>ds_bag.get_table_as_dataframe(table_name)</code></p>"},{"location":"user-guide/overview/#step4-ml-execution","title":"Step4: ML Execution","text":"<p>Run ML algorithms within a managed context. This ensures execution status is logged back to the catalog.</p> <pre><code>with ml_instance.execution(configuration=configuration_record) as exec:\n  # Run machine learning algorithms here ...\n  pass\n</code></pre>"},{"location":"user-guide/overview/#step5-upload-results","title":"Step5: Upload results","text":"<p>Save and upload outputs to the catalog, categorized into:</p> <ul> <li>Metadata: Stored in the directory obtained by <code>execution.execution_metadata_path(&lt;metadata_type&gt;)</code>.</li> <li>Assets: Stored in the directory obtained by <code>execution.execution_assets_path(&lt;asset_type&gt;)</code>.</li> </ul> <p>Ensure that the metadata and asset types exist in the catalog before uploading.</p> <pre><code># Add controlled vocabulary terms for metadata and assets\nml_instance.add_term(vc.Execution_Metadata_Type, \"Example_Metadata_Type\", description=\"Metadata Type description\")\nml_instance.add_term(vc.Execution_Asset_Type, \"Example_Asset_Type\", description=\"Asset Type description\")\n\nmetadata_path = execution.execution_metadata_path(Example_Metadata_Type)\nasset_path = execution.execution_asset_path(Example_Asset_Type)\n\n# Save files to metadata path and asset path.\n\n# Upload files to the catalog\nexecution.upload_execution_outputs(clean_folder=True)\n</code></pre> <p>Upon completion, all files can be accessed in the Execution Assets, Execution Metadata, and Features tables,  or through the current execution in the Execution table.</p>"}]}