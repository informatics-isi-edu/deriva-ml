{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94ffee41-4986-4196-b35d-b38b6e10490a",
   "metadata": {},
   "source": [
    "DerivaML is a class library built on the Deriva Scientific Asset management system that is designed to help simplify a number of the basic operations associated with building and testing ML libraries based on common toolkits such as TensorFlow.  This notebook reviews the basic features of the DerivaML library."
   ]
  },
  {
   "cell_type": "code",
   "id": "ff605747-195b-40a1-b915-0e799f8d0748",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from deriva.core.utils.globus_auth_utils import GlobusNativeLogin\n",
    "from deriva_ml.schema_setup.test_catalog import create_test_catalog, DemoML\n",
    "from math import floor"
   ],
   "id": "edd011382d623f72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set the details for the catalog we want and authenticate to the server if needed.",
   "id": "502f9c5ca4feee8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hostname = 'dev.eye-ai.org'\n",
    "domain_schema = 'demo-schema'\n",
    "\n",
    "gnl = GlobusNativeLogin(host=hostname)\n",
    "if gnl.is_logged_in([hostname]):\n",
    "    print(\"You are already logged in.\")\n",
    "else:\n",
    "    gnl.login([hostname], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n",
    "    print(\"Login Successful\")\n"
   ],
   "id": "1d18222d50d0ef7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create a test catalog and get an instance of the DemoML class.",
   "id": "eb169235902c77be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_catalog = create_test_catalog(hostname, domain_schema)\n",
    "ml_instance = DemoML(hostname, test_catalog.catalog_id)"
   ],
   "id": "677df6f200423a9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Current dataset element types: {[a.name for a in ml_instance.list_dataset_element_types()]}\")\n",
    "ml_instance.add_dataset_element_type(\"Subject\")\n",
    "ml_instance.add_dataset_element_type(\"Image\")\n",
    "print(f\"New dataset element types {[a.name for a in ml_instance.list_dataset_element_types()]}\")"
   ],
   "id": "b1a71deb6e6f8c4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a new dataset\n",
    "ml_instance.add_term(\"Dataset_Type\", \"DemoSet\", description=\"A test dataset\")\n",
    "ml_instance.add_term('Dataset_Type', 'Partitioned', description=\"A partitioned dataset for ML training.\")\n",
    "ml_instance.add_term(\"Dataset_Type\", \"Subject\", description=\"A test dataset\")\n",
    "ml_instance.add_term(\"Dataset_Type\", \"Image\", description=\"A test dataset\")\n",
    "ml_instance.add_term(\"Dataset_Type\", \"Training\", description=\"Training dataset\")\n",
    "ml_instance.add_term(\"Dataset_Type\", \"Testing\", description=\"Training dataset\")\n",
    "ml_instance.add_term(\"Dataset_Type\", \"Validation\", description=\"Validation dataset\")\n",
    "\n",
    "subject_dataset = ml_instance.create_dataset(['DemoSet', 'Subject'], description=\"A subject dataset\")\n",
    "image_dataset = ml_instance.create_dataset(['DemoSet', 'Image'], description=\"A image training dataset\")\n",
    "\n",
    "dp = ml_instance.domain_path  # Each call returns a new path instance, so only call once...\n",
    "subject_rids = [i['RID'] for i in dp.tables['Subject'].entities().fetch()]\n",
    "image_rids = [i['RID'] for i in dp.tables['Image'].entities().fetch()]\n",
    "\n",
    "ml_instance.add_dataset_members(dataset_rid=subject_dataset, members=subject_rids)\n",
    "ml_instance.add_dataset_members(dataset_rid=image_dataset, members=image_rids)"
   ],
   "id": "a843170d141c8a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def strip_system(d):\n",
    "    return {k:v for k,v in d.items() if k not in ['RCT', 'RMT', 'RCB', 'RMB']}\n",
    "\n",
    "display(pd.DataFrame([strip_system(d) for d in ml_instance.list_dataset_members(subject_dataset)['Subject']]))\n",
    "display(pd.DataFrame([strip_system(d) for d in ml_instance.list_dataset_members(image_dataset)['Image']]))\n"
   ],
   "id": "21e3d412920ac963",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now lets create some subsets of the origional dataset based on subject level metadata. We are going to create the subsets based on the metadata values of the subjects.  SO we will download the subject dataset and look at its metadata to figure out whow to partition the origional data.",
   "id": "a64f3d24c5c5df5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bag_path, bag_rid = ml_instance.materialize_bdbag(subject_dataset)\n",
    "print(f\"Bag materialized to {bag_path}\")"
   ],
   "id": "348c09465600d0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The domain model has two object: Subject and Images where an Image is associated with a subject, but a subject can have multiple images associated with it.  Lets look at the subjects and partition into test and training datasets.",
   "id": "4f52eca30da9c89f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import csv\n",
    "print(f\"Bag path is: {bag_path}\")\n",
    "os.chdir(bag_path / 'data/Subject')\n",
    "%ls \n",
    "\n",
    "# Get information about the subjects.....\n",
    "with open('Subject.csv') as csvfile:\n",
    "    subject_map = {s['RID']: {'Subject RID': s['RID'], 'Name': s['Name']} for s in csv.DictReader(csvfile)}\n",
    "\n",
    "# and combine with image (lets assume that there is only one image per subject).\n",
    "with open('Image/Image.csv') as csvfile:\n",
    "    metadata = [ subject_map[row['Subject']] | {'Image RID': row['RID'], 'URL': row['URL']} for row in csv.DictReader(csvfile)]\n",
    "        \n",
    "\n",
    "display(pd.DataFrame(metadata))"
   ],
   "id": "e5428c869fe14b0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def thing_number(name: str) -> int:\n",
    "    return int(name.replace('Thing',''))\n",
    "    \n",
    "training_rids = [s['Image RID'] for s in metadata if thing_number(s['Name']) % 3 == 0]\n",
    "testing_rids =  [s['Image RID'] for s in metadata if thing_number(s['Name']) % 3 == 1]\n",
    "validation_rids = [s['Image RID'] for s in metadata if thing_number(s['Name']) % 3 == 2]\n",
    "print(f'Training images: {training_rids}')\n",
    "print(f'Testing images: {testing_rids}')\n",
    "print(f'Validation images: {validation_rids}')\n"
   ],
   "id": "696515d6078a2f43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nested_dataset = ml_instance.create_dataset(['Partitioned', 'Image'], description='A nested dataset for machine learning')\n",
    "training_dataset = ml_instance.create_dataset('Training', description='An image dataset for training')\n",
    "testing_dataset = ml_instance.create_dataset('Testing', description='A image dataset for testing')\n",
    "validation_dataset = ml_instance.create_dataset('Validation', description='A image dataset for validation')\n",
    "\n",
    "ml_instance.add_dataset_members(dataset_rid=nested_dataset, members=[training_dataset, testing_dataset, validation_dataset])\n",
    "ml_instance.add_dataset_members(dataset_rid=training_dataset, members=training_rids)\n",
    "ml_instance.add_dataset_members(dataset_rid=testing_dataset, members=testing_rids)\n",
    "ml_instance.add_dataset_members(dataset_rid=validation_dataset, members=validation_rids)\n"
   ],
   "id": "d8028a3d346a2ea7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ok, lets see what we have now.",
   "id": "2ac02c487e8a3be3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pd.DataFrame([strip_system(d) for d in ml_instance.find_datasets()])",
   "id": "ab2db7fc47520eb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "display(pd.DataFrame([strip_system(d) for d in ml_instance.list_dataset_members(nested_dataset)['Dataset']]))\n",
    "display(pd.DataFrame([strip_system(d) for d in ml_instance.list_dataset_members(training_dataset)['Image']]))\n",
    "display(pd.DataFrame([strip_system(d) for d in ml_instance.list_dataset_members(testing_dataset)['Image']]))\n",
    "display(pd.DataFrame([strip_system(d) for d in ml_instance.list_dataset_members(validation_dataset)['Image']]))\n"
   ],
   "id": "fac3a1b3d7db6556",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display(pd.DataFrame([strip_system(m) for m in ml_instance.find_datasets()]))",
   "id": "5f8f12b942310485",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ml_instance.cite(nested_dataset)",
   "id": "4025b1c81ca7eb8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now lets download a dataset so that we can compute on it locally.",
   "id": "52c5ddd58bc45d95"
  },
  {
   "cell_type": "code",
   "id": "6e76bbdf-2441-4444-be7f-e55399bcc32a",
   "metadata": {},
   "source": [
    "test_catalog.delete_ermrest_catalog(really=True)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deriva-ml",
   "language": "python",
   "name": "deriva-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
