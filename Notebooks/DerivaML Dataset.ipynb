{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ff6e4c93a70770",
   "metadata": {},
   "source": [
    "# DerivaML Dataset Example.\n",
    "\n",
    "DerivaML is a class library built on the Deriva Scientific Asset management system that is designed to help simplify a number of the basic operations associated with building and testing ML libraries based on common toolkits such as TensorFlow.  This notebook reviews the basic features of the DerivaML library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db30e3e5ccd3ad77",
   "metadata": {},
   "source": [
    "## Set up DerivaML  for test case"
   ]
  },
  {
   "cell_type": "code",
   "id": "29f3e870d1fea4f8",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d18222d50d0ef7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T20:30:53.855014Z",
     "start_time": "2024-12-02T20:30:53.457829Z"
    }
   },
   "source": [
    "from deriva.core.utils.globus_auth_utils import GlobusNativeLogin\n",
    "from demo_catalog import create_demo_catalog, DemoML\n",
    "from deriva_ml import MLVocab as vt, DatasetBag\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown, HTML"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'create_demo_catalog' from 'demo_catalog' (/Users/carl/opt/anaconda3/envs/deriva-ml/lib/python3.12/site-packages/demo_catalog/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mderiva\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mglobus_auth_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GlobusNativeLogin\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdemo_catalog\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m create_demo_catalog, DemoML\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mderiva_ml\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MLVocab \u001B[38;5;28;01mas\u001B[39;00m vt, DatasetBag\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'create_demo_catalog' from 'demo_catalog' (/Users/carl/opt/anaconda3/envs/deriva-ml/lib/python3.12/site-packages/demo_catalog/__init__.py)"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "eb169235902c77be",
   "metadata": {},
   "source": [
    "Set the details for the catalog we want and authenticate to the server if needed."
   ]
  },
  {
   "cell_type": "code",
   "id": "677df6f200423a9d",
   "metadata": {},
   "source": [
    "hostname = 'dev.eye-ai.org'\n",
    "domain_schema = 'demo-schema'\n",
    "\n",
    "gnl = GlobusNativeLogin(host=hostname)\n",
    "if gnl.is_logged_in([hostname]):\n",
    "    print(\"You are already logged in.\")\n",
    "else:\n",
    "    gnl.login([hostname], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n",
    "    print(\"Login Successful\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cc01e85a63c60536",
   "metadata": {},
   "source": [
    "Create a test catalog and get an instance of the DemoML class."
   ]
  },
  {
   "cell_type": "code",
   "id": "a843170d141c8a8",
   "metadata": {},
   "source": [
    "test_catalog = create_demo_catalog(hostname, domain_schema)\n",
    "ml_instance = DemoML(hostname, test_catalog.catalog_id)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "48c7854af033245",
   "metadata": {},
   "source": [
    "## Configure DerivaML Datasets\n",
    "\n",
    "In Deriva-ML a dataset is used to aggregate instances of entities.  However, before we can create any datasets, we must configure \n",
    "Deriva-ML for the specifics of the datasets.  The first stp is we need to tell Deriva-ML what types of use defined objects can be associated with a dataset.  \n",
    "\n",
    "Note that out of the box, Deriva-ML is configured to allow datasets to contained dataset (i.e. nested datasets), so we don't need to do anything for that specific configuration."
   ]
  },
  {
   "cell_type": "code",
   "id": "21e3d412920ac963",
   "metadata": {},
   "source": [
    "print(f\"Current dataset element types: {[a.name for a in ml_instance.list_dataset_element_types()]}\")\n",
    "ml_instance.add_dataset_element_type(\"Subject\")\n",
    "ml_instance.add_dataset_element_type(\"Image\")\n",
    "print(f\"New dataset element types {[a.name for a in ml_instance.list_dataset_element_types()]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "874f49d81439e0c9",
   "metadata": {},
   "source": "Now that we have configured our datasets, we need to identify the dataset types so we can distinguish between them."
  },
  {
   "cell_type": "code",
   "id": "6908fba1443aee13",
   "metadata": {},
   "source": [
    "# Create a new dataset\n",
    "ml_instance.add_term(vt.dataset_type, \"DemoSet\", description=\"A test dataset\")\n",
    "ml_instance.add_term(vt.dataset_type, 'Partitioned', description=\"A partitioned dataset for ML training.\")\n",
    "ml_instance.add_term(vt.dataset_type, \"Subject\", description=\"A test dataset\")\n",
    "ml_instance.add_term(vt.dataset_type, \"Image\", description=\"A test dataset\")\n",
    "ml_instance.add_term(vt.dataset_type, \"Training\", description=\"Training dataset\")\n",
    "ml_instance.add_term(vt.dataset_type, \"Testing\", description=\"Training dataset\")\n",
    "ml_instance.add_term(vt.dataset_type, \"Validation\", description=\"Validation dataset\")\n",
    "\n",
    "ml_instance.list_vocabulary_terms(vt.dataset_type)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1f902cd9561842c2",
   "metadata": {},
   "source": [
    "Now create datasets and populate with elements from the test catalogs."
   ]
  },
  {
   "cell_type": "code",
   "id": "10567931cbd3c322",
   "metadata": {},
   "source": [
    "system_columns = ['RCT', 'RMT', 'RCB', 'RMB']\n",
    "\n",
    "subject_dataset = ml_instance.create_dataset(['DemoSet', 'Subject'], description=\"A subject dataset\")\n",
    "image_dataset = ml_instance.create_dataset(['DemoSet', 'Image'], description=\"A image training dataset\")\n",
    "datasets = pd.DataFrame(ml_instance.find_datasets()).drop(columns=system_columns)\n",
    "display(\n",
    "    Markdown('## Datasets'),\n",
    "    datasets)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6474d1edb488097f",
   "metadata": {},
   "source": "And now that we have defined some datasets, we can add elements of the appropriate type to them.  We can see what is in our new datasets by listing the dataset members."
  },
  {
   "cell_type": "code",
   "id": "16bb5b8b8b2a5288",
   "metadata": {},
   "source": [
    "dp = ml_instance.domain_path  # Each call returns a new path instance, so only call once...\n",
    "subject_rids = [i['RID'] for i in dp.tables['Subject'].entities().fetch()]\n",
    "image_rids = [i['RID'] for i in dp.tables['Image'].entities().fetch()]\n",
    "\n",
    "ml_instance.add_dataset_members(dataset_rid=subject_dataset, members=subject_rids)\n",
    "ml_instance.add_dataset_members(dataset_rid=image_dataset, members=image_rids)\n",
    "\n",
    "# List the contents of our datasets, and let's not include columns like modify time.\n",
    "display(\n",
    "    Markdown('## Subject Dataset'),\n",
    "    pd.DataFrame(ml_instance.list_dataset_members(subject_dataset)['Subject']).drop(columns=system_columns),\n",
    "    Markdown('## Image Dataset'),\n",
    "    pd.DataFrame(ml_instance.list_dataset_members(image_dataset)['Image']).drop(columns=system_columns))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2ac02c487e8a3be3",
   "metadata": {},
   "source": [
    "## Create partitioned dataset\n",
    "\n",
    "Now let's create some subsets of the original dataset based on subject level metadata. We are going to create the subsets based on the metadata values of the subjects. We will download the subject dataset and look at its metadata to figure out how to partition the original data. Since we are not going to look at the images, we use download_dataset_bag, rather than materialize_bag."
   ]
  },
  {
   "cell_type": "code",
   "id": "5f8f12b942310485",
   "metadata": {},
   "source": [
    "bag_path, bag_rid = ml_instance.download_dataset_bag(subject_dataset)\n",
    "ml_instance.materialize_dataset_bag(subject_dataset)\n",
    "dataset_bag = DatasetBag(bag_path)\n",
    "print(f\"Bag materialized to {bag_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aefa71000a8888b5",
   "metadata": {},
   "source": "The domain model has two object: Subject and Images where an Image is associated with a subject, but a subject can have multiple images associated with it.  Let's look at the subjects and partition into test and training datasets."
  },
  {
   "cell_type": "code",
   "id": "405cebbc9d686f9b",
   "metadata": {},
   "source": [
    "# Get information about the subjects.....\n",
    "subject_df = dataset_bag.get_table_as_dataframe('Subject')[['RID', 'Name']]\n",
    "image_df = dataset_bag.get_table_as_dataframe('Image')[['RID', 'Subject', 'URL']]\n",
    "metadata_df = subject_df.join(image_df, lsuffix=\"_subject\", rsuffix=\"_image\")\n",
    "display(metadata_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a251643b1b7389b2",
   "metadata": {},
   "source": [
    "For ths example, lets partition the data based on the name of the subject.  Of course in real examples, we would do a more complex analysis in deciding\n",
    "what subset goes into each data set."
   ]
  },
  {
   "cell_type": "code",
   "id": "fac3a1b3d7db6556",
   "metadata": {},
   "source": [
    "def thing_number(name: pd.Series) -> pd.Series:\n",
    "    return name.map(lambda n: int(n.replace('Thing','')))\n",
    "\n",
    "training_rids = metadata_df.loc[lambda df: thing_number(df['Name']) % 3 == 0]['RID_image'].tolist()\n",
    "testing_rids =  metadata_df.loc[lambda df: thing_number(df['Name']) % 3 == 1]['RID_image'].tolist()\n",
    "validation_rids = metadata_df.loc[lambda df: thing_number(df['Name']) % 3 == 2]['RID_image'].tolist()\n",
    "\n",
    "print(f'Training images: {training_rids}')\n",
    "print(f'Testing images: {testing_rids}')\n",
    "print(f'Validation images: {validation_rids}')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "27e66ce59b0fb193",
   "metadata": {},
   "source": [
    "Now that we know what we want in each dataset, lets create datasets for each of our partitioned elements along with a nested dataset to track the entire collection."
   ]
  },
  {
   "cell_type": "code",
   "id": "1fcf179738fad9f2",
   "metadata": {},
   "source": [
    "nested_dataset = ml_instance.create_dataset(['Partitioned', 'Image'], description='A nested dataset for machine learning')\n",
    "training_dataset = ml_instance.create_dataset('Training', description='An image dataset for training', dataset_parent=nested_dataset)\n",
    "testing_dataset = ml_instance.create_dataset('Testing', description='A image dataset for testing', dataset_parent=nested_dataset)\n",
    "validation_dataset = ml_instance.create_dataset('Validation', description='A image dataset for validation', nested_dataset=nested_dataset)\n",
    "pd.DataFrame(ml_instance.find_datasets()).drop(columns=system_columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ac71fa8b087340ec",
   "metadata": {},
   "source": [
    "And then fill the datasets with the appropriate members."
   ]
  },
  {
   "cell_type": "code",
   "id": "6e76bbdf-2441-4444-be7f-e55399bcc32a",
   "metadata": {},
   "source": [
    "\n",
    "ml_instance.add_dataset_members(dataset_rid=nested_dataset, members=[training_dataset, testing_dataset, validation_dataset])\n",
    "ml_instance.add_dataset_members(dataset_rid=training_dataset, members=training_rids)\n",
    "ml_instance.add_dataset_members(dataset_rid=testing_dataset, members=testing_rids)\n",
    "ml_instance.add_dataset_members(dataset_rid=validation_dataset, members=validation_rids)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9d67f5a8334cde9d",
   "metadata": {},
   "source": [
    "Ok, lets see what we have now."
   ]
  },
  {
   "cell_type": "code",
   "id": "93af8b022d6c7edf",
   "metadata": {},
   "source": [
    "display(\n",
    "    Markdown('## Nested Dataset'),\n",
    "    pd.DataFrame(ml_instance.list_dataset_members(nested_dataset)['Dataset']).drop(columns=system_columns),\n",
    "    Markdown('## Training Dataset'),\n",
    "    pd.DataFrame(ml_instance.list_dataset_members(training_dataset)['Image']).drop(columns=system_columns),\n",
    "    Markdown('## Testing Dataset'),\n",
    "    pd.DataFrame(ml_instance.list_dataset_members(testing_dataset)['Image']).drop(columns=system_columns),\n",
    "    Markdown('## Validation Dataset'),\n",
    "    pd.DataFrame(ml_instance.list_dataset_members(validation_dataset)['Image']).drop(columns=system_columns))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fc6b8ddd57688870",
   "metadata": {},
   "source": "As our very last step, lets get a PID that will allow us to share and cite the dataset that we just created"
  },
  {
   "cell_type": "code",
   "id": "636459f674713ec9",
   "metadata": {},
   "source": [
    "dataset_citation = ml_instance.cite(nested_dataset)\n",
    "display(\n",
    "    HTML(f'Nested dataset citation: <a href={nested_dataset}>{nested_dataset}</a>')\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display(HTML(f'<a href={ml_instance.chaise_url(\"Dataset\")}>Browse Datasets</a>'))",
   "id": "4a27565e0e1fcd85",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2b47915187993a22",
   "metadata": {},
   "source": [
    "test_catalog.delete_ermrest_catalog(really=True)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deriva-ml",
   "language": "python",
   "name": "deriva-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
